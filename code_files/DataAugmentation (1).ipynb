{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (3.6.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.6/site-packages (from nltk) (2021.11.10)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from nltk) (4.51.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from nltk) (1.0.1)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.6/site-packages (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.19.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.51.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers) (3.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers) (4.4.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (0.1.96)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: seqeval in /opt/conda/lib/python3.6/site-packages (1.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.6/site-packages (from seqeval) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.6/site-packages (from seqeval) (1.19.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn>=0.21.3->seqeval) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.6/site-packages (from scikit-learn>=0.21.3->seqeval) (1.5.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.6/site-packages (4.15.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.51.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers) (4.4.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers) (3.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.19.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (0.1.96)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model_new'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0dbf5727d75a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mseqeval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_new\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEntityModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model_new'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "\n",
    "# from transformers import AdamW\n",
    "# from transformers import get_linear_schedule_with_warmup\n",
    "from nltk.corpus import wordnet\n",
    "import config\n",
    "# from transformers import XLMRobertaForTokenClassification, XLMRobertaConfig ,BertModel, XLMRobertaTokenizer, XLMRobertaModel, BertForTokenClassification\n",
    "from data_utils import loadDatafromFile,createTokenizedDf,CompDataset,createkfoldData,createDataloaders\n",
    "from seqeval.metrics import accuracy_score, classification_report\n",
    "import engine\n",
    "from model_new import EntityModel\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAugmentation:\n",
    "    def __init__(self):\n",
    "        self.stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', \n",
    "    'ours', 'ourselves', 'you', 'your', 'yours', \n",
    "    'yourself', 'yourselves', 'he', 'him', 'his', \n",
    "    'himself', 'she', 'her', 'hers', 'herself', \n",
    "    'it', 'its', 'itself', 'they', 'them', 'their', \n",
    "    'theirs', 'themselves', 'what', 'which', 'who', \n",
    "    'whom', 'this', 'that', 'these', 'those', 'am', \n",
    "    'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
    "    'have', 'has', 'had', 'having', 'do', 'does', 'did',\n",
    "    'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
    "    'because', 'as', 'until', 'while', 'of', 'at', \n",
    "    'by', 'for', 'with', 'about', 'against', 'between',\n",
    "    'into', 'through', 'during', 'before', 'after', \n",
    "    'above', 'below', 'to', 'from', 'up', 'down', 'in',\n",
    "    'out', 'on', 'off', 'over', 'under', 'again', \n",
    "    'further', 'then', 'once', 'here', 'there', 'when', \n",
    "    'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "    'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "    'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', \n",
    "    'very', 's', 't', 'can', 'will', 'just', 'don', \n",
    "    'should', 'now', '']\n",
    "    #DataAugmentation methods\n",
    "    \n",
    "    def swap_word(self,new_words,labels_src):\n",
    "        '''Helper function for random swap.''' \n",
    "\n",
    "        random_idx_1 = random.randint(0, len(new_words)-1)\n",
    "        random_idx_2 = random_idx_1\n",
    "        counter = 0\n",
    "        while random_idx_2 == random_idx_1:\n",
    "            random_idx_2 = random.randint(0, len(new_words)-1)\n",
    "            counter += 1\n",
    "            if counter > 3:\n",
    "                return (new_words,labels_src)\n",
    "        new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n",
    "        labels_src[random_idx_1], labels_src[random_idx_2] = labels_src[random_idx_2], labels_src[random_idx_1]\n",
    "        return (new_words, labels_src)\n",
    "\n",
    "    \n",
    "    def random_swap(self,dataframe,n): # n is number of times to swap randomly 2 words\n",
    "\n",
    "        '''Takes in input the initial dataframe read from the files and returns \n",
    "        modefied/increased dataframe with swapped word fro each source sentence and its corresponding token wise labels '''\n",
    "        \n",
    "        source_sentences  = list(dataframe.source)\n",
    "        target_sentences = list(dataframe.target)\n",
    "        labels_src = list(dataframe.src_tokens)\n",
    "        labels_tar = list(dataframe.tar_tokens)\n",
    "        source_sentences_temp =[]\n",
    "        labels_sec_temp=[]\n",
    "        i=0\n",
    "\n",
    "        for sentences, labels in zip(source_sentences,labels_src):\n",
    "\n",
    "            sentences = sentences.split()\n",
    "            labels = labels.split()\n",
    "            for _ in range(n):\n",
    "                sentences, labels = self.swap_word(sentences,labels)\n",
    "\n",
    "            sentences_str = ' '.join(sentences)\n",
    "            labels_str = ' '.join(labels)\n",
    "            target_sentences.append(target_sentences[i])\n",
    "            labels_tar.append(labels_tar[i])\n",
    "            source_sentences_temp.append(sentences_str)\n",
    "            labels_sec_temp.append(labels_str)\n",
    "    #         break\n",
    "            i+=1\n",
    "\n",
    "        source_sentences.extend(source_sentences_temp)\n",
    "        labels_src.extend(labels_sec_temp)\n",
    "\n",
    "        column_names = [\"source\",\"target\",\"src_tokens\",\"tar_tokens\"]\n",
    "        df = pd.DataFrame(columns=column_names,dtype=object)\n",
    "        df = df.assign(source=source_sentences)\n",
    "        df = df.assign(target = target_sentences)\n",
    "        df = df.assign(src_tokens = labels_src)\n",
    "        df = df.assign(tar_tokens = labels_tar)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    \n",
    "    def random_deletion(self,dataframe, p):\n",
    "        \n",
    "        '''Takes input dataframe created after reading data files and the probabaility for deletion of random tokens in the\n",
    "        source sentences and returns increased dataframe with combined orignal sentences and noisy sentences '''\n",
    "    \n",
    "        source_sentences  = list(dataframe.source)\n",
    "        target_sentences = list(dataframe.target)\n",
    "        labels_src = list(dataframe.src_tokens)\n",
    "        labels_tar = list(dataframe.tar_tokens)\n",
    "        senetences_temp=[]\n",
    "        labels_temp= []\n",
    "        #randomly delete words with probability p\n",
    "        i=0\n",
    "        for sentences, labels in zip(source_sentences,labels_src):\n",
    "            sentences = sentences.split()\n",
    "            labels = labels.split() \n",
    "            source_sentences_temp=[]\n",
    "            labels_sec_temp=[]\n",
    "            if len(sentences) == 1:\n",
    "                pass\n",
    "            for word,label in zip(sentences,labels):\n",
    "                r = random.uniform(0, 1)\n",
    "                if r > p:\n",
    "                    source_sentences_temp.append(word)\n",
    "                    labels_sec_temp.append(label)\n",
    "            if len(source_sentences_temp) == 0: #if you end up deleting all words, just return a random word\n",
    "                rand_int = random.randint(0, len(source_sentences_temp)-1)\n",
    "                source_sentences_temp.append(sentences[rand_int])\n",
    "                labels_sec_temp.append(labels[rand_int])\n",
    "\n",
    "            sentences_str = ' '.join(source_sentences_temp)\n",
    "            labels_str = ' '.join(labels_sec_temp)\n",
    "            senetences_temp.append(sentences_str)\n",
    "            labels_temp.append(labels_str)\n",
    "            target_sentences.append(target_sentences[i])\n",
    "            labels_tar.append(labels_tar[i])\n",
    "    #         break\n",
    "            i+=1\n",
    "        source_sentences.extend(senetences_temp)\n",
    "        labels_src.extend(labels_temp)    \n",
    "        column_names = [\"source\",\"target\",\"src_tokens\",\"tar_tokens\"]\n",
    "        df = pd.DataFrame(columns=column_names,dtype=object)\n",
    "        df = df.assign(source=source_sentences)\n",
    "        df = df.assign(target = target_sentences)\n",
    "        df = df.assign(src_tokens = labels_src)\n",
    "        df = df.assign(tar_tokens = labels_tar)  \n",
    "\n",
    "\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def get_synonyms(self,word):\n",
    "        \n",
    "        '''Helper function for synonym replacement '''\n",
    "        \n",
    "        synonyms = set()\n",
    "        for syn in wordnet.synsets(word): \n",
    "            for l in syn.lemmas(): \n",
    "                synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "                synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "                synonyms.add(synonym) \n",
    "        if word in synonyms:\n",
    "            synonyms.remove(word)\n",
    "        return list(synonyms)\n",
    "    \n",
    "    \n",
    "    def synonym_replacement(self,dataframe, n):\n",
    "    #     new_words = words.copy()\n",
    "        source_sentences  = list(dataframe.source)\n",
    "        target_sentences = list(dataframe.target)\n",
    "        labels_src = list(dataframe.src_tokens)\n",
    "        labels_tar = list(dataframe.tar_tokens)\n",
    "        senetences_temp=[]\n",
    "        labels_temp= []\n",
    "        k=0\n",
    "        for sentences,labels in zip(source_sentences,labels_src):\n",
    "            dict={}\n",
    "            sentences = sentences.split() # ['tarun', 'are', 'bad', 'for','health']\n",
    "            labels = labels.split()#['OK','BAD','OK','OK','OK']\n",
    "            for i, words in enumerate(sentences) :\n",
    "                dict[words] = i                      # {tarun:0, is : 1 , bad: 3 , for : 4, health : 5}\n",
    "            random_word_list = list(set([word for word in sentences if word not in self.stop_words]))\n",
    "            random.shuffle(random_word_list) # ['bad', 'are'...]\n",
    "            num_replaced = 0\n",
    "            for random_word in random_word_list: # ['bad', 'are'...]\n",
    "                synonyms = get_synonyms(random_word)\n",
    "                if len(synonyms) >= 1:\n",
    "                    synonym = random.choice(list(synonyms)) # bad -- > not good\n",
    "                    sentences = [synonym if word == random_word else word for word in sentences] # ['tarun', 'are', 'not good', 'for','health']\n",
    "                    synonym_len = len(synonym.split()) # 2 \n",
    "                    index_random_word = dict[random_word] # 2\n",
    "                    flag=0\n",
    "                    for key,values in dict.items():\n",
    "                        if key == random_word: # {tarun:0, is : 1 , bad: 2 , for : 3, health : 4}\n",
    "                            flag+=1\n",
    "                        if flag ==1:\n",
    "                            dict[key] = values+synonym_len-1 # {tarun:0, is : 1 , bad: 3 , for : 4, health : 5}\n",
    "                    label_synonym = labels[index_random_word] # 2 --> OK\n",
    "                    labels_append_synonym = [label_synonym] * synonym_len # ['OK','OK']\n",
    "                    labels_1 = labels[:index_random_word] # ['OK','BAD']\n",
    "                    labels_2 = labels[index_random_word+1:]#['OK','OK']\n",
    "                    labels_1.extend(labels_append_synonym)# ['OK','BAD','OK','OK']\n",
    "                    labels_1.extend(labels_2)# ['OK','BAD','OK','OK','OK','OK']\n",
    "                    labels = labels_1\n",
    "                    num_replaced += 1\n",
    "\n",
    "                if num_replaced >= n: #only replace up to n words\n",
    "                    break\n",
    "            sentence = ' '.join(sentences)\n",
    "            labels_splt = ' '.join(labels)\n",
    "            senetences_temp.append(sentence)\n",
    "            labels_temp.append(labels_splt)\n",
    "            target_sentences.append(target_sentences[k])\n",
    "            labels_tar.append(labels_tar[k])\n",
    "    #         break\n",
    "            k+=1 \n",
    "\n",
    "        source_sentences.extend(senetences_temp)\n",
    "        labels_src.extend(labels_temp)\n",
    "        column_names = [\"source\",\"target\",\"src_tokens\",\"tar_tokens\"]\n",
    "        df = pd.DataFrame(columns=column_names,dtype=object)\n",
    "        df = df.assign(source=source_sentences)\n",
    "        df = df.assign(target = target_sentences)\n",
    "        df = df.assign(src_tokens = labels_src)\n",
    "        df = df.assign(tar_tokens = labels_tar)\n",
    "\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>src_tokens</th>\n",
       "      <th>tar_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>José Ortega y Gasset visited Husserl at Freibu...</td>\n",
       "      <td>1934 besuchte José Ortega y Gasset Husserl in ...</td>\n",
       "      <td>OK OK OK OK BAD OK OK OK BAD BAD OK</td>\n",
       "      <td>OK BAD OK BAD OK OK OK OK OK OK OK OK BAD OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>however , a disappointing ninth in China meant...</td>\n",
       "      <td>eine enttäuschende Neunte in China bedeutete j...</td>\n",
       "      <td>OK BAD BAD BAD BAD OK OK OK OK OK OK OK OK OK ...</td>\n",
       "      <td>BAD BAD OK BAD OK BAD OK OK OK OK OK OK OK OK ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in his diary , Chase wrote that the release of...</td>\n",
       "      <td>in seinem Tagebuch , Chase schrieb , dass die ...</td>\n",
       "      <td>OK OK OK OK OK BAD OK OK BAD OK OK BAD OK BAD ...</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heavy arquebuses mounted on wagons were called...</td>\n",
       "      <td>schwere Arquebuses auf Waggons montiert wurden...</td>\n",
       "      <td>BAD BAD OK OK BAD BAD BAD OK OK OK OK</td>\n",
       "      <td>OK OK BAD BAD OK OK OK BAD OK OK BAD OK BAD OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>once North Pacific salmon die off after spawni...</td>\n",
       "      <td>sobald der nordpazifische Lachs nach dem Laich...</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK BAD BAD BAD BAD OK...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK BAD ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13995</th>\n",
       "      <td>some may too discourage or disallow insanitary...</td>\n",
       "      <td>einige können auch unhygienische Praktiken wie...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK BAD ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13996</th>\n",
       "      <td>in the late 1860s , the crinolines disappeared...</td>\n",
       "      <td>in den späten 1860er Jahren verschwanden die K...</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK BAD BAD BAD OK OK ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13997</th>\n",
       "      <td>disco was knock as mindless , consumerist , ov...</td>\n",
       "      <td>Disco wurde als geistlos , konsumistisch , übe...</td>\n",
       "      <td>OK OK OK OK BAD BAD OK OK OK BAD OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13998</th>\n",
       "      <td>planters would then satisfy large hogsheads wi...</td>\n",
       "      <td>die Pflanzer würden dann große Heuschrecken mi...</td>\n",
       "      <td>OK OK BAD OK OK BAD OK OK OK OK OK BAD BAD BAD...</td>\n",
       "      <td>OK OK OK OK OK BAD OK OK OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13999</th>\n",
       "      <td>he trend krishna 's most dangerous enemy , Jar...</td>\n",
       "      <td>er tötete Krishnas gefährlichsten Feind Jarasa...</td>\n",
       "      <td>OK BAD OK OK OK OK OK OK OK OK OK BAD BAD OK O...</td>\n",
       "      <td>OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  source  \\\n",
       "0      José Ortega y Gasset visited Husserl at Freibu...   \n",
       "1      however , a disappointing ninth in China meant...   \n",
       "2      in his diary , Chase wrote that the release of...   \n",
       "3      Heavy arquebuses mounted on wagons were called...   \n",
       "4      once North Pacific salmon die off after spawni...   \n",
       "...                                                  ...   \n",
       "13995  some may too discourage or disallow insanitary...   \n",
       "13996  in the late 1860s , the crinolines disappeared...   \n",
       "13997  disco was knock as mindless , consumerist , ov...   \n",
       "13998  planters would then satisfy large hogsheads wi...   \n",
       "13999  he trend krishna 's most dangerous enemy , Jar...   \n",
       "\n",
       "                                                  target  \\\n",
       "0      1934 besuchte José Ortega y Gasset Husserl in ...   \n",
       "1      eine enttäuschende Neunte in China bedeutete j...   \n",
       "2      in seinem Tagebuch , Chase schrieb , dass die ...   \n",
       "3      schwere Arquebuses auf Waggons montiert wurden...   \n",
       "4      sobald der nordpazifische Lachs nach dem Laich...   \n",
       "...                                                  ...   \n",
       "13995  einige können auch unhygienische Praktiken wie...   \n",
       "13996  in den späten 1860er Jahren verschwanden die K...   \n",
       "13997  Disco wurde als geistlos , konsumistisch , übe...   \n",
       "13998  die Pflanzer würden dann große Heuschrecken mi...   \n",
       "13999  er tötete Krishnas gefährlichsten Feind Jarasa...   \n",
       "\n",
       "                                              src_tokens  \\\n",
       "0                    OK OK OK OK BAD OK OK OK BAD BAD OK   \n",
       "1      OK BAD BAD BAD BAD OK OK OK OK OK OK OK OK OK ...   \n",
       "2      OK OK OK OK OK BAD OK OK BAD OK OK BAD OK BAD ...   \n",
       "3                  BAD BAD OK OK BAD BAD BAD OK OK OK OK   \n",
       "4      OK OK OK OK OK OK OK BAD OK BAD BAD BAD BAD OK...   \n",
       "...                                                  ...   \n",
       "13995  OK OK OK OK OK OK OK OK OK OK OK OK OK OK BAD ...   \n",
       "13996  OK OK OK OK OK OK OK BAD OK BAD BAD BAD OK OK ...   \n",
       "13997             OK OK OK OK BAD BAD OK OK OK BAD OK OK   \n",
       "13998  OK OK BAD OK OK BAD OK OK OK OK OK BAD BAD BAD...   \n",
       "13999  OK BAD OK OK OK OK OK OK OK OK OK BAD BAD OK O...   \n",
       "\n",
       "                                              tar_tokens  \n",
       "0      OK BAD OK BAD OK OK OK OK OK OK OK OK BAD OK O...  \n",
       "1      BAD BAD OK BAD OK BAD OK OK OK OK OK OK OK OK ...  \n",
       "2      OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...  \n",
       "3      OK OK BAD BAD OK OK OK BAD OK OK BAD OK BAD OK...  \n",
       "4      OK OK OK OK OK OK OK OK OK OK OK OK OK OK BAD ...  \n",
       "...                                                  ...  \n",
       "13995  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "13996  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK B...  \n",
       "13997  OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...  \n",
       "13998  OK OK OK OK OK BAD OK OK OK OK OK BAD OK OK OK...  \n",
       "13999  OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...  \n",
       "\n",
       "[14000 rows x 4 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataObj = loadDatafromFile(config.filePath_src,config.filePath_tar, config.filePath_srcTags,config.filePath_tarTags)\n",
    "df= dataObj.createDf() \n",
    "augObj = DataAugmentation()\n",
    "augObj.synonym_replacement(df,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "augObj = DataAugmentation()\n",
    "swapped_df = augObj.random_swap(df,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>src_tokens</th>\n",
       "      <th>tar_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>José Ortega y Gasset visited Husserl at Freibu...</td>\n",
       "      <td>1934 besuchte José Ortega y Gasset Husserl in ...</td>\n",
       "      <td>OK OK OK OK BAD OK OK OK BAD BAD OK</td>\n",
       "      <td>OK BAD OK BAD OK OK OK OK OK OK OK OK BAD OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>however , a disappointing ninth in China meant...</td>\n",
       "      <td>eine enttäuschende Neunte in China bedeutete j...</td>\n",
       "      <td>OK BAD BAD BAD BAD OK OK OK OK OK OK OK OK OK ...</td>\n",
       "      <td>BAD BAD OK BAD OK BAD OK OK OK OK OK OK OK OK ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in his diary , Chase wrote that the release of...</td>\n",
       "      <td>in seinem Tagebuch , Chase schrieb , dass die ...</td>\n",
       "      <td>OK OK OK OK OK BAD OK OK BAD OK OK BAD OK BAD ...</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heavy arquebuses mounted on wagons were called...</td>\n",
       "      <td>schwere Arquebuses auf Waggons montiert wurden...</td>\n",
       "      <td>BAD BAD OK OK BAD BAD BAD OK OK OK OK</td>\n",
       "      <td>OK OK BAD BAD OK OK OK BAD OK OK BAD OK BAD OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>once North Pacific salmon die off after spawni...</td>\n",
       "      <td>sobald der nordpazifische Lachs nach dem Laich...</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK BAD BAD BAD BAD OK...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK BAD ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13995</th>\n",
       "      <td>the may also on or disallow unsanitary practic...</td>\n",
       "      <td>einige können auch unhygienische Praktiken wie...</td>\n",
       "      <td>BAD OK OK OK OK OK OK OK OK OK OK OK OK OK OK ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13996</th>\n",
       "      <td>in 1870s late 1860s , the the disappeared and ...</td>\n",
       "      <td>in den späten 1860er Jahren verschwanden die K...</td>\n",
       "      <td>OK OK OK OK OK OK BAD BAD OK OK BAD BAD OK OK ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13997</th>\n",
       "      <td>criticized was disco as mindless , consumerist...</td>\n",
       "      <td>Disco wurde als geistlos , konsumistisch , übe...</td>\n",
       "      <td>OK OK OK OK BAD BAD OK OK OK BAD OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13998</th>\n",
       "      <td>planters would and fill large tobacco with hog...</td>\n",
       "      <td>die Pflanzer würden dann große Heuschrecken mi...</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD BAD OK BAD BAD BAD BA...</td>\n",
       "      <td>OK OK OK OK OK BAD OK OK OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13999</th>\n",
       "      <td>he Kichaka Krishna 's most dangerous enemy , i...</td>\n",
       "      <td>er tötete Krishnas gefährlichsten Feind Jarasa...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK BAD BAD OK OK...</td>\n",
       "      <td>OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  source  \\\n",
       "0      José Ortega y Gasset visited Husserl at Freibu...   \n",
       "1      however , a disappointing ninth in China meant...   \n",
       "2      in his diary , Chase wrote that the release of...   \n",
       "3      Heavy arquebuses mounted on wagons were called...   \n",
       "4      once North Pacific salmon die off after spawni...   \n",
       "...                                                  ...   \n",
       "13995  the may also on or disallow unsanitary practic...   \n",
       "13996  in 1870s late 1860s , the the disappeared and ...   \n",
       "13997  criticized was disco as mindless , consumerist...   \n",
       "13998  planters would and fill large tobacco with hog...   \n",
       "13999  he Kichaka Krishna 's most dangerous enemy , i...   \n",
       "\n",
       "                                                  target  \\\n",
       "0      1934 besuchte José Ortega y Gasset Husserl in ...   \n",
       "1      eine enttäuschende Neunte in China bedeutete j...   \n",
       "2      in seinem Tagebuch , Chase schrieb , dass die ...   \n",
       "3      schwere Arquebuses auf Waggons montiert wurden...   \n",
       "4      sobald der nordpazifische Lachs nach dem Laich...   \n",
       "...                                                  ...   \n",
       "13995  einige können auch unhygienische Praktiken wie...   \n",
       "13996  in den späten 1860er Jahren verschwanden die K...   \n",
       "13997  Disco wurde als geistlos , konsumistisch , übe...   \n",
       "13998  die Pflanzer würden dann große Heuschrecken mi...   \n",
       "13999  er tötete Krishnas gefährlichsten Feind Jarasa...   \n",
       "\n",
       "                                              src_tokens  \\\n",
       "0                    OK OK OK OK BAD OK OK OK BAD BAD OK   \n",
       "1      OK BAD BAD BAD BAD OK OK OK OK OK OK OK OK OK ...   \n",
       "2      OK OK OK OK OK BAD OK OK BAD OK OK BAD OK BAD ...   \n",
       "3                  BAD BAD OK OK BAD BAD BAD OK OK OK OK   \n",
       "4      OK OK OK OK OK OK OK BAD OK BAD BAD BAD BAD OK...   \n",
       "...                                                  ...   \n",
       "13995  BAD OK OK OK OK OK OK OK OK OK OK OK OK OK OK ...   \n",
       "13996  OK OK OK OK OK OK BAD BAD OK OK BAD BAD OK OK ...   \n",
       "13997             OK OK OK OK BAD BAD OK OK OK BAD OK OK   \n",
       "13998  OK OK OK OK OK OK OK BAD BAD OK BAD BAD BAD BA...   \n",
       "13999  OK OK OK OK OK OK OK OK OK OK OK BAD BAD OK OK...   \n",
       "\n",
       "                                              tar_tokens  \n",
       "0      OK BAD OK BAD OK OK OK OK OK OK OK OK BAD OK O...  \n",
       "1      BAD BAD OK BAD OK BAD OK OK OK OK OK OK OK OK ...  \n",
       "2      OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...  \n",
       "3      OK OK BAD BAD OK OK OK BAD OK OK BAD OK BAD OK...  \n",
       "4      OK OK OK OK OK OK OK OK OK OK OK OK OK OK BAD ...  \n",
       "...                                                  ...  \n",
       "13995  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "13996  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK B...  \n",
       "13997  OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...  \n",
       "13998  OK OK OK OK OK BAD OK OK OK OK OK BAD OK OK OK...  \n",
       "13999  OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...  \n",
       "\n",
       "[14000 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swapped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>src_tokens</th>\n",
       "      <th>tar_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>José Ortega y Gasset visited Husserl at Freibu...</td>\n",
       "      <td>1934 besuchte José Ortega y Gasset Husserl in ...</td>\n",
       "      <td>OK OK OK OK BAD OK OK OK BAD BAD OK</td>\n",
       "      <td>OK BAD OK BAD OK OK OK OK OK OK OK OK BAD OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>however , a disappointing ninth in China meant...</td>\n",
       "      <td>eine enttäuschende Neunte in China bedeutete j...</td>\n",
       "      <td>OK BAD BAD BAD BAD OK OK OK OK OK OK OK OK OK ...</td>\n",
       "      <td>BAD BAD OK BAD OK BAD OK OK OK OK OK OK OK OK ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in his diary , Chase wrote that the release of...</td>\n",
       "      <td>in seinem Tagebuch , Chase schrieb , dass die ...</td>\n",
       "      <td>OK OK OK OK OK BAD OK OK BAD OK OK BAD OK BAD ...</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heavy arquebuses mounted on wagons were called...</td>\n",
       "      <td>schwere Arquebuses auf Waggons montiert wurden...</td>\n",
       "      <td>BAD BAD OK OK BAD BAD BAD OK OK OK OK</td>\n",
       "      <td>OK OK BAD BAD OK OK OK BAD OK OK BAD OK BAD OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>once North Pacific salmon die off after spawni...</td>\n",
       "      <td>sobald der nordpazifische Lachs nach dem Laich...</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK BAD BAD BAD BAD OK...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK BAD ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13995</th>\n",
       "      <td>some may also or disallow unsanitary such as k...</td>\n",
       "      <td>einige können auch unhygienische Praktiken wie...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK BAD OK</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13996</th>\n",
       "      <td>the late , the crinolines disappeared and the ...</td>\n",
       "      <td>in den späten 1860er Jahren verschwanden die K...</td>\n",
       "      <td>OK OK OK OK OK BAD OK BAD BAD OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13997</th>\n",
       "      <td>disco criticized as , consumerist , and escapi...</td>\n",
       "      <td>Disco wurde als geistlos , konsumistisch , übe...</td>\n",
       "      <td>OK OK OK BAD OK OK BAD OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13998</th>\n",
       "      <td>planters would then hogsheads with tobacco con...</td>\n",
       "      <td>die Pflanzer würden dann große Heuschrecken mi...</td>\n",
       "      <td>OK OK BAD BAD OK OK OK BAD BAD BAD OK</td>\n",
       "      <td>OK OK OK OK OK BAD OK OK OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13999</th>\n",
       "      <td>slew Krishna 's dangerous enemy , Jarasandha ,...</td>\n",
       "      <td>er tötete Krishnas gefährlichsten Feind Jarasa...</td>\n",
       "      <td>BAD OK OK OK OK OK OK OK OK BAD OK BAD OK OK O...</td>\n",
       "      <td>OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  source  \\\n",
       "0      José Ortega y Gasset visited Husserl at Freibu...   \n",
       "1      however , a disappointing ninth in China meant...   \n",
       "2      in his diary , Chase wrote that the release of...   \n",
       "3      Heavy arquebuses mounted on wagons were called...   \n",
       "4      once North Pacific salmon die off after spawni...   \n",
       "...                                                  ...   \n",
       "13995  some may also or disallow unsanitary such as k...   \n",
       "13996  the late , the crinolines disappeared and the ...   \n",
       "13997  disco criticized as , consumerist , and escapi...   \n",
       "13998  planters would then hogsheads with tobacco con...   \n",
       "13999  slew Krishna 's dangerous enemy , Jarasandha ,...   \n",
       "\n",
       "                                                  target  \\\n",
       "0      1934 besuchte José Ortega y Gasset Husserl in ...   \n",
       "1      eine enttäuschende Neunte in China bedeutete j...   \n",
       "2      in seinem Tagebuch , Chase schrieb , dass die ...   \n",
       "3      schwere Arquebuses auf Waggons montiert wurden...   \n",
       "4      sobald der nordpazifische Lachs nach dem Laich...   \n",
       "...                                                  ...   \n",
       "13995  einige können auch unhygienische Praktiken wie...   \n",
       "13996  in den späten 1860er Jahren verschwanden die K...   \n",
       "13997  Disco wurde als geistlos , konsumistisch , übe...   \n",
       "13998  die Pflanzer würden dann große Heuschrecken mi...   \n",
       "13999  er tötete Krishnas gefährlichsten Feind Jarasa...   \n",
       "\n",
       "                                              src_tokens  \\\n",
       "0                    OK OK OK OK BAD OK OK OK BAD BAD OK   \n",
       "1      OK BAD BAD BAD BAD OK OK OK OK OK OK OK OK OK ...   \n",
       "2      OK OK OK OK OK BAD OK OK BAD OK OK BAD OK BAD ...   \n",
       "3                  BAD BAD OK OK BAD BAD BAD OK OK OK OK   \n",
       "4      OK OK OK OK OK OK OK BAD OK BAD BAD BAD BAD OK...   \n",
       "...                                                  ...   \n",
       "13995               OK OK OK OK OK OK OK OK OK OK BAD OK   \n",
       "13996          OK OK OK OK OK BAD OK BAD BAD OK OK OK OK   \n",
       "13997                       OK OK OK BAD OK OK BAD OK OK   \n",
       "13998              OK OK BAD BAD OK OK OK BAD BAD BAD OK   \n",
       "13999  BAD OK OK OK OK OK OK OK OK BAD OK BAD OK OK O...   \n",
       "\n",
       "                                              tar_tokens  \n",
       "0      OK BAD OK BAD OK OK OK OK OK OK OK OK BAD OK O...  \n",
       "1      BAD BAD OK BAD OK BAD OK OK OK OK OK OK OK OK ...  \n",
       "2      OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...  \n",
       "3      OK OK BAD BAD OK OK OK BAD OK OK BAD OK BAD OK...  \n",
       "4      OK OK OK OK OK OK OK OK OK OK OK OK OK OK BAD ...  \n",
       "...                                                  ...  \n",
       "13995  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "13996  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK B...  \n",
       "13997  OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...  \n",
       "13998  OK OK OK OK OK BAD OK OK OK OK OK BAD OK OK OK...  \n",
       "13999  OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...  \n",
       "\n",
       "[14000 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augObj.random_deletion(df,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def synonym_replacement(dataframe, n):\n",
    "# #     new_words = words.copy()\n",
    "#     source_sentences  = list(dataframe.source)\n",
    "#     target_sentences = list(dataframe.target)\n",
    "#     labels_src = list(dataframe.src_tokens)\n",
    "#     labels_tar = list(dataframe.tar_tokens)\n",
    "#     senetences_temp=[]\n",
    "#     labels_temp= []\n",
    "#     k=0\n",
    "#     for sentences,labels in zip(source_sentences,labels_src):\n",
    "#         dict={}\n",
    "#         sentences = sentences.split() # ['tarun', 'are', 'bad', 'for','health']\n",
    "#         labels = labels.split()#['OK','BAD','OK','OK','OK']\n",
    "#         for i, words in enumerate(sentences) :\n",
    "#             dict[words] = i                      # {tarun:0, is : 1 , bad: 3 , for : 4, health : 5}\n",
    "#         random_word_list = list(set([word for word in sentences if word not in config.stop_words]))\n",
    "#         random.shuffle(random_word_list) # ['bad', 'are'...]\n",
    "#         num_replaced = 0\n",
    "#         for random_word in random_word_list: # ['bad', 'are'...]\n",
    "#             synonyms = get_synonyms(random_word)\n",
    "#             if len(synonyms) >= 1:\n",
    "#                 synonym = random.choice(list(synonyms)) # bad -- > not good\n",
    "#                 sentences = [synonym if word == random_word else word for word in sentences] # ['tarun', 'are', 'not good', 'for','health']\n",
    "#                 synonym_len = len(synonym.split()) # 2 \n",
    "#                 index_random_word = dict[random_word] # 2\n",
    "#                 flag=0\n",
    "#                 for key,values in dict.items():\n",
    "#                     if key == random_word: # {tarun:0, is : 1 , bad: 2 , for : 3, health : 4}\n",
    "#                         flag+=1\n",
    "#                     if flag ==1:\n",
    "#                         dict[key] = values+synonym_len-1 # {tarun:0, is : 1 , bad: 3 , for : 4, health : 5}\n",
    "#                 label_synonym = labels[index_random_word] # 2 --> OK\n",
    "#                 labels_append_synonym = [label_synonym] * synonym_len # ['OK','OK']\n",
    "#                 labels_1 = labels[:index_random_word] # ['OK','BAD']\n",
    "#                 labels_2 = labels[index_random_word+1:]#['OK','OK']\n",
    "#                 labels_1.extend(labels_append_synonym)# ['OK','BAD','OK','OK']\n",
    "#                 labels_1.extend(labels_2)# ['OK','BAD','OK','OK','OK','OK']\n",
    "#                 labels = labels_1\n",
    "#                 num_replaced += 1\n",
    "  \n",
    "#             if num_replaced >= n: #only replace up to n words\n",
    "#                 break\n",
    "#         sentence = ' '.join(sentences)\n",
    "#         labels_splt = ' '.join(labels)\n",
    "#         senetences_temp.append(sentence)\n",
    "#         labels_temp.append(labels_splt)\n",
    "#         target_sentences.append(target_sentences[k])\n",
    "#         labels_tar.append(labels_tar[k])\n",
    "# #         break\n",
    "#         k+=1 \n",
    "         \n",
    "#     source_sentences.extend(senetences_temp)\n",
    "#     labels_src.extend(labels_temp)\n",
    "#     column_names = [\"source\",\"target\",\"src_tokens\",\"tar_tokens\"]\n",
    "#     df = pd.DataFrame(columns=column_names,dtype=object)\n",
    "#     df = df.assign(source=source_sentences)\n",
    "#     df = df.assign(target = target_sentences)\n",
    "#     df = df.assign(src_tokens = labels_src)\n",
    "#     df = df.assign(tar_tokens = labels_tar)\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def get_synonyms(word):\n",
    "#     synonyms = set()\n",
    "#     for syn in wordnet.synsets(word): \n",
    "#         for l in syn.lemmas(): \n",
    "#             synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "#             synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "#             synonyms.add(synonym) \n",
    "#     if word in synonyms:\n",
    "#         synonyms.remove(word)\n",
    "#     return list(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/pytorch-1.6-gpu-py36-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
