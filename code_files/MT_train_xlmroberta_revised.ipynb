{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers) (4.4.0)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2022.4.24-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.25.1)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Using cached tokenizers-0.12.1-cp36-cp36m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.4.1-py3-none-any.whl (9.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Using cached huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.51.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Installing collected packages: regex, filelock, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.4.1 huggingface-hub-0.4.0 regex-2022.4.24 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.96-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.96\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.6.7-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from nltk) (4.51.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.6/site-packages (from nltk) (2022.4.24)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.6.7\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-7.7.0-py2.py3-none-any.whl (123 kB)\n",
      "Collecting nbformat>=4.2.0\n",
      "  Using cached nbformat-5.1.3-py3-none-any.whl (178 kB)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.6/site-packages (from ipywidgets) (4.3.3)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.6/site-packages (from ipywidgets) (7.16.3)\n",
      "Collecting widgetsnbextension~=3.6.0\n",
      "  Using cached widgetsnbextension-3.6.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting jupyterlab-widgets>=1.0.0\n",
      "  Using cached jupyterlab_widgets-1.1.0-py3-none-any.whl (245 kB)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.6/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.6/site-packages (from ipywidgets) (5.5.6)\n",
      "Requirement already satisfied: jupyter-client in /opt/conda/lib/python3.6/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.1.2)\n",
      "Requirement already satisfied: tornado>=4.2 in /opt/conda/lib/python3.6/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: pexpect in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.28)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (2.11.2)\n",
      "Collecting jedi<=0.17.2,>=0.10\n",
      "  Using cached jedi-0.17.2-py2.py3-none-any.whl (1.4 MB)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (59.3.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Collecting parso<0.8.0,>=0.7.0\n",
      "  Using cached parso-0.7.1-py2.py3-none-any.whl (109 kB)\n",
      "Collecting jsonschema!=2.5.0,>=2.4\n",
      "  Using cached jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.6/site-packages (from nbformat>=4.2.0->ipywidgets) (4.9.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (21.2.0)\n",
      "Collecting pyrsistent>=0.14.0\n",
      "  Using cached pyrsistent-0.18.0-cp36-cp36m-manylinux1_x86_64.whl (117 kB)\n",
      "Requirement already satisfied: six>=1.11.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (4.4.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Collecting notebook>=4.4.1\n",
      "  Using cached notebook-6.4.10-py3-none-any.whl (9.9 MB)\n",
      "Collecting Send2Trash>=1.8.0\n",
      "  Using cached Send2Trash-1.8.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.1)\n",
      "Collecting prometheus-client\n",
      "  Using cached prometheus_client-0.14.1-py3-none-any.whl (59 kB)\n",
      "Collecting argon2-cffi\n",
      "  Using cached argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\n",
      "Collecting nbconvert>=5\n",
      "  Using cached nbconvert-6.0.7-py3-none-any.whl (552 kB)\n",
      "Requirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (22.1.0)\n",
      "Collecting terminado>=0.8.3\n",
      "  Using cached terminado-0.12.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.6/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
      "Collecting mistune<2,>=0.8.1\n",
      "  Using cached mistune-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting nbclient<0.6.0,>=0.5.0\n",
      "  Using cached nbclient-0.5.9-py3-none-any.whl (69 kB)\n",
      "Collecting bleach\n",
      "  Using cached bleach-4.1.0-py2.py3-none-any.whl (157 kB)\n",
      "Collecting pandocfilters>=1.4.1\n",
      "  Using cached pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting defusedxml\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting testpath\n",
      "  Using cached testpath-0.6.0-py3-none-any.whl (83 kB)\n",
      "Collecting jupyterlab-pygments\n",
      "  Using cached jupyterlab_pygments-0.1.2-py2.py3-none-any.whl (4.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.6/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.0.1)\n",
      "Collecting async-generator\n",
      "  Using cached async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: ptyprocess in /opt/conda/lib/python3.6/site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.10.0.0)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8)\n",
      "Collecting argon2-cffi-bindings\n",
      "  Using cached argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.20)\n",
      "Collecting webencodings\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (20.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.4.7)\n",
      "Installing collected packages: pyrsistent, parso, jsonschema, webencodings, nbformat, jedi, async-generator, testpath, pandocfilters, nbclient, mistune, jupyterlab-pygments, defusedxml, bleach, argon2-cffi-bindings, terminado, Send2Trash, prometheus-client, nbconvert, argon2-cffi, notebook, widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "  Attempting uninstall: parso\n",
      "    Found existing installation: parso 0.8.0\n",
      "    Uninstalling parso-0.8.0:\n",
      "      Successfully uninstalled parso-0.8.0\n",
      "  Attempting uninstall: jedi\n",
      "    Found existing installation: jedi 0.18.0\n",
      "    Uninstalling jedi-0.18.0:\n",
      "      Successfully uninstalled jedi-0.18.0\n",
      "Successfully installed Send2Trash-1.8.0 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 async-generator-1.10 bleach-4.1.0 defusedxml-0.7.1 ipywidgets-7.7.0 jedi-0.17.2 jsonschema-3.2.0 jupyterlab-pygments-0.1.2 jupyterlab-widgets-1.1.0 mistune-0.8.4 nbclient-0.5.9 nbconvert-6.0.7 nbformat-5.1.3 notebook-6.4.10 pandocfilters-1.5.0 parso-0.7.1 prometheus-client-0.14.1 pyrsistent-0.18.0 terminado-0.12.1 testpath-0.6.0 webencodings-0.5.1 widgetsnbextension-3.6.0\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting tensorboard\n",
      "  Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard) (1.19.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard) (3.17.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard) (2.0.1)\n",
      "Collecting grpcio>=1.24.3\n",
      "  Using cached grpcio-1.44.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorboard) (0.35.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard) (59.3.0)\n",
      "Collecting absl-py>=0.4\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from absl-py>=0.4->tensorboard) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.7.2)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (4.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.10.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.11)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2021.5.30)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from werkzeug>=0.11.15->tensorboard) (0.8)\n",
      "Installing collected packages: pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, absl-py, tensorboard\n",
      "Successfully installed absl-py-1.0.0 cachetools-4.2.4 google-auth-2.6.6 google-auth-oauthlib-0.4.6 grpcio-1.44.0 markdown-3.3.6 oauthlib-3.2.0 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install nltk\n",
    "!pip install ipywidgets\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import joblib\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "# from DataAugmentation \n",
    "from data_utils import loadDatafromFile,createTokenizedDf,CompDataset,createkfoldData,createDataloaders\n",
    "from data_utils_unlabelled import loadUnlabelledDatafromFile,createTokenizedDfUnlabelled,CompDatasetUnlabelled\n",
    "from DataAugmentation import DataAugmentation\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from importlib import reload\n",
    "import config\n",
    "from transformers import XLMRobertaForTokenClassification, XLMRobertaConfig ,BertModel, XLMRobertaTokenizer, XLMRobertaModel, BertForTokenClassification\n",
    "from seqeval.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import matthews_corrcoef,classification_report,f1_score\n",
    "import engine\n",
    "# from model_new import EntityModel\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from  loss_functions import ClassificationCost, ConsistencyCost, update_teacher_params\n",
    "from utils_functions import set_seed, sigmoid_rampup, get_consistency_weight, EarlyStopping\n",
    "from model_xlmroberta import EntityModel , GaussianNoise\n",
    "from DataAugmentation import DataAugmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data_utils\n",
    "# data_utils = reload(data_utils)\n",
    "# from data_utils import loadDatafromFile,createTokenizedDf,CompDataset,createkfoldData,createDataloaders\n",
    "# import data_utils_unlabelled\n",
    "# data_utils_unlabelled = reload(data_utils_unlabelled)\n",
    "# from data_utils_unlabelled import loadUnlabelledDatafromFile,createTokenizedDfUnlabelled,CompDatasetUnlabelled, createkfoldData,createDataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config= reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.tensorboard.summary import hparams\n",
    "\n",
    "class SummaryWriter(SummaryWriter):\n",
    "    def add_hparams(self, hparam_dict, metric_dict):\n",
    "        torch._C._log_api_usage_once(\"tensorboard.logging.add_hparams\")\n",
    "        if type(hparam_dict) is not dict or type(metric_dict) is not dict:\n",
    "            raise TypeError('hparam_dict and metric_dict should be dictionary.')\n",
    "        exp, ssi, sei = hparams(hparam_dict, metric_dict)\n",
    "\n",
    "        logdir = self._get_file_writer().get_logdir()\n",
    "        \n",
    "        with SummaryWriter(log_dir=logdir) as w_hp:\n",
    "            w_hp.file_writer.add_summary(exp)\n",
    "            w_hp.file_writer.add_summary(ssi)\n",
    "            w_hp.file_writer.add_summary(sei)\n",
    "            for k, v in metric_dict.items():\n",
    "                w_hp.add_scalar(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_labelled(filePath_src,filePath_tar, filePath_srcTags,filePath_tarTags,\n",
    "                 filePath_src_eval,filePath_tar_eval, filePath_srcTags_eval,filePath_tarTags_eval,train_data_needed,\n",
    "                          unlabel_data_needed,total_train_data=2000, model_type='xlm'):\n",
    "    \n",
    "    set_seed()\n",
    "    dataObjTrain = loadDatafromFile(filePath_src,filePath_tar, filePath_srcTags,filePath_tarTags)\n",
    "    df_train= dataObjTrain.createDf() # get dataframe from files    \n",
    "    \n",
    "    dataObjEval = loadDatafromFile(filePath_src_eval,filePath_tar_eval, filePath_srcTags_eval,filePath_tarTags_eval)\n",
    "    df_test = dataObjEval.createDf()\n",
    "#     df_combined = df_train.append(df_eval, ignore_index=True)\n",
    "#     df_combined.reset_index(drop=True)\n",
    "    split_ratio_len = int(len(df_train) - config.test_split_ratio*len(df_train))# 7k - 2450 = 4550\n",
    "   \n",
    "    df_train_splitted = df_train.iloc[:split_ratio_len,:] #[:4550]\n",
    "    df_eval_splitted = df_train.iloc[split_ratio_len:,:] #[4550:] \n",
    "    df_train_splitted = df_train_splitted[:total_train_data]#2000,3000\n",
    "    df_train_splitted_ = df_train_splitted[:train_data_needed] # amount of labeled data needed for training out of 2000 for eg. 250\n",
    "    eval_data_needed_ssl = int(0.25 * (train_data_needed+unlabel_data_needed)) # 25 % reserve for validation -->500 for 2k data train\n",
    "    eval_data_needed_sl = int(0.25*train_data_needed)\n",
    "    df_eval_splitted_ssl = df_eval_splitted[:eval_data_needed_ssl] # fix this 500 for all experiments\n",
    "    df_eval_splitted_sl = df_eval_splitted[:eval_data_needed_sl] \n",
    "    \n",
    "#     df_train_splitted_ = df_train_splitted[:1000]\n",
    "    df_unlabel_splitted_train = df_train_splitted[train_data_needed:] # get remaining data in train df to be used as unlabeled\n",
    "    df_unlabel_splitted_train = df_unlabel_splitted_train.reset_index(drop=True)\n",
    "#     df_eval_splitted = df_eval_splitted[:500]\n",
    "\n",
    "#     df_unlabel_splitted_test = df_eval_splitted[eval_data_needed:]# get remaining data in eval df to use for unlabel\n",
    "#     df_unlabel_splitted_test=df_unlabel_splitted_test.reset_index(drop=True)\n",
    "    \n",
    "#     df_unlabeled = df_unlabel_splitted_train.append(df_unlabel_splitted_test, ignore_index = True) # combine both unlabel df's\n",
    "    \n",
    "    obj_tokenized_train = createTokenizedDf(df_train_splitted_,model_type)\n",
    "    obj_tokenized_eval_ssl = createTokenizedDf(df_eval_splitted_ssl,model_type)\n",
    "    obj_tokenized_eval_sl = createTokenizedDf(df_eval_splitted_sl,model_type)\n",
    "    obj_tokenized_test = createTokenizedDf(df_test,model_type)\n",
    "    \n",
    "    \n",
    "    df_new_train = obj_tokenized_train.convertDf()\n",
    "    df_new_eval_ssl = obj_tokenized_eval_ssl.convertDf()\n",
    "    df_new_eval_sl = obj_tokenized_eval_sl.convertDf()\n",
    "    df_new_test = obj_tokenized_test.convertDf()\n",
    "    \n",
    "    train_data = CompDataset(df_new_train,model_type)\n",
    "    eval_data_ssl = CompDataset(df_new_eval_ssl,model_type)\n",
    "    eval_data_sl = CompDataset(df_new_eval_sl,model_type)\n",
    "    test_data = CompDataset(df_new_test,model_type)\n",
    "    \n",
    "    return train_data, eval_data_ssl, eval_data_sl, test_data, df_unlabel_splitted_train,df_train_splitted_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_data_labelled(filePath_src,filePath_tar, filePath_srcTags,filePath_tarTags,\n",
    "#                  filePath_src_eval,filePath_tar_eval, filePath_srcTags_eval,filePath_tarTags_eval,train_data_needed,\n",
    "#                           unlabel_data_needed,total_train_data=2000, model_type='xlm'):\n",
    "    \n",
    "#     set_seed()\n",
    "#     dataObjTrain = loadDatafromFile(filePath_src,filePath_tar, filePath_srcTags,filePath_tarTags)\n",
    "#     df_train= dataObjTrain.createDf() # get dataframe from files    \n",
    "    \n",
    "    \n",
    "#     dataObjEval = loadDatafromFile(filePath_src_eval,filePath_tar_eval, filePath_srcTags_eval,filePath_tarTags_eval)\n",
    "#     df_test = dataObjEval.createDf()\n",
    "\n",
    "#     split_ratio_len = int(len(df_train) - config.test_split_ratio*len(df_train))# 7k - 2450 = 4550\n",
    "   \n",
    "#     df_train_splitted = df_train.iloc[:split_ratio_len,:] #[:4550]\n",
    "#     df_eval_splitted = df_train.iloc[split_ratio_len:,:] #[4550:] \n",
    "#     df_eval_splitted = df_eval_splitted.reset_index(drop=True)\n",
    "# #     df_train_splitted = df_train_splitted[:total_train_data]#2000,3000\n",
    "#     df_train_splitted=df_train_splitted.sample(total_train_data, replace=True)\n",
    "#     df_train_splitted=df_train_splitted.reset_index(drop=True) #choose randomly 2000 data\n",
    "    \n",
    "#     df_train_splitted_ = df_train_splitted[:train_data_needed] # amount of labeled data needed for training out of 2000 for eg. 250\n",
    "#     eval_data_needed_ssl = int(0.25 * (train_data_needed+unlabel_data_needed)) # 25 % reserve for validation -->500 for 2k data train\n",
    "#     eval_data_needed_sl = int(0.25*train_data_needed)\n",
    "#     df_eval_splitted_ssl = df_eval_splitted[:eval_data_needed_ssl] # fix this 500 for all experiments\n",
    "#     df_eval_splitted_sl = df_eval_splitted[:eval_data_needed_sl] \n",
    "    \n",
    "# #     df_train_splitted_ = df_train_splitted[:1000]\n",
    "#     df_unlabel_splitted_train = df_train_splitted[train_data_needed:] # get remaining data in train df to be used as unlabeled\n",
    "#     df_unlabel_splitted_train = df_unlabel_splitted_train.reset_index(drop=True)\n",
    "# #     df_eval_splitted = df_eval_splitted[:500]\n",
    "\n",
    "# #     df_unlabel_splitted_test = df_eval_splitted[eval_data_needed:]# get remaining data in eval df to use for unlabel\n",
    "# #     df_unlabel_splitted_test=df_unlabel_splitted_test.reset_index(drop=True)\n",
    "    \n",
    "# #     df_unlabeled = df_unlabel_splitted_train.append(df_unlabel_splitted_test, ignore_index = True) # combine both unlabel df's\n",
    "    \n",
    "#     obj_tokenized_train = createTokenizedDf(df_train_splitted_,model_type)\n",
    "#     obj_tokenized_eval_ssl = createTokenizedDf(df_eval_splitted_ssl,model_type)\n",
    "#     obj_tokenized_eval_sl = createTokenizedDf(df_eval_splitted_sl,model_type)\n",
    "#     obj_tokenized_test = createTokenizedDf(df_test,model_type)\n",
    "    \n",
    "    \n",
    "#     df_new_train = obj_tokenized_train.convertDf()\n",
    "#     df_new_eval_ssl = obj_tokenized_eval_ssl.convertDf()\n",
    "#     df_new_eval_sl = obj_tokenized_eval_sl.convertDf()\n",
    "#     df_new_test = obj_tokenized_test.convertDf()\n",
    "    \n",
    "#     train_data = CompDataset(df_new_train,model_type)\n",
    "#     eval_data_ssl = CompDataset(df_new_eval_ssl,model_type)\n",
    "#     eval_data_sl = CompDataset(df_new_eval_sl,model_type)\n",
    "#     test_data = CompDataset(df_new_test,model_type)\n",
    "    \n",
    "#     return train_data, eval_data_ssl, eval_data_sl, test_data, df_unlabel_splitted_train,df_train_splitted_,df_eval_splitted_sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the labelled data\n",
    "# def process_data_labelled(filePath_src,filePath_tar, filePath_srcTags,filePath_tarTags,\n",
    "#                  filePath_src_eval,filePath_tar_eval, filePath_srcTags_eval,filePath_tarTags_eval,\n",
    "#                  model_type='xlm'):\n",
    "    \n",
    "    \n",
    "#     dataObjTrain = loadDatafromFile(filePath_src,filePath_tar, filePath_srcTags,filePath_tarTags)\n",
    "#     df_train= dataObjTrain.createDf() # get dataframe from files    \n",
    "#     df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "#     df_train=df_train.iloc[:500,:]\n",
    "#     dataObjEval = loadDatafromFile(filePath_src_eval,filePath_tar_eval, filePath_srcTags_eval,filePath_tarTags_eval)\n",
    "#     df_eval = dataObjEval.createDf()\n",
    "#     obj_tokenized_train = createTokenizedDf(df_train,model_type)\n",
    "#     obj_tokenized_test = createTokenizedDf(df_eval,model_type)\n",
    "#     df_new_train = obj_tokenized_train.convertDf()\n",
    "#     df_new_eval = obj_tokenized_test.convertDf()\n",
    "#     train_data = CompDataset(df_new_train,model_type)\n",
    "#     test_data = CompDataset(df_new_eval,model_type)\n",
    "#     return train_data,test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config=reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter labeled data needed 250\n"
     ]
    }
   ],
   "source": [
    "total_train_data = 2000 \n",
    "label_data_required = int(input('enter labeled data needed'))\n",
    "unlabel_data_required = total_train_data - label_data_required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set_seed()\n",
    "train_data , eval_data_ssl,eval_data_sl, test_data, df_unlabel, df_train = process_data_labelled(config.filePath_src,config.filePath_tar, config.filePath_srcTags,config.filePath_tarTags,\n",
    "                                                config.filePath_src_eval,config.filePath_tar_eval, config.filePath_srcTags_eval,\n",
    "                                                config.filePath_tarTags_eval,label_data_required,unlabel_data_required, total_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<data_utils.CompDataset at 0x7f3869b6c4e0>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>src_tokens</th>\n",
       "      <th>tar_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>José Ortega y Gasset visited Husserl at Freibu...</td>\n",
       "      <td>1934 besuchte José Ortega y Gasset Husserl in ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>However , a disappointing ninth in China meant...</td>\n",
       "      <td>Eine enttäuschende Neunte in China bedeutete j...</td>\n",
       "      <td>OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...</td>\n",
       "      <td>OK BAD OK BAD OK BAD OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In his diary , Chase wrote that the release of...</td>\n",
       "      <td>In seinem Tagebuch , Chase schrieb , dass die ...</td>\n",
       "      <td>OK OK OK OK BAD BAD OK OK OK OK OK OK OK OK BA...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK BAD OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heavy arquebuses mounted on wagons were called...</td>\n",
       "      <td>Schwere Arquebuses auf Waggons montiert wurden...</td>\n",
       "      <td>OK BAD BAD OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK BAD OK OK OK OK BAD BAD OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once North Pacific salmon die off after spawni...</td>\n",
       "      <td>Sobald der nordpazifische Lachs nach dem Laich...</td>\n",
       "      <td>OK OK OK OK BAD OK OK OK OK OK BAD OK BAD OK O...</td>\n",
       "      <td>OK OK OK BAD OK OK OK BAD OK OK OK OK OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>. October , she toured Robin with Gibb in Aust...</td>\n",
       "      <td>Im Oktober tourte sie mit Robin Gibb in Austra...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>In and New she toured with Robin Gibb in Austr...</td>\n",
       "      <td>Im Oktober tourte sie mit Robin Gibb in Austra...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>with October , she toured In Australia Gibb in...</td>\n",
       "      <td>Im Oktober tourte sie mit Robin Gibb in Austra...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>In October , she toured with Robin Gibb Zealan...</td>\n",
       "      <td>Im Oktober tourte sie mit Robin Gibb in Austra...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>In October Robin she toured with , Gibb in Zea...</td>\n",
       "      <td>Im Oktober tourte sie mit Robin Gibb in Austra...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 source  \\\n",
       "0     José Ortega y Gasset visited Husserl at Freibu...   \n",
       "1     However , a disappointing ninth in China meant...   \n",
       "2     In his diary , Chase wrote that the release of...   \n",
       "3     Heavy arquebuses mounted on wagons were called...   \n",
       "4     Once North Pacific salmon die off after spawni...   \n",
       "...                                                 ...   \n",
       "1995  . October , she toured Robin with Gibb in Aust...   \n",
       "1996  In and New she toured with Robin Gibb in Austr...   \n",
       "1997  with October , she toured In Australia Gibb in...   \n",
       "1998  In October , she toured with Robin Gibb Zealan...   \n",
       "1999  In October Robin she toured with , Gibb in Zea...   \n",
       "\n",
       "                                                 target  \\\n",
       "0     1934 besuchte José Ortega y Gasset Husserl in ...   \n",
       "1     Eine enttäuschende Neunte in China bedeutete j...   \n",
       "2     In seinem Tagebuch , Chase schrieb , dass die ...   \n",
       "3     Schwere Arquebuses auf Waggons montiert wurden...   \n",
       "4     Sobald der nordpazifische Lachs nach dem Laich...   \n",
       "...                                                 ...   \n",
       "1995  Im Oktober tourte sie mit Robin Gibb in Austra...   \n",
       "1996  Im Oktober tourte sie mit Robin Gibb in Austra...   \n",
       "1997  Im Oktober tourte sie mit Robin Gibb in Austra...   \n",
       "1998  Im Oktober tourte sie mit Robin Gibb in Austra...   \n",
       "1999  Im Oktober tourte sie mit Robin Gibb in Austra...   \n",
       "\n",
       "                                             src_tokens  \\\n",
       "0                      OK OK OK OK OK OK OK OK OK OK OK   \n",
       "1     OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...   \n",
       "2     OK OK OK OK BAD BAD OK OK OK OK OK OK OK OK BA...   \n",
       "3                    OK BAD BAD OK OK OK OK OK OK OK OK   \n",
       "4     OK OK OK OK BAD OK OK OK OK OK BAD OK BAD OK O...   \n",
       "...                                                 ...   \n",
       "1995          OK OK OK OK OK OK OK OK OK OK OK OK OK OK   \n",
       "1996          OK OK OK OK OK OK OK OK OK OK OK OK OK OK   \n",
       "1997          OK OK OK OK OK OK OK OK OK OK OK OK OK OK   \n",
       "1998          OK OK OK OK OK OK OK OK OK OK OK OK OK OK   \n",
       "1999          OK OK OK OK OK OK OK OK OK OK OK OK OK OK   \n",
       "\n",
       "                                             tar_tokens  \n",
       "0     OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "1     OK BAD OK BAD OK BAD OK OK OK OK OK OK OK OK O...  \n",
       "2     OK OK OK OK OK OK OK OK OK BAD OK BAD OK OK OK...  \n",
       "3     OK OK OK BAD OK OK OK OK BAD BAD OK OK OK OK O...  \n",
       "4     OK OK OK BAD OK OK OK BAD OK OK OK OK OK OK OK...  \n",
       "...                                                 ...  \n",
       "1995  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "1996  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "1997  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "1998  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "1999  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataaug_obj = DataAugmentation(df_train,swap_words=2,syn_words=2,del_words_prob=0.2,num_sentences=7)  \n",
    "swapDataset = dataaug_obj.random_swap()\n",
    "swapDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>src_tokens</th>\n",
       "      <th>tar_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nicole later reveals that she cheated on Azan ...</td>\n",
       "      <td>Nicole verrät später , dass sie Azan betrogen ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wood 's smashing helped the Sabres to 210 for ...</td>\n",
       "      <td>Holz Zerschlagung half den Sabres 210 für 6 , ...</td>\n",
       "      <td>BAD BAD BAD OK OK OK BAD OK OK OK OK OK OK OK ...</td>\n",
       "      <td>OK BAD OK BAD OK OK OK OK OK OK BAD OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nevertheless , the Chinese communists and the ...</td>\n",
       "      <td>Nichtsdestotrotz drängten die chinesischen Kom...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Finns succeeded in repelling the main Sovi...</td>\n",
       "      <td>Den Finnen gelang es , den wichtigsten sowjeti...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Conrad had to enforce his royal prerogatives i...</td>\n",
       "      <td>Conrad musste seine königlichen Vorrechte im H...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>A chronically prolapsed rectal tissue may unde...</td>\n",
       "      <td>Ein chronisch vorgezogenes rektales Gewebe kan...</td>\n",
       "      <td>OK OK BAD BAD BAD OK OK BAD OK OK OK OK OK BAD...</td>\n",
       "      <td>OK OK OK OK OK BAD OK BAD OK BAD OK OK OK BAD ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2013 Foreign Policy Top 100 Global Thinkers , ...</td>\n",
       "      <td>2013 Foreign Policy Top 100 Global Thinkers , ...</td>\n",
       "      <td>OK OK BAD BAD BAD BAD BAD OK OK OK</td>\n",
       "      <td>BAD BAD OK BAD OK BAD OK BAD OK BAD OK BAD OK ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>It remained operational until 1988 , disbandin...</td>\n",
       "      <td>Sie blieb bis 1988 in Betrieb und zerfiel am 3...</td>\n",
       "      <td>BAD BAD OK OK OK OK BAD OK OK OK OK OK</td>\n",
       "      <td>OK BAD OK OK OK OK OK OK OK OK OK OK OK OK BAD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Dark Shadows Almanac , edited by Kathryn Leigh...</td>\n",
       "      <td>Dark Shadows Almanac , herausgegeben von Kathr...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK BAD OK ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>\" In Memory of Sigmund Freud \" , 1940 , poets....</td>\n",
       "      <td>\" In Memory of Sigmund Freud \" , 1940 , poets....</td>\n",
       "      <td>OK OK BAD BAD OK OK OK OK OK OK OK OK OK OK OK...</td>\n",
       "      <td>OK OK OK OK OK BAD OK BAD OK OK OK OK OK OK OK...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               source  \\\n",
       "0   Nicole later reveals that she cheated on Azan ...   \n",
       "1   Wood 's smashing helped the Sabres to 210 for ...   \n",
       "2   Nevertheless , the Chinese communists and the ...   \n",
       "3   The Finns succeeded in repelling the main Sovi...   \n",
       "4   Conrad had to enforce his royal prerogatives i...   \n",
       "..                                                ...   \n",
       "57  A chronically prolapsed rectal tissue may unde...   \n",
       "58  2013 Foreign Policy Top 100 Global Thinkers , ...   \n",
       "59  It remained operational until 1988 , disbandin...   \n",
       "60  Dark Shadows Almanac , edited by Kathryn Leigh...   \n",
       "61  \" In Memory of Sigmund Freud \" , 1940 , poets....   \n",
       "\n",
       "                                               target  \\\n",
       "0   Nicole verrät später , dass sie Azan betrogen ...   \n",
       "1   Holz Zerschlagung half den Sabres 210 für 6 , ...   \n",
       "2   Nichtsdestotrotz drängten die chinesischen Kom...   \n",
       "3   Den Finnen gelang es , den wichtigsten sowjeti...   \n",
       "4   Conrad musste seine königlichen Vorrechte im H...   \n",
       "..                                                ...   \n",
       "57  Ein chronisch vorgezogenes rektales Gewebe kan...   \n",
       "58  2013 Foreign Policy Top 100 Global Thinkers , ...   \n",
       "59  Sie blieb bis 1988 in Betrieb und zerfiel am 3...   \n",
       "60  Dark Shadows Almanac , herausgegeben von Kathr...   \n",
       "61  \" In Memory of Sigmund Freud \" , 1940 , poets....   \n",
       "\n",
       "                                           src_tokens  \\\n",
       "0        OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK   \n",
       "1   BAD BAD BAD OK OK OK BAD OK OK OK OK OK OK OK ...   \n",
       "2   OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...   \n",
       "3                       OK OK OK OK OK OK OK OK OK OK   \n",
       "4   OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...   \n",
       "..                                                ...   \n",
       "57  OK OK BAD BAD BAD OK OK BAD OK OK OK OK OK BAD...   \n",
       "58                 OK OK BAD BAD BAD BAD BAD OK OK OK   \n",
       "59             BAD BAD OK OK OK OK BAD OK OK OK OK OK   \n",
       "60  OK OK OK OK OK OK OK OK OK OK OK OK OK BAD OK ...   \n",
       "61  OK OK BAD BAD OK OK OK OK OK OK OK OK OK OK OK...   \n",
       "\n",
       "                                           tar_tokens  \n",
       "0   OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "1   OK BAD OK BAD OK OK OK OK OK OK BAD OK OK OK O...  \n",
       "2   OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "3   OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "4   OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "..                                                ...  \n",
       "57  OK OK OK OK OK BAD OK BAD OK BAD OK OK OK BAD ...  \n",
       "58  BAD BAD OK BAD OK BAD OK BAD OK BAD OK BAD OK ...  \n",
       "59  OK BAD OK OK OK OK OK OK OK OK OK OK OK OK BAD...  \n",
       "60  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "61  OK OK OK OK OK BAD OK BAD OK OK OK OK OK OK OK...  \n",
       "\n",
       "[62 rows x 4 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled data 250\n",
      "ssl eval data 500\n",
      "sl eval data 62\n",
      "unlabel data 1750\n",
      "test data 1000\n"
     ]
    }
   ],
   "source": [
    "len_labelled_data = len(train_data)\n",
    "len_eval_data_ssl = len(eval_data_ssl)\n",
    "print('labeled data',len_labelled_data)\n",
    "print('ssl eval data',len_eval_data_ssl)\n",
    "print('sl eval data',len(eval_data_sl))\n",
    "print('unlabel data',len(df_unlabel))\n",
    "print('test data',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([     0,  17151,  12426,   2765,    113,    527, 110896,  36997,     71,\n",
      "         20387,   2189,    141,     99, 183124,     23,  58020,      6,      5,\n",
      "             2,      2,    656,  58020,    656, 138438,     13,    656,  17151,\n",
      "           656,  12426,   2765,    656,    113,    656,    527, 110896,    656,\n",
      "         20387,   2189,    141,    656,     23,    656, 183124,    656,      6,\n",
      "             5,    656,      2,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1])\n",
      "tensor([     0, 121220,  14432, 122273,      7,    450,   2412,    290,  27686,\n",
      "            98,   1155,     66,      6,      4,   3129,   5809,   3564,     70,\n",
      "         76755,      6,      5,      2,      2,    656, 121220,    656,    493,\n",
      "         65512,    656,  38985,    656,      6,      4,    656,   1421,    656,\n",
      "          1329,    656,   1155,     66,    656,   1600,  79556,    656,   1256,\n",
      "           656,      6,      4,    656,    509,    656,     68,    656,  93577,\n",
      "           656,   2809,    555,    656,  25482,    656,      6,      5,    656,\n",
      "             2,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1])\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(train_data))[0]) # 0,17151,12456,..\n",
    "print(next(iter(eval_data_sl))[0])#121220,.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_obj = createDataloaders(train_data,config.TRAIN_BATCH_SIZE)\n",
    "train_dataloader = loader_obj.createDataloaders()\n",
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_obj = createDataloaders(eval_data_ssl,config.TRAIN_BATCH_SIZE)\n",
    "val_dataloader = loader_obj.createDataloaders()\n",
    "len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_obj = createDataloaders(test_data,config.TRAIN_BATCH_SIZE)\n",
    "test_dataloader = loader_obj.createDataloaders()\n",
    "len(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Training supervised Model for the same labeled data </H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Experiment name :  supervised_250_labeled_data_withdropout_aug_data\n",
      "Enter Model name : supervised_with_250_labeled_data_aug_data\n"
     ]
    }
   ],
   "source": [
    "experiment_name_supervised = input('Enter Experiment name : ')\n",
    "model_name_supervised = input('Enter Model name :')\n",
    "#supervised_250_labeled_data_withdropout_new_data\n",
    "#supervised_with_250_labeled_data_new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'supervised_with_250_labeled_data_aug_data'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_supervised = SummaryWriter(f\"runs/Supervised_and_MeanTeacherTraining_resampleddata/supervised_with_{len(train_data)}_labelled_withdropout_augData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = reload(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EntityModel(with_noise_layer = False, dropout_layer=True)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.001,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "num_train_steps = int(len(train_data) / config.TRAIN_BATCH_SIZE * config.EPOCHS) #10 is the batchsize\n",
    "optimizer = AdamW(optimizer_parameters, lr=2e-5) # used 3e-5\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",
    ")\n",
    "early_stopping = EarlyStopping(patience=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 2898, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 70, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 101, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1675, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1683, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 876\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 193, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\", line 2906, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 2900, in get_loc\n    raise KeyError(key) from err\nKeyError: 876\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-432daddf5b1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch+1} of {config.EPOCHS}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mtest_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Thesis/src/code/code_files/engine.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(data_loader, model, optimizer, scheduler, large_model)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mlst_active_tar_and_gaps_labels\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mlst_active_tar_and_gaps_labels_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mb_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1172\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;31m# The next_data is updated to return a tuple that consists of worker id in addition to the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m         \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;31m# It is possible that we may not get the correct worker id for samples generated out-of-order.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1067\u001b[0m                 \u001b[0mw_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 2898, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 70, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 101, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1675, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1683, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 876\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 193, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\", line 2906, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 2900, in get_loc\n    raise KeyError(key) from err\nKeyError: 876\n"
     ]
    }
   ],
   "source": [
    "set_seed()\n",
    "best_accuracy = -1\n",
    "train_loss_lst = []\n",
    "val_loss_lst = []\n",
    "with open(f'../Logs/Supervised_logs/{experiment_name_supervised}', 'w') as f:\n",
    "    f.write(f'{experiment_name_supervised}'+'\\n')\n",
    "    for epoch in range(config.EPOCHS):\n",
    "            print(f'Epoch {epoch+1} of {config.EPOCHS}')\n",
    "            train_metrics = engine.train_fn(train_dataloader, model, optimizer, scheduler)\n",
    "            print(classification_report(train_metrics[21],train_metrics[22]))\n",
    "            test_metrics = engine.eval_fn(val_dataloader, model)\n",
    "            print(classification_report(test_metrics[21],test_metrics[22]))\n",
    "#             print('For gaps : ',classification_report(test_metrics[27],test_metrics[28]) )\n",
    "#             print('For tar :',classification_report(test_metrics[25],test_metrics[26]) )\n",
    "#             print('For source : ',classification_report(test_metrics[23],test_metrics[24]) )\n",
    "            print(f\"Train Loss = {train_metrics[0]} Valid Loss = {test_metrics[0]}\")\n",
    "            writer_supervised.add_scalar('Train_loss/Supervised',train_metrics[0], epoch)\n",
    "            writer_supervised.add_scalar('Validation_loss/Supervised',test_metrics[0],epoch)\n",
    "            writer_supervised.add_scalar('mcc_score/Supervised_validation',test_metrics[4],epoch)\n",
    "            writer_supervised.add_scalar('mcc_score/Supervised_train',train_metrics[4],epoch)\n",
    "            train_loss_lst.append(train_metrics[:-14])\n",
    "            val_loss_lst.append(test_metrics[:-14])\n",
    "            f.write(f\"Train_loss {epoch+1} : {str(train_loss_lst)}\" + '\\n')\n",
    "            f.write(f\"val_loss {epoch+1} : {str(val_loss_lst)}\" + '\\n')\n",
    "            \n",
    "            if early_stopping.step(float(\"{:.2f}\".format(test_metrics[4]))): #mcc score for early stopping \"{:.2f}\".format(a_float)\n",
    "                  break  # early stop criterion is met, we can stop now\n",
    "            if float(\"{:.2f}\".format(test_metrics[4])) >  best_accuracy:\n",
    "                torch.save(model.state_dict(), f'../models/training_data/{model_name_supervised}.bin')\n",
    "                best_accuracy = float(\"{:.2f}\".format(test_metrics[4]))\n",
    "                print('Model saved for this epoch!')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Testing Supervised model on hold out data <h/4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Thesis/src/code/code_files/engine.py:411: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  np_arr_labels = np.array(tar_labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for whole sentence class 0 : 0.35844583151884646\n",
      "Average F1 Validation score for whole sentence class 1 : 0.732427905474946\n",
      "Average Accuracy Validation score whole sentence  : 0.6223583024779266\n",
      "Average mcc Validation score whole sentence : 0.26871249782677653\n",
      "Average F1 Validation score for source sentence class 0 : 0.3452993385719939\n",
      "Average F1 Validation score for source sentence class 1 : 0.6598925141699216\n",
      "Average Accuracy Validation score source sentence  : 0.5523386161577116\n",
      "Average mcc Validation score source sentence : 0.21806601866243785\n",
      "Average F1 Validation score for target sentence class 0 : 0.3755228772262711\n",
      "Average F1 Validation score for target sentence class 1 : 0.5368303214457062\n",
      "Average Accuracy Validation score target sentence  : 0.4681382451243251\n",
      "Average mcc Validation score target sentence : 0.20839890931442623\n",
      "Average F1 Validation score for gaps in target sentence class 0 : 0.0\n",
      "Average F1 Validation score for gaps in target sentence class 1 : 0.9865048831015093\n",
      "Average Accuracy Validation score gaps in target sentence  : 0.9733691526017637\n",
      "Average mcc Validation score gaps in target sentence : -0.0012626608241693691\n",
      "Average F1 Validation score for whole target sentence class 0 : 0.3682808204694438\n",
      "Average F1 Validation score for whole target sentence class 1 : 0.7703942817615863\n",
      "Average Accuracy Validation score whole target sentence  : 0.6632018038331454\n",
      "Average mcc Validation score whole target sentence : 0.29757871953327575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.26871249782677653"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'../models/training_data/{model_name_supervised}.bin'))\n",
    "model.eval()\n",
    "test_metrics = engine.eval_fn(test_dataloader, model)\n",
    "mcc_score_supervised = test_metrics[4]\n",
    "mcc_score_supervised\n",
    "# Thesis/src/code/models/training_data/supervised_with_1250_labeled_data.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'supervised_with_250_labeled_data_resampled_2'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-04-27 15:38:05.376 pytorch-1-6-gpu-py-ml-g4dn-2xlarge-775645915fb58e49f1a4db4ff39d:48 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2022-04-27 15:38:05.401 pytorch-1-6-gpu-py-ml-g4dn-2xlarge-775645915fb58e49f1a4db4ff39d:48 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Thesis/src/code/code_files/engine.py:411: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  np_arr_labels = np.array(tar_labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for whole sentence class 0 : 0.45008152276949914\n",
      "Average F1 Validation score for whole sentence class 1 : 0.872855427073562\n",
      "Average Accuracy Validation score whole sentence  : 0.7934634007405298\n",
      "Average mcc Validation score whole sentence : 0.3606928518711579\n",
      "Average F1 Validation score for source sentence class 0 : 0.43341172371744624\n",
      "Average F1 Validation score for source sentence class 1 : 0.8384212225837702\n",
      "Average Accuracy Validation score source sentence  : 0.748550444530344\n",
      "Average mcc Validation score source sentence : 0.32287545960497577\n",
      "Average F1 Validation score for target sentence class 0 : 0.478185234152112\n",
      "Average F1 Validation score for target sentence class 1 : 0.8129132050994187\n",
      "Average Accuracy Validation score target sentence  : 0.7245748705329269\n",
      "Average mcc Validation score target sentence : 0.34419956265479695\n",
      "Average F1 Validation score for gaps in target sentence class 0 : 0.04222648752399232\n",
      "Average F1 Validation score for gaps in target sentence class 1 : 0.9852038547071905\n",
      "Average Accuracy Validation score gaps in target sentence  : 0.9708579104128949\n",
      "Average mcc Validation score gaps in target sentence : 0.054184275881262664\n",
      "Average F1 Validation score for whole target sentence class 0 : 0.4629331184528606\n",
      "Average F1 Validation score for whole target sentence class 1 : 0.8916377628441362\n",
      "Average Accuracy Validation score whole target sentence  : 0.8196617812852312\n",
      "Average mcc Validation score whole target sentence : 0.3845762604810503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3606928518711579"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'../models/training_data/supervised_with_3000_labeled_data.bin'))\n",
    "model.eval()\n",
    "test_metrics = engine.eval_fn(test_dataloader, model)\n",
    "mcc_score_supervised = test_metrics[4]\n",
    "mcc_score_supervised\n",
    "# Thesis/src/code/models/training_data/supervised_with_1250_labeled_data.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Thesis/src/code/code_files/engine.py:411: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  np_arr_labels = np.array(tar_labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for whole sentence class 0 : 0.4520824269271843\n",
      "Average F1 Validation score for whole sentence class 1 : 0.8797033614977814\n",
      "Average Accuracy Validation score whole sentence  : 0.8027200227855312\n",
      "Average mcc Validation score whole sentence : 0.3613334105721021\n",
      "Average F1 Validation score for source sentence class 0 : 0.43302990897269183\n",
      "Average F1 Validation score for source sentence class 1 : 0.8510274808415093\n",
      "Average Accuracy Validation score source sentence  : 0.7640510243525319\n",
      "Average mcc Validation score source sentence : 0.3201388797931383\n",
      "Average F1 Validation score for target sentence class 0 : 0.48088287646849415\n",
      "Average F1 Validation score for target sentence class 1 : 0.8195698977950455\n",
      "Average Accuracy Validation score target sentence  : 0.7322143460535497\n",
      "Average mcc Validation score target sentence : 0.34725342659258746\n",
      "Average F1 Validation score for gaps in target sentence class 0 : 0.029661016949152543\n",
      "Average F1 Validation score for gaps in target sentence class 1 : 0.9864392728134067\n",
      "Average Accuracy Validation score gaps in target sentence  : 0.9732523506394908\n",
      "Average mcc Validation score gaps in target sentence : 0.07550230156356405\n",
      "Average F1 Validation score for whole target sentence class 0 : 0.46621202727836325\n",
      "Average F1 Validation score for whole target sentence class 1 : 0.8955421053341062\n",
      "Average Accuracy Validation score whole target sentence  : 0.8252762119503946\n",
      "Average mcc Validation score whole target sentence : 0.38740244612386665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3613334105721021"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'../models/MeanTeacher_models/TeacherModels/teacherModel_v1_with_1750_labeled_250_unlabeled_data_withdropout_alpha_0.99_noises_changed_t_0.5_s_0.3.bin'))\n",
    "model.eval()\n",
    "test_metrics = engine.eval_fn(test_dataloader, model)\n",
    "mcc_score_supervised = test_metrics[4]\n",
    "mcc_score_supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'supervised_with_250_labeled_data_resampled_2'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_supervised "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'writer_supervised' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-7f4a083340ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m writer_supervised.add_hparams(\n\u001b[0m\u001b[1;32m      2\u001b[0m                     { \"lr\": float(config.lr),  \n\u001b[1;32m      3\u001b[0m                      \u001b[0;34m\"labelled_data\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                      \u001b[0;34m\"validation_data\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_data_sl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                      \u001b[0;34m\"alpha\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'writer_supervised' is not defined"
     ]
    }
   ],
   "source": [
    "writer_supervised.add_hparams(\n",
    "                    { \"lr\": float(config.lr),  \n",
    "                     \"labelled_data\": float(len(train_data)), \n",
    "                     \"validation_data\": float(len(eval_data_sl)),\n",
    "                     \"alpha\":0.0, \n",
    "                     \"teacher_noise_gaussian_std\":0.0,\n",
    "                     \"student_noise_gaussian_std\":0.0,\n",
    "                     \"unlabelled_data\":0.0, \n",
    "                    },\n",
    "                    {\n",
    "                    \"mcc_score_test\" : float(mcc_score_supervised),\n",
    "                    \"mcc_score_teacher\": float(0) ,\n",
    "                    \"mcc_score_student\" : float(0),\n",
    "                    },\n",
    "                )\n",
    "# writer_supervised.flush()\n",
    "# writer_supervised.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training semi supervised Model for the same labeled data and added unlabeled data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_semisupervised = SummaryWriter(f\"runs/Supervised_and_MeanTeacherTraining_resampleddata/SemiSupervised_with_{label_data_required}_labelled_and_{unlabel_data_required}_unlabeled_data_rampup_steps_dropout_0.99_resampled3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_unlabel['len_src'] = df_unlabel['source'].str.split().map(lambda x:len(x))\n",
    "# df_unlabel['len_tar'] = df_unlabel['target'].str.split().map(lambda x:len(x))\n",
    "# print('Source : Max length: {}, Min length: {}, Average Length :  {}'.format(max(df_unlabel['len_tar']),min(df_unlabel['len_tar']),df_unlabel['len_tar'].mean()))\n",
    "\n",
    "# df_unlabel['len_tar'].hist()\n",
    "# df_unlabel[df_unlabel['len_tar']==30].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_data_unlabelled(filePath_src,filePath_tar,len_train_data,df_unlabelled_train,model_type):\n",
    "    \n",
    "#     dataObj = loadUnlabelledDatafromFile(filePath_src,filePath_tar)\n",
    "#     df= dataObj.createDfUnlabelled() # get dataframe from files\n",
    "#     df['len_src'] = df['source'].str.split().map(lambda x:len(x))\n",
    "#     df['len_tar'] = df['target'].str.split().map(lambda x:len(x))\n",
    "    \n",
    "#     df = df[df.len_tar != 36]\n",
    "#     df = df[df.len_tar != 2]\n",
    "#     df = df[df.len_src != 1]\n",
    "    \n",
    "#     df = df.iloc[:,:-2]\n",
    "#     df = df.reset_index(drop=True)\n",
    "#     df_unlabelled_train = df_unlabelled_train.iloc[:,:-2]\n",
    "#     df_new = df.append(df_unlabelled_train, ignore_index = True)\n",
    "#     len_unlabelled_needed = config.fix_train_size - len_train_data\n",
    "#     df_new=df_new[:len_unlabelled_needed]\n",
    "# #     print(df_new)\n",
    "    \n",
    "#     obj_tokenized = createTokenizedDfUnlabelled(df_new,model_type)\n",
    "#     df_unlabelled= obj_tokenized.convertDf()\n",
    "# #     enc_label = preprocessing.LabelEncoder()\n",
    "# #     df_new['labels']= enc_label.fit_transform(df_new['labels'])\n",
    "#     train_data = CompDatasetUnlabelled(df_unlabelled,model_type)\n",
    "#     return df_new,train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_unlabelled(filePath_src,filePath_tar,df_unlabel, unlabel_data_needed, model_type):\n",
    "    \n",
    "    set_seed()\n",
    "    dataObj = loadUnlabelledDatafromFile(filePath_src,filePath_tar)\n",
    "    df= dataObj.createDfUnlabelled() # get dataframe from files\n",
    "    df['len_src'] = df['source'].str.split().map(lambda x:len(x))\n",
    "    df['len_tar'] = df['target'].str.split().map(lambda x:len(x))\n",
    "    \n",
    "    df = df[df.len_tar != 36]\n",
    "    df = df[df.len_tar != 2]\n",
    "    df = df[df.len_src != 1]\n",
    "    \n",
    "    df=df.iloc[:,:-2]\n",
    "#     df=df.iloc[:1000,:]\n",
    "#     df = df[:2000]\n",
    "    if unlabel_data_needed > len(df_unlabel):\n",
    "        \n",
    "        extra_data_needed = unlabel_data_needed - len(df_unlabel) # TAKE IT FROM THE BACKTRANSLATED DATASET\n",
    "        df = df[:extra_data_needed]\n",
    "        df_unlabel = df_unlabel.append(df,ignore_index=True)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        df_unlabel = df_unlabel[:unlabel_data_needed]\n",
    "        \n",
    "    obj_tokenized = createTokenizedDfUnlabelled(df_unlabel,model_type) # change to df\n",
    "    df_new = obj_tokenized.convertDf()\n",
    "#     enc_label = preprocessing.LabelEncoder()\n",
    "#     df_new['labels']= enc_label.fit_transform(df_new['labels'])\n",
    "    train_data = CompDatasetUnlabelled(df_new,model_type)\n",
    "    return df_new,train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([     0, 104431,     56,   1225,  80430,     10,    228,  32775,  35966,\n",
       "              6,      4,  12960, 123573,     31,   4527,      7,     10,  51876,\n",
       "           6967,    136,     10,   4989,  13986,    147,      6,      5,      2,\n",
       "              2,    656, 104431,     56,    656,  45786,    214,     18,    656,\n",
       "            599,    656,    228,  32775,  35966,    656,      6,      4,    656,\n",
       "          31005,    656, 123573,     31,    656,    909,    656,   8643,  16210,\n",
       "          36361,   1479,    656,    165,    656,   1918,    656,   1937,     33,\n",
       "            656,  26530,     56,    656, 190602,    656,      6,      5,    656,\n",
       "              2,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unlabelled_train, dataset_train_unlabelled = process_data_unlabelled(config.filePath_src_backtranslated,config.filePath_tar_backtranslated,df_unlabel,\n",
    "                                                                       unlabel_data_required, model_type = 'xlm')\n",
    "len_unlabelled_data = len(dataset_train_unlabelled)\n",
    "print(len_unlabelled_data)\n",
    "temp_val = next(iter(dataset_train_unlabelled)) # 31384,..\n",
    "temp_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combining 2 tuples --> labelled and unlabelled data \n",
    "combined_Data = train_data + dataset_train_unlabelled\n",
    "len(combined_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config= reload(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataloaders\n",
    "loader_obj = createDataloaders(combined_Data,config.TRAIN_BATCH_SIZE)\n",
    "train_dataloader = loader_obj.createDataloaders()\n",
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_obj = createDataloaders(eval_data_ssl,config.VALID_BATCH_SIZE)\n",
    "val_dataloader = loader_obj.createDataloaders()\n",
    "len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     0,  20204,    191,   1314,   1295,     70,  30839,    209,    927,\n",
       "         39076,   3089,  26783, 141117,   1799,    191,    613,  18280,   2822,\n",
       "            47,     70,    606,    330,  60636,  70069,      7,      6,      5,\n",
       "             2,      2,    656,   2206,  72450,     13,    656,    224,    656,\n",
       "          4068,    656,  39076,   3089,  26783,    656,  42173,  62339,      7,\n",
       "           656,      6,  65860,    656,    833,    656,    168,    656,   1248,\n",
       "         69418,  26316,    656,  62980,   9022,    656,   5122,    656,      6,\n",
       "             5,    656,      2,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "batch[0][1] # as seen, combination of labelled and unlabelled data in one training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_seed()\n",
    "# a = torch.tensor([[1,2],[3,4]])\n",
    "# a= a.cuda()\n",
    "# noise = GaussianNoise(0.7)\n",
    "# print(noise(a))\n",
    "# # print(torch.randn(a.size()) * 0.7 )\n",
    "# # print(torch.autograd.Variable(torch.randn(a.size()).cuda() * 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EntityModel(nn.Module):\n",
    "    \n",
    "#     def __init__(self,std_gaussian=0.1,with_noise_layer = True, dropout_layer=False, dropout_prob = 0.3):\n",
    "        \n",
    "#         super(EntityModel, self).__init__()\n",
    "        \n",
    "#         self.std_gaussian = std_gaussian\n",
    "#         self.with_noise_layer = with_noise_layer\n",
    "#         self.bert = XLMRobertaModel.from_pretrained(config.BASE_MODEL,output_attentions = False, output_hidden_states = False) # , add_pooling_layer=False\n",
    "#         self.dropout_layer = dropout_layer\n",
    "#         self.dropout_prob = dropout_prob\n",
    "#         if self.with_noise_layer:\n",
    "#             self.noise = GaussianNoise(stddev=self.std_gaussian)\n",
    "#         if self.dropout_layer:\n",
    "#             self.bert_drop_1 = nn.Dropout(self.dropout_prob) # remove this or noise \n",
    "#         self.out_tag = nn.Linear(768, 2)\n",
    "#         self.softmax = nn.Softmax(dim=2)\n",
    "    \n",
    "#     def forward(self, ids, attention_mask):\n",
    "        \n",
    "#         outputs = self.bert(ids,\n",
    "#                             attention_mask = attention_mask,\n",
    "#                             return_dict=False)\n",
    "        \n",
    "#         if (self.with_noise_layer):\n",
    "            \n",
    "#             noise = self.noise(outputs[0]) # 256*768\n",
    "            \n",
    "#             if self.dropout_layer:\n",
    "#                 bo_tag = self.bert_drop_1(noise)\n",
    "#                 tag = self.out_tag(bo_tag)\n",
    "#             else:\n",
    "#                 tag = self.out_tag(noise)\n",
    "        \n",
    "#         else:\n",
    "            \n",
    "#             if self.dropout_layer:\n",
    "#                 bo_tag = self.bert_drop_1(outputs[0])\n",
    "#                 tag = self.out_tag(bo_tag)\n",
    "            \n",
    "#             else :\n",
    "#                 tag = self.out_tag(outputs[0])\n",
    "# #         tag = self.out_tag(bo_tag) # 256 * 2 \n",
    "       \n",
    "#         softmax_prob = self.softmax(tag)\n",
    "        \n",
    "# #         loss_tag = loss_fn(tag,labels,attention_mask)\n",
    "        \n",
    "#         return softmax_prob,tag\n",
    "# #         return outputs[0], outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = EntityModel(std_gaussian=config.gaussian_noise_std_student, with_noise_layer=True )\n",
    "# model.cuda()# print(noise ,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs, outputs1 = model(batch[0].cuda(), attention_mask = batch[1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = EntityModel(std_gaussian=config.gaussian_noise_std_teacher)\n",
    "# model2.cuda()\n",
    "# outputs, outputs1 = model2(batch[0].cuda(), attention_mask = batch[1].cuda())\n",
    "# print(outputs,'\\n')\n",
    "# print(outputs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ClassificationCost(output,target,mask):\n",
    "    \n",
    "    \n",
    "#     active_loss = mask.view(-1) == 1 #loss calculation for non padded tokens only (mask =1)\n",
    "#     active_logits = output.view(-1,2)\n",
    "    \n",
    "#     active_labels = torch.where( # just append -100 for the padded tokens so its ignored when computing loss , no need now\n",
    "#         active_loss,             # since its handled in preprocessing only\n",
    "#         target.view(-1),\n",
    "#         torch.tensor(-100).type_as(target)    \n",
    "#     )\n",
    "#     try:\n",
    "#         class_0_weights =1/len(torch.where(active_labels==0)[0]) # trying to weight the labels as its unbalanced mostly\n",
    "    \n",
    "#     except ZeroDivisionError:\n",
    "#         class_0_weights = float(1)\n",
    "    \n",
    "\n",
    "#     try:\n",
    "#         class_1_weights =1/len(torch.where(active_labels==1)[0])\n",
    "    \n",
    "#     except ZeroDivisionError:\n",
    "#         class_1_weights = float(1)\n",
    "    \n",
    "#     weights_tensor = torch.tensor([class_0_weights,class_1_weights]).cuda()\n",
    "#     lfn = nn.CrossEntropyLoss(weight = weights_tensor)\n",
    "#     loss = lfn(active_logits,active_labels)\n",
    "    \n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.tensor([[0.2,0.5],[0.9,0.4],[0.9,0.2],[0.2,0.9],[0.7,0.4]])\n",
    "# b = torch.tensor([-100,-100,-100,-100,-100])\n",
    "# c = torch.tensor([1,1,1,1,1])\n",
    "# ClassificationCost(a,b,c).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ConsistencyCost(student_output, teacher_output,mask,scale=10):\n",
    "    \n",
    "#     assert len(student_output) == len(teacher_output)\n",
    "    \n",
    "#     loss = nn.MSELoss()\n",
    "#     active_outputs = mask.view(-1) == 1 #loss calculation for non padded tokens only (mask =1)\n",
    "#     flattened_outputs_student = student_output.view(-1,2)\n",
    "#     flatenned_outputs_teacher = teacher_output.view(-1,2)\n",
    "\n",
    "#     active_outputs_teacher=flatenned_outputs_teacher[torch.where(mask.view(-1) == 1)]\n",
    "#     active_outputs_student=flattened_outputs_student[torch.where(mask.view(-1) == 1)]\n",
    "    \n",
    "#     return scale * loss(active_outputs_student.view(-1),active_outputs_teacher.view(-1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ConsistencyCost(student_output, teacher_output,mask):\n",
    "    \n",
    "#     assert len(student_output) == len(teacher_output)\n",
    "    \n",
    "#     loss = nn.MSELoss()\n",
    "#     active_outputs = mask.view(-1) == 1 #loss calculation for non padded tokens only (mask =1)\n",
    "#     flattened_outputs_student = student_output.view(-1,2)\n",
    "#     flatenned_outputs_teacher = teacher_output.view(-1,2)\n",
    "    \n",
    "#     # [0.2,0.8] , [0.3,0.7] , [0.9,0.1] -> s\n",
    "#     # [0.7,0.3] , [0.9,0.1] , [0.8,0.2] -> T \n",
    "    \n",
    "    \n",
    "    \n",
    "#     pseudo_label = torch.softmax(logits_u_w.detach()/args.T, dim=-1)\n",
    "#     max_probs, targets_u = torch.max(pseudo_label, dim=-1)\n",
    "#     mask = max_probs.ge(config.threshold).float()\n",
    "\n",
    "#     Lu = (F.cross_entropy(logits_u_s, targets_u,\n",
    "#                                   reduction='none') * mask)\n",
    "    \n",
    "#     active_outputs_teacher=flatenned_outputs_teacher[torch.where(active_outputs == 1)]\n",
    "#     active_outputs_student=flattened_outputs_student[torch.where(active_outputs == 1)]\n",
    "    \n",
    "#     return torch.sqrt(loss(active_outputs_student,active_outputs_teacher))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Update teacher to be exponential moving average of student params.\n",
    "# def update_teacher_params(student_model, teacher_model, alpha, global_step):\n",
    "#     # Use the true average until the exponential average is more correct\n",
    "    \n",
    "#     teacher_params = teacher_model.parameters()\n",
    "#     student_params = student_model.parameters()\n",
    "    \n",
    "#     assert sum(p.numel() for p in student_model.parameters()) == sum(p.numel() for p in teacher_model.parameters())\n",
    "    \n",
    "# #     alpha = min(1 - 1 / (global_step + 1), alpha)\n",
    "    \n",
    "# #     if global_step == 10 or global_step==20 or global_step == 30:\n",
    "# #         print(alpha)\n",
    "    \n",
    "#     for teacher_param, student_param in zip(teacher_params, student_params):\n",
    "#             teacher_param.data.mul_(alpha).add_(1 - alpha, student_param.data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, optimizer,scheduler, student_model, teacher_model ,writer, global_step, epoch):\n",
    "    \n",
    "    student_model.train() # put student model in training mode\n",
    "    teacher_model.train()\n",
    "    total_train_loss = 0\n",
    "    consistency_cst = 0\n",
    "    classification_cst_student=0\n",
    "    classification_cst_teacher=0\n",
    "    classification_cst_lst = []\n",
    "    consistency_cst_lst = []\n",
    "    overall_cst_lst = []\n",
    "#     global_step = 0\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(train_dataloader, total = len(train_dataloader),position = 0 , leave = True)):\n",
    "\n",
    "            \n",
    "        b_input_ids = batch[0].cuda()\n",
    "        b_input_mask = batch[1].cuda()\n",
    "        b_labels = batch[2].cuda()\n",
    "     \n",
    "        \n",
    "        output_student_softmax, output_student_logit = student_model( b_input_ids, attention_mask = b_input_mask )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "                output_teacher_softmax, output_teacher_logit = teacher_model( b_input_ids, attention_mask = b_input_mask )\n",
    "          \n",
    "        \n",
    "        classification_cost_student =  ClassificationCost(output_student_logit,b_labels,b_input_mask)\n",
    "        classification_cst_student+=classification_cost_student.item()\n",
    "        \n",
    "        classification_cost_teacher =  ClassificationCost(output_teacher_logit,b_labels,b_input_mask)\n",
    "        classification_cst_teacher+=classification_cost_teacher.item()\n",
    "        \n",
    "        writer.add_scalar('Train_loss/Classification', classification_cost_student.item(), global_step)\n",
    "        \n",
    "        consistency_cost = ConsistencyCost(output_student_softmax,output_teacher_softmax.detach(),b_input_mask,scale=10)\n",
    "        consistency_cst+=consistency_cost.item()\n",
    "        \n",
    "        writer.add_scalar('Train_loss/Consistency', consistency_cost.item(), global_step)\n",
    "        \n",
    "#         overall_cost = (config.ratio * classification_cost) + ((1 - config.ratio) * consistency_cost)\n",
    "        \n",
    "        overall_cost =  classification_cost_student + get_consistency_weight(global_step) * consistency_cost\n",
    "        \n",
    "        writer.add_scalar('Train_loss/Overall', overall_cost.item(), global_step)\n",
    "        \n",
    "        total_train_loss+=overall_cost.item()\n",
    "\n",
    "        overall_cost.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        update_teacher_params(student_model , teacher_model , config.alpha,global_step)\n",
    "\n",
    "        if (len(train_dataloader) < 200):\n",
    "            if step % 50 == 0 and step!=0:\n",
    "                print ('train loss : ', total_train_loss/step)\n",
    "                print('Consistency Cost :', consistency_cst/step)\n",
    "                print('Classification Cost :', classification_cst_student/step)\n",
    "                print('Consistency Weight is:', get_consistency_weight(global_step))\n",
    "        else:\n",
    "            if step % 200 == 0 and step!=0:\n",
    "                print ('train loss : ', total_train_loss/step)\n",
    "                print('Consistency Cost :', consistency_cst/step)\n",
    "                print('Classification Cost :', classification_cst_student/step)\n",
    "                print('Consistency Weight is:', get_consistency_weight(global_step))\n",
    "\n",
    "        \n",
    "        global_step+=1\n",
    "        \n",
    "    return (consistency_cst/len(train_dataloader), classification_cst_student/len(train_dataloader), total_train_loss/len(train_dataloader),global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def soft_frequency(logits, power=2, probs=False):\n",
    "#     \"\"\"\n",
    "#     Unsupervised Deep Embedding for Clustering Analysis\n",
    "#     https://arxiv.org/abs/1511.06335\n",
    "#     \"\"\"\n",
    "#     if not probs:\n",
    "#         softmax = torch.nn.Softmax(dim=1)\n",
    "#         y = softmax(logits.view(-1, logits.shape[-1])).view(logits.shape)\n",
    "#     else:\n",
    "#         y = logits\n",
    "#     f = torch.sum(y, dim=(0, 1))\n",
    "#     t = y**power / f\n",
    "#     p = t/torch.sum(t, dim=2, keepdim=True)\n",
    "\n",
    "#     return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits = torch.tensor([[[0.3,0.7] , [0.2,0.8] , [0.5569,0.5]]])\n",
    "# f = torch.sum(logits, dim=(0,1))\n",
    "# print(f)\n",
    "# new_logits = soft_frequency(logits,probs = True)\n",
    "# # t = logits**2 / f\n",
    "# # print(t.shape)\n",
    "# # p = t/torch.sum(t, dim=2, keepdim=True)\n",
    "# # p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax = torch.nn.Softmax(dim=1)\n",
    "# y = softmax(new_logits.view(-1, new_logits.shape[-1])).view(new_logits.shape)\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _confidence = y.max(dim=-1)[0]\n",
    "# _confidence[0]\n",
    "# # torch.where(_confidence[0]>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test( model, test_loader):\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    lst_active_preds = []\n",
    "    lst_active_labels = []\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch in tqdm(test_loader,total = len(test_loader),position = 0 , leave = True):            \n",
    "            b_input_ids_eval = batch[0].cuda()\n",
    "            b_input_mask_eval = batch[1].cuda()\n",
    "            b_labels_eval = batch[2].cuda()\n",
    "            \n",
    "            _, output= model(b_input_ids_eval,attention_mask = b_input_mask_eval)\n",
    "            \n",
    "            classification_cost =  ClassificationCost(output,b_labels_eval,b_input_mask_eval)\n",
    "            test_loss+=classification_cost\n",
    "            \n",
    "            labels = b_labels_eval.view(-1) \n",
    "            active_logits = output.view(-1, 2)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1)\n",
    "            active_accuracy = labels.view(-1) != -100\n",
    "            labels_tmp = torch.masked_select(labels, active_accuracy) \n",
    "            pred_tmp = torch.masked_select(flattened_predictions, active_accuracy) \n",
    "            lst_active_labels.extend(labels_tmp.tolist())\n",
    "            lst_active_preds.extend(pred_tmp.tolist())\n",
    "            \n",
    "    avg_f1_score_0=f1_score(lst_active_labels,lst_active_preds,average='binary',pos_label = 0)\n",
    "    avg_f1_score_1=f1_score(lst_active_labels,lst_active_preds,average='binary',pos_label = 1)\n",
    "    avg_accuracy_score=accuracy_score(lst_active_labels,lst_active_preds)\n",
    "    avg_mcc_score = matthews_corrcoef(lst_active_labels,lst_active_preds)\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    \n",
    "    print('Overall validation loss:', test_loss)\n",
    "    print('Average F1 Validation score for whole sentence class 0 :' ,avg_f1_score_0)\n",
    "    print('Average F1 Validation score for whole sentence class 1 :' ,avg_f1_score_1)\n",
    "    print('Average Accuracy Validation score whole sentence  :' ,avg_accuracy_score)\n",
    "    print('Average mcc Validation score whole sentence :' ,avg_mcc_score)\n",
    "    print('Classification Report :'+'\\n', classification_report(lst_active_labels,lst_active_preds))\n",
    "\n",
    "    return (test_loss, avg_f1_score_0 , avg_f1_score_1, avg_accuracy_score, avg_mcc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MeanTeacher(train_dataloader, val_dataloader,len_labelled_data ,len_unlabelled_data, writer,early_stopping,experiment_name,model_name,dropout_layer = False, noise_layer=True):\n",
    "    \n",
    "    exp_name = experiment_name\n",
    "    model_name = model_name\n",
    "    \n",
    "    global_step = 0\n",
    "    best_mcc_teacher = -1\n",
    "    best_mcc_student = -1\n",
    "    # train losses\n",
    "    epochs_consistency_lst =[]\n",
    "    epochs_classification_lst = []\n",
    "    epochs_overall_lst = []\n",
    "    \n",
    "    #test metrices for student\n",
    "    epochs_f1_score_0_lst_student = []\n",
    "    epochs_f1_score_1_lst_student = []\n",
    "    epochs_accuracy_lst_student = []\n",
    "    epochs_mcc_lst_student = []\n",
    "    epochs_cost_lst_student=[]\n",
    "    \n",
    "    #test metrices for teacher\n",
    "    epochs_f1_score_0_lst_teacher= []\n",
    "    epochs_f1_score_1_lst_teacher = []\n",
    "    epochs_accuracy_lst_teacher = []\n",
    "    epochs_mcc_lst_teacher = []\n",
    "    epochs_cost_lst_teacher=[]\n",
    "    \n",
    "    set_seed()\n",
    "    #initialaize the language models\n",
    "    student = EntityModel(std_gaussian=config.gaussian_noise_std_student, with_noise_layer=noise_layer,dropout_layer=dropout_layer) \n",
    "    teacher = EntityModel(std_gaussian=config.gaussian_noise_std_teacher, with_noise_layer=noise_layer)\n",
    "#     student.load_state_dict(torch.load('../models/training_data/model_xlmrobertatokenclassificationmodel_verylessData_weightedlss.bin'))\n",
    "#     teacher.load_state_dict(torch.load('../models/training_data/model_xlmrobertatokenclassificationmodel_verylessData_weightedlss.bin'))\n",
    "    student.cuda() # take model to gpu\n",
    "    teacher.cuda()\n",
    "#     teacher.eval()\n",
    "    param_optimizer_student = list(student.named_parameters())\n",
    "    param_teacher = list(teacher.named_parameters())\n",
    "    \n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer_student if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.001,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer_student if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "    optimizer = AdamW(optimizer_parameters, lr=config.lr)\n",
    "    num_train_steps = int(len(train_dataloader) * config.EPOCHS)\n",
    "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "#         optimizer, T_max=num_train_steps)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n",
    "    set_seed() # setting seed again before training\n",
    "    \n",
    "    with open(f'../Logs/{exp_name}.txt', 'w') as f:\n",
    "        f.write(f'{exp_name}'+'\\n')\n",
    "        for epoch in range(0,config.EPOCHS):\n",
    "            print(f'Current epoch is {epoch+1} of {config.EPOCHS}')\n",
    "\n",
    "            consistency_cost , classification_cost , overall_cost, global_step = train(train_dataloader, optimizer,scheduler,student,teacher,writer,global_step,epoch)\n",
    "\n",
    "            writer.add_scalar('Train_loss/Consistency_epoch :', consistency_cost,epoch )\n",
    "            writer.add_scalar('Train_loss/Classification_epoch', classification_cost,epoch)\n",
    "            writer.add_scalar('Train_loss/Overall_epoch',overall_cost,epoch)\n",
    "\n",
    "            epochs_consistency_lst.append(consistency_cost)\n",
    "            epochs_classification_lst.append(classification_cost)\n",
    "            epochs_overall_lst.append(overall_cost)\n",
    "\n",
    "        \n",
    "            print('---------Running validation for student -------------')\n",
    "            test_loss_student, avg_f1_score_0_student, avg_f1_score_1_student, avg_accuracy_score_student, avg_mcc_score_student = test(student,val_dataloader)\n",
    "\n",
    "            epochs_f1_score_0_lst_student.append(avg_f1_score_0_student)\n",
    "            epochs_f1_score_1_lst_student.append(avg_f1_score_1_student)\n",
    "            epochs_accuracy_lst_student.append(avg_accuracy_score_student)\n",
    "            epochs_mcc_lst_student.append(avg_mcc_score_student)\n",
    "            epochs_cost_lst_student.append(test_loss_student)\n",
    "\n",
    "            writer.add_scalar('mcc_score/Student',avg_mcc_score_student,epoch)\n",
    "            writer.add_scalar('Validation_loss/Student',test_loss_student,epoch)\n",
    "\n",
    "\n",
    "            for name, weight in student.named_parameters():\n",
    "\n",
    "                if name == 'bert.embeddings.LayerNorm.weight':\n",
    "                    writer.add_histogram(name,weight, epoch)\n",
    "                if name =='bert.encoder.layer.0.attention.output.LayerNorm.weight':\n",
    "                    writer.add_histogram(name,weight, epoch)\n",
    "                if name =='bert.encoder.layer.1.attention.output.dense.weight':\n",
    "                    writer.add_histogram(name,weight, epoch)\n",
    "                if name =='bert.encoder.layer.2.output.dense.weight':\n",
    "                    writer.add_histogram(name,weight, epoch)\n",
    "                if name == 'bert.encoder.layer.5.attention.self.key.weight':\n",
    "                    writer.add_histogram(name,weight, epoch)\n",
    "                if name == 'bert.encoder.layer.9.attention.self.key.weight':\n",
    "                    writer.add_histogram(name,weight, epoch)\n",
    "                if name == 'bert.encoder.layer.10.attention.output.LayerNorm.weight':\n",
    "                    writer.add_histogram(name,weight, epoch)\n",
    "                if name == 'bbert.encoder.layer.11.output.LayerNorm.weight':\n",
    "                    writer.add_histogram(name,weight, epoch)\n",
    "                if name == 'out_tag.weight':\n",
    "                    writer.add_histogram(name,weight, epoch)\n",
    "\n",
    "            if float(\"{:.2f}\".format(avg_mcc_score_student)) >  best_mcc_student :\n",
    "                    torch.save(student.state_dict(), f'../models/MeanTeacher_models/StudentModels/studentModel_{model_name}.bin')\n",
    "                    best_mcc_student = float(\"{:.2f}\".format(avg_mcc_score_student))\n",
    "\n",
    "            print('---------Running validation for teacher -------------')\n",
    "\n",
    "            test_loss_teacher, avg_f1_score_0_teacher, avg_f1_score_1_teacher, avg_accuracy_score_teacher, avg_mcc_score_teacher=test(teacher,val_dataloader)\n",
    "\n",
    "            epochs_f1_score_0_lst_teacher.append(avg_f1_score_1_teacher)\n",
    "            epochs_f1_score_1_lst_teacher.append(avg_f1_score_0_teacher)\n",
    "            epochs_accuracy_lst_teacher.append(avg_accuracy_score_teacher)\n",
    "            epochs_mcc_lst_teacher.append(avg_mcc_score_teacher)\n",
    "            epochs_cost_lst_teacher.append(test_loss_teacher)\n",
    "\n",
    "            writer.add_scalar('mcc_score/Teacher',avg_mcc_score_teacher,epoch)\n",
    "            writer.add_scalar('Validation_loss/Teacher',test_loss_teacher,epoch)\n",
    "\n",
    "            f.write(f\"Consistency_loss {epoch+1} : {str(consistency_cost)}\" + '\\n')\n",
    "            f.write(f\"Classification_loss {epoch+1} : {str(classification_cost)}\" + '\\n')\n",
    "            f.write(f\"Overall_loss {epoch+1} : {str(overall_cost)}\" + '\\n')\n",
    "            f.write(f\"'mcc_score/Teacher {epoch+1} : {str(avg_mcc_score_teacher)}\" + '\\n')\n",
    "            f.write(f\"ValidationLoss/Teacher {epoch+1} : {str(test_loss_teacher)}\" + '\\n')\n",
    "            f.write(f\"'mcc_score/Student {epoch+1} : {str(avg_mcc_score_student)}\" + '\\n')\n",
    "            f.write(f\"ValidationLoss/Student {epoch+1} : {str(test_loss_student)}\" + '\\n')\n",
    "\n",
    "            for name, weight in teacher.named_parameters():\n",
    "                if name == 'bert.embeddings.LayerNorm.weight':\n",
    "                    writer.add_histogram(name,weight, epoch)\n",
    "                if name =='bert.encoder.layer.0.attention.output.LayerNorm.weight':\n",
    "                    writer.add_histogram(name,weight, epoch)\n",
    "                if name =='bert.encoder.layer.1.attention.output.dense.weight':\n",
    "                    writer.add_histogram(name,weight, epoch)\n",
    "                if name =='bert.encoder.layer.2.output.dense.weight':\n",
    "                    writer.add_histogram(name,weight, epoch)\n",
    "                if name == 'bert.encoder.layer.5.attention.self.key.weight':\n",
    "                    writer.add_histogram(name,weight, epoch)\n",
    "                if name == 'bert.encoder.layer.9.attention.self.key.weight':\n",
    "                    writer.add_histogram(name,weight, epoch)\n",
    "                if name == 'bert.encoder.layer.10.attention.output.LayerNorm.weight':\n",
    "                    writer.add_histogram(name,weight, epoch)\n",
    "                if name == 'bbert.encoder.layer.11.output.LayerNorm.weight':\n",
    "                    writer.add_histogram(name,weight, epoch)\n",
    "                if name == 'out_tag.weight':\n",
    "                    writer.add_histogram(name,weight, epoch)\n",
    "\n",
    "\n",
    "            if float(\"{:.2f}\".format(avg_mcc_score_teacher)) >  best_mcc_teacher :\n",
    "                    torch.save(teacher.state_dict(), f'../models/MeanTeacher_models/TeacherModels/teacherModel_{model_name}.bin')\n",
    "                    best_mcc_teacher = float(\"{:.2f}\".format(avg_mcc_score_teacher))\n",
    "                    print(f'model saved for {epoch}th epoch')\n",
    "\n",
    "\n",
    "            if early_stopping.step(float(\"{:.2f}\".format(avg_mcc_score_teacher))): #mcc score for early stopping \"{:.2f}\".format(a_float)\n",
    "                      break \n",
    "\n",
    "   \n",
    "    return (epochs_consistency_lst,epochs_classification_lst,epochs_overall_lst,epochs_f1_score_0_lst_student,epochs_f1_score_1_lst_student,\n",
    "           epochs_accuracy_lst_student,epochs_mcc_lst_student,epochs_cost_lst_student,epochs_f1_score_0_lst_teacher,epochs_f1_score_1_lst_teacher,\n",
    "            epochs_accuracy_lst_teacher,epochs_mcc_lst_teacher,epochs_cost_lst_teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = reload(config)\n",
    "# config.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_tokens = np.array(test_metrics[27])\n",
    "# print(len(np.where(check_tokens==0)[0]))\n",
    "# check_tokens_p = np.array(test_metrics[28])\n",
    "# print(len(np.where(check_tokens_p==0)[0]))\n",
    "# print(len(test_metrics[27]))\n",
    "# print(len(test_metrics[28]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MeanTeacher_initWithsuptrainedmodel_withConsistencyRampup_for_5_epochs\n",
    "# MeanTeacher_withConsistencyRampup_for_1500_steps\n",
    "# teacherModel_v1_rampup_const_cost_1500_steps_withDropout\n",
    "# v1_initSupModel_rampup_const_cost_5_epochs_withDropout\n",
    "# MeanTeacher_withConsistencyRampup_for_500_steps_to_1_250_labeled_1750_unlabeled_noises_interchanged\n",
    "# v1_rampup_const_cost_500_steps_to_1_withDropout_250_labeled_1750_unlabeled_noises_interchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine = reload(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v1_with_250_labeled_1750_unlabeled_data_withdropout_alpha_resampledData2'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = f'semisupervised_with_{len(train_data)}_labeled_{unlabel_data_required}_unlabeled_data_withdropout_resampledData2'\n",
    "model_name = f'v1_with_{len(train_data)}_labeled_{unlabel_data_required}_unlabeled_data_withdropout_alpha_resampledData2'\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/conda/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 1 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:10<00:17,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.43700134637067095\n",
      "Consistency Cost : 0.3392463060468435\n",
      "Classification Cost : 0.41870374232530594\n",
      "Consistency Weight is: 0.16529888822158656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [01:27<00:00,  2.85it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:03<00:00, 16.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.6018, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.354868456739897\n",
      "Average F1 Validation score for whole sentence class 1 : 0.7671554711513988\n",
      "Average Accuracy Validation score whole sentence  : 0.6578144780757279\n",
      "Average mcc Validation score whole sentence : 0.2622192471066497\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.74      0.35      4418\n",
      "           1       0.94      0.65      0.77     30338\n",
      "\n",
      "    accuracy                           0.66     34756\n",
      "   macro avg       0.59      0.69      0.56     34756\n",
      "weighted avg       0.85      0.66      0.71     34756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:03<00:00, 16.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.6293, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.35310148705492034\n",
      "Average F1 Validation score for whole sentence class 1 : 0.792544601295765\n",
      "Average Accuracy Validation score whole sentence  : 0.6858384163885373\n",
      "Average mcc Validation score whole sentence : 0.2514408902219676\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.67      0.35      4418\n",
      "           1       0.94      0.69      0.79     30338\n",
      "\n",
      "    accuracy                           0.69     34756\n",
      "   macro avg       0.59      0.68      0.57     34756\n",
      "weighted avg       0.85      0.69      0.74     34756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved for 0th epoch\n",
      "Current epoch is 2 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:12<00:17,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.49557117765769365\n",
      "Consistency Cost : 0.17550245463848113\n",
      "Classification Cost : 0.3870862476527691\n",
      "Consistency Weight is: 0.951229424500714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [01:30<00:00,  2.76it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:03<00:00, 16.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.6112, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.3594968410268086\n",
      "Average F1 Validation score for whole sentence class 1 : 0.7833586816317886\n",
      "Average Accuracy Validation score whole sentence  : 0.6762285648521119\n",
      "Average mcc Validation score whole sentence : 0.26471436464941334\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.71      0.36      4418\n",
      "           1       0.94      0.67      0.78     30338\n",
      "\n",
      "    accuracy                           0.68     34756\n",
      "   macro avg       0.59      0.69      0.57     34756\n",
      "weighted avg       0.85      0.68      0.73     34756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:03<00:00, 16.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.6207, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.3657071857527438\n",
      "Average F1 Validation score for whole sentence class 1 : 0.8330031803725578\n",
      "Average Accuracy Validation score whole sentence  : 0.73561399470595\n",
      "Average mcc Validation score whole sentence : 0.26069066594775314\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.60      0.37      4418\n",
      "           1       0.93      0.76      0.83     30338\n",
      "\n",
      "    accuracy                           0.74     34756\n",
      "   macro avg       0.60      0.68      0.60     34756\n",
      "weighted avg       0.84      0.74      0.77     34756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved for 1th epoch\n",
      "Current epoch is 3 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:13<00:17,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.4754611954838037\n",
      "Consistency Cost : 0.12880889389663935\n",
      "Classification Cost : 0.34665230110287665\n",
      "Consistency Weight is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [01:30<00:00,  2.75it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:03<00:00, 17.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.6427, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.3689740916018288\n",
      "Average F1 Validation score for whole sentence class 1 : 0.8620913314050311\n",
      "Average Accuracy Validation score whole sentence  : 0.7736505927034181\n",
      "Average mcc Validation score whole sentence : 0.2614453919516613\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.52      0.37      4418\n",
      "           1       0.92      0.81      0.86     30338\n",
      "\n",
      "    accuracy                           0.77     34756\n",
      "   macro avg       0.60      0.67      0.62     34756\n",
      "weighted avg       0.84      0.77      0.80     34756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:03<00:00, 17.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.6257, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.37581871565158476\n",
      "Average F1 Validation score for whole sentence class 1 : 0.8525493962190329\n",
      "Average Accuracy Validation score whole sentence  : 0.7614512602140637\n",
      "Average mcc Validation score whole sentence : 0.2713024198912733\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.56      0.38      4418\n",
      "           1       0.93      0.79      0.85     30338\n",
      "\n",
      "    accuracy                           0.76     34756\n",
      "   macro avg       0.60      0.68      0.61     34756\n",
      "weighted avg       0.84      0.76      0.79     34756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved for 2th epoch\n",
      "Current epoch is 4 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:08<00:16,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.4462102504074574\n",
      "Consistency Cost : 0.12376986045390367\n",
      "Classification Cost : 0.3224403890967369\n",
      "Consistency Weight is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [01:24<00:00,  2.95it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:03<00:00, 17.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.6419, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.370791844476055\n",
      "Average F1 Validation score for whole sentence class 1 : 0.8599669351718316\n",
      "Average Accuracy Validation score whole sentence  : 0.7709172516975487\n",
      "Average mcc Validation score whole sentence : 0.2638854365549824\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.53      0.37      4418\n",
      "           1       0.92      0.81      0.86     30338\n",
      "\n",
      "    accuracy                           0.77     34756\n",
      "   macro avg       0.60      0.67      0.62     34756\n",
      "weighted avg       0.84      0.77      0.80     34756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:03<00:00, 17.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.6456, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.37290850836596656\n",
      "Average F1 Validation score for whole sentence class 1 : 0.8790925938636831\n",
      "Average Accuracy Validation score whole sentence  : 0.7972724133962481\n",
      "Average mcc Validation score whole sentence : 0.2671855449299469\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.47      0.37      4418\n",
      "           1       0.92      0.84      0.88     30338\n",
      "\n",
      "    accuracy                           0.80     34756\n",
      "   macro avg       0.61      0.66      0.63     34756\n",
      "weighted avg       0.84      0.80      0.81     34756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 5 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:08<00:16,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.33673534329980614\n",
      "Consistency Cost : 0.11137923255562782\n",
      "Classification Cost : 0.2253561115078628\n",
      "Consistency Weight is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [01:25<00:00,  2.92it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:03<00:00, 17.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.7225, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.35170969993021634\n",
      "Average F1 Validation score for whole sentence class 1 : 0.8906709705620283\n",
      "Average Accuracy Validation score whole sentence  : 0.8128956151455864\n",
      "Average mcc Validation score whole sentence : 0.24657946880906315\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.40      0.35      4418\n",
      "           1       0.91      0.87      0.89     30338\n",
      "\n",
      "    accuracy                           0.81     34756\n",
      "   macro avg       0.61      0.64      0.62     34756\n",
      "weighted avg       0.83      0.81      0.82     34756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:03<00:00, 17.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.6967, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.3578158241312937\n",
      "Average F1 Validation score for whole sentence class 1 : 0.893714055767006\n",
      "Average Accuracy Validation score whole sentence  : 0.8176142248820347\n",
      "Average mcc Validation score whole sentence : 0.25479210354646925\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.40      0.36      4418\n",
      "           1       0.91      0.88      0.89     30338\n",
      "\n",
      "    accuracy                           0.82     34756\n",
      "   macro avg       0.62      0.64      0.63     34756\n",
      "weighted avg       0.84      0.82      0.83     34756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 6 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:10<00:17,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.2646125303953886\n",
      "Consistency Cost : 0.10537009075284004\n",
      "Classification Cost : 0.15924244049936534\n",
      "Consistency Weight is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [01:27<00:00,  2.84it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:03<00:00, 16.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.8337, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.3123247592344264\n",
      "Average F1 Validation score for whole sentence class 1 : 0.9079906702115513\n",
      "Average Accuracy Validation score whole sentence  : 0.8376970882725285\n",
      "Average mcc Validation score whole sentence : 0.22178526597820203\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.29      0.31      4418\n",
      "           1       0.90      0.92      0.91     30338\n",
      "\n",
      "    accuracy                           0.84     34756\n",
      "   macro avg       0.62      0.60      0.61     34756\n",
      "weighted avg       0.83      0.84      0.83     34756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:03<00:00, 16.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.7606, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.34365156945802106\n",
      "Average F1 Validation score for whole sentence class 1 : 0.8997927203382804\n",
      "Average Accuracy Validation score whole sentence  : 0.8261307400161123\n",
      "Average mcc Validation score whole sentence : 0.243894890569092\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.36      0.34      4418\n",
      "           1       0.91      0.89      0.90     30338\n",
      "\n",
      "    accuracy                           0.83     34756\n",
      "   macro avg       0.62      0.63      0.62     34756\n",
      "weighted avg       0.83      0.83      0.83     34756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 7 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:08<00:16,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.24616737317293882\n",
      "Consistency Cost : 0.10047351453453303\n",
      "Classification Cost : 0.14569385818205774\n",
      "Consistency Weight is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [01:25<00:00,  2.92it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:03<00:00, 17.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.8500, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.33846840812681406\n",
      "Average F1 Validation score for whole sentence class 1 : 0.9021369356277043\n",
      "Average Accuracy Validation score whole sentence  : 0.8294970652549201\n",
      "Average mcc Validation score whole sentence : 0.24065559043625614\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.34      0.34      4418\n",
      "           1       0.90      0.90      0.90     30338\n",
      "\n",
      "    accuracy                           0.83     34756\n",
      "   macro avg       0.62      0.62      0.62     34756\n",
      "weighted avg       0.83      0.83      0.83     34756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:03<00:00, 17.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.8605, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.3292343387470998\n",
      "Average F1 Validation score for whole sentence class 1 : 0.9050449977008472\n",
      "Average Accuracy Validation score whole sentence  : 0.8336402347796064\n",
      "Average mcc Validation score whole sentence : 0.2344433138147628\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.32      0.33      4418\n",
      "           1       0.90      0.91      0.91     30338\n",
      "\n",
      "    accuracy                           0.83     34756\n",
      "   macro avg       0.62      0.61      0.62     34756\n",
      "weighted avg       0.83      0.83      0.83     34756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 8 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:08<00:16,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.22554008647799492\n",
      "Consistency Cost : 0.09521294279024005\n",
      "Classification Cost : 0.13032714328728617\n",
      "Consistency Weight is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [01:24<00:00,  2.94it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:03<00:00, 17.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(1.0396, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.28959153209390687\n",
      "Average F1 Validation score for whole sentence class 1 : 0.9157588143475532\n",
      "Average Accuracy Validation score whole sentence  : 0.849378524571297\n",
      "Average mcc Validation score whole sentence : 0.21439995480608978\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.24      0.29      4418\n",
      "           1       0.89      0.94      0.92     30338\n",
      "\n",
      "    accuracy                           0.85     34756\n",
      "   macro avg       0.63      0.59      0.60     34756\n",
      "weighted avg       0.83      0.85      0.84     34756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:03<00:00, 17.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.9719, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.31569796323894683\n",
      "Average F1 Validation score for whole sentence class 1 : 0.9103481939472828\n",
      "Average Accuracy Validation score whole sentence  : 0.8414662216595695\n",
      "Average mcc Validation score whole sentence : 0.22839188871133675\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.29      0.32      4418\n",
      "           1       0.90      0.92      0.91     30338\n",
      "\n",
      "    accuracy                           0.84     34756\n",
      "   macro avg       0.62      0.60      0.61     34756\n",
      "weighted avg       0.83      0.84      0.83     34756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 9 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:09<00:16,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.20911877293139697\n",
      "Consistency Cost : 0.08930019930005073\n",
      "Classification Cost : 0.1198185740748886\n",
      "Consistency Weight is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [01:26<00:00,  2.89it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:03<00:00, 17.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.9902, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.29934469200524244\n",
      "Average F1 Validation score for whole sentence class 1 : 0.9136097734397725\n",
      "Average Accuracy Validation score whole sentence  : 0.846184831396018\n",
      "Average mcc Validation score whole sentence : 0.21882666196970652\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.26      0.30      4418\n",
      "           1       0.90      0.93      0.91     30338\n",
      "\n",
      "    accuracy                           0.85     34756\n",
      "   macro avg       0.63      0.60      0.61     34756\n",
      "weighted avg       0.83      0.85      0.84     34756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:03<00:00, 17.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.9719, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.30675954592363264\n",
      "Average F1 Validation score for whole sentence class 1 : 0.9129857512953369\n",
      "Average Accuracy Validation score whole sentence  : 0.8453792150995512\n",
      "Average mcc Validation score whole sentence : 0.2244390373153901\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.27      0.31      4418\n",
      "           1       0.90      0.93      0.91     30338\n",
      "\n",
      "    accuracy                           0.85     34756\n",
      "   macro avg       0.63      0.60      0.61     34756\n",
      "weighted avg       0.83      0.85      0.84     34756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 10 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:12<00:17,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.16691393811255695\n",
      "Consistency Cost : 0.08387032335624099\n",
      "Classification Cost : 0.08304361493559555\n",
      "Consistency Weight is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [01:30<00:00,  2.76it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:03<00:00, 16.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(1.1199, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.29460470085470086\n",
      "Average F1 Validation score for whole sentence class 1 : 0.914839416999871\n",
      "Average Accuracy Validation score whole sentence  : 0.8480262400736563\n",
      "Average mcc Validation score whole sentence : 0.21695142373742696\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.25      0.29      4418\n",
      "           1       0.90      0.94      0.91     30338\n",
      "\n",
      "    accuracy                           0.85     34756\n",
      "   macro avg       0.63      0.59      0.60     34756\n",
      "weighted avg       0.83      0.85      0.84     34756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:03<00:00, 16.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(1.0601, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.30104125477790966\n",
      "Average F1 Validation score for whole sentence class 1 : 0.9143641501816714\n",
      "Average Accuracy Validation score whole sentence  : 0.8474220278513063\n",
      "Average mcc Validation score whole sentence : 0.22179638719780995\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.26      0.30      4418\n",
      "           1       0.90      0.93      0.91     30338\n",
      "\n",
      "    accuracy                           0.85     34756\n",
      "   macro avg       0.63      0.60      0.61     34756\n",
      "weighted avg       0.83      0.85      0.84     34756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 11 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:13<00:17,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.16044422781094908\n",
      "Consistency Cost : 0.08276496617123484\n",
      "Classification Cost : 0.07767926155822352\n",
      "Consistency Weight is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [01:31<00:00,  2.72it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:03<00:00, 16.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(1.2429, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.2776682624612785\n",
      "Average F1 Validation score for whole sentence class 1 : 0.917801634353469\n",
      "Average Accuracy Validation score whole sentence  : 0.8523995856830475\n",
      "Average mcc Validation score whole sentence : 0.20864615974715908\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.22      0.28      4418\n",
      "           1       0.89      0.94      0.92     30338\n",
      "\n",
      "    accuracy                           0.85     34756\n",
      "   macro avg       0.63      0.58      0.60     34756\n",
      "weighted avg       0.83      0.85      0.84     34756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:03<00:00, 16.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(1.1232, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.29436997319034847\n",
      "Average F1 Validation score for whole sentence class 1 : 0.9151679236769161\n",
      "Average Accuracy Validation score whole sentence  : 0.8485441362642422\n",
      "Average mcc Validation score whole sentence : 0.21741099450856566\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.25      0.29      4418\n",
      "           1       0.90      0.94      0.92     30338\n",
      "\n",
      "    accuracy                           0.85     34756\n",
      "   macro avg       0.63      0.59      0.60     34756\n",
      "weighted avg       0.83      0.85      0.84     34756\n",
      "\n",
      "terminating because of early stopping!\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(patience=8)\n",
    "# experiment_name = input('Enter Experiment name')\n",
    "# model_name = input('Enter name for the models')\n",
    "# exp_name = f'semisupervised_with_{len(train_data)}_labeled_{unlabel_data_required}_unlabeled_data_withdropout_alpha_0.99_noises_changed_t_0.5_s_0.3_retest'\n",
    "# model_name = f'v1_with_{len(train_data)}_labeled_{unlabel_data_required}_unlabeled_data_withdropout_alpha_0.99_noises_changed_t_0.5_s_0.3_retest'\n",
    "metrices = train_MeanTeacher(train_dataloader,val_dataloader,len_labelled_data,len_unlabelled_data,writer_semisupervised, early_stopping,exp_name,model_name,dropout_layer=True, noise_layer=True)\n",
    "# semisupervised_with_250_labeled_1750_unlabeled_data_withdropout\n",
    "# v1_with_250_labeled_1750_unlabeled_data_withdropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.268281227313873\n",
      "0.263273337759238\n"
     ]
    }
   ],
   "source": [
    "# def tensors_to_lst(lst):\n",
    "#     new_lst = []\n",
    "#     for items in lst:\n",
    "#         new_lst.append(items.item())\n",
    "#     return new_lst\n",
    "\n",
    "# def get_items(lst):\n",
    "#     new_lst = []\n",
    "#     for items in lst:\n",
    "#         new_lst.append(float(items))\n",
    "#     return new_lst\n",
    "\n",
    "\n",
    "# consistency_cost_lst = get_items(metrices[0])\n",
    "# classification_cost_lst = get_items(metrices[1])\n",
    "# overall_cost_lst = get_items(metrices[2])\n",
    "# f1_class0_student_test = metrices[3]\n",
    "# f1_class1_student_test= metrices[4]\n",
    "# accuracy_student = metrices[5]\n",
    "# mcc_cost_student = metrices[6]\n",
    "# test_cost_student = tensors_to_lst(metrices[7])\n",
    "# f1_class1_teacher_test=metrices[8]\n",
    "# f1_class0_teacher_test = metrices[9]\n",
    "# accuracy_teacher = metrices[10]\n",
    "# mcc_cost_teacher = metrices[11]\n",
    "# test_cost_teacher=tensors_to_lst(metrices[12])\n",
    "# max_mcc_student = np.amax(metrices[6])\n",
    "# max_mcc_teacher = np.amax(mcc_cost_teacher)\n",
    "# print(max_mcc_student)\n",
    "# print(max_mcc_teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Thesis/src/code/models/MeanTeacher_models/StudentModels/studentModel_v1_with_250_labeled_1750_unlabeled_data_withdropout_alpha_oldData.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v1_with_250_labeled_1750_unlabeled_data_withdropout_alpha_resampledData2'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name=f'v1_with_{label_data_required}_labeled_{unlabel_data_required}_unlabeled_data_withdropout_alpha_resampledData2'\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Thesis/src/code/code_files/engine.py:411: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  np_arr_labels = np.array(tar_labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for whole sentence class 0 : 0.37304724261246003\n",
      "Average F1 Validation score for whole sentence class 1 : 0.7741673042838187\n",
      "Average Accuracy Validation score whole sentence  : 0.6679436058103104\n",
      "Average mcc Validation score whole sentence : 0.27993996378879654\n",
      "Average F1 Validation score for source sentence class 0 : 0.35814277570152653\n",
      "Average F1 Validation score for source sentence class 1 : 0.7181598197646927\n",
      "Average Accuracy Validation score source sentence  : 0.6083107846926943\n",
      "Average mcc Validation score source sentence : 0.23015406411587624\n",
      "Average F1 Validation score for target sentence class 0 : 0.39238149792352855\n",
      "Average F1 Validation score for target sentence class 1 : 0.620086554245635\n",
      "Average Accuracy Validation score target sentence  : 0.532486135086495\n",
      "Average mcc Validation score target sentence : 0.23090011862286192\n",
      "Average F1 Validation score for gaps in target sentence class 0 : 0.0\n",
      "Average F1 Validation score for gaps in target sentence class 1 : 0.9865348761504543\n",
      "Average Accuracy Validation score gaps in target sentence  : 0.9734275535829002\n",
      "Average mcc Validation score gaps in target sentence : 0.0\n",
      "Average F1 Validation score for whole target sentence class 0 : 0.38404036628667537\n",
      "Average F1 Validation score for whole target sentence class 1 : 0.8040893961008083\n",
      "Average Accuracy Validation score whole target sentence  : 0.702728297632469\n",
      "Average mcc Validation score whole target sentence : 0.3092285902991288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.27993996378879654"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = EntityModel(with_noise_layer=False, dropout_layer=False) \n",
    "# model.cuda()\n",
    "# Thesis/src/code/models/MeanTeacher_models/StudentModels/studentModel_v1_with_250_labeled_1750_unlabeled_data_withdropout_alpha_resampledData.bin\n",
    "model.load_state_dict(torch.load(f'../models/MeanTeacher_models/StudentModels/studentModel_{model_name}.bin'))\n",
    "model.eval() # for 1k, 1.0 k data- teacher model -- less seq len\n",
    "test_metrics = engine.eval_fn(test_dataloader, model)\n",
    "student_mcc_test = test_metrics[4]# student\n",
    "student_mcc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Thesis/src/code/code_files/engine.py:411: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  np_arr_labels = np.array(tar_labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for whole sentence class 0 : 0.38945292527938064\n",
      "Average F1 Validation score for whole sentence class 1 : 0.8521289957367014\n",
      "Average Accuracy Validation score whole sentence  : 0.7619196810025634\n",
      "Average mcc Validation score whole sentence : 0.2830861334619802\n",
      "Average F1 Validation score for source sentence class 0 : 0.3733573357335733\n",
      "Average F1 Validation score for source sentence class 1 : 0.8286487816884075\n",
      "Average Accuracy Validation score source sentence  : 0.7308851952068033\n",
      "Average mcc Validation score source sentence : 0.24083357876610945\n",
      "Average F1 Validation score for target sentence class 0 : 0.4119618132389201\n",
      "Average F1 Validation score for target sentence class 1 : 0.7592722002225846\n",
      "Average Accuracy Validation score target sentence  : 0.6583905681859918\n",
      "Average mcc Validation score target sentence : 0.24848986279792368\n",
      "Average F1 Validation score for gaps in target sentence class 0 : 0.0\n",
      "Average F1 Validation score for gaps in target sentence class 1 : 0.9865348761504543\n",
      "Average Accuracy Validation score gaps in target sentence  : 0.9734275535829002\n",
      "Average mcc Validation score gaps in target sentence : 0.0\n",
      "Average F1 Validation score for whole target sentence class 0 : 0.40044247787610615\n",
      "Average F1 Validation score for whole target sentence class 1 : 0.8653007124316563\n",
      "Average Accuracy Validation score whole target sentence  : 0.7800225479143179\n",
      "Average mcc Validation score whole target sentence : 0.30934649451011026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2830861334619802"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.load_state_dict(torch.load(f'../models/MeanTeacher_models/TeacherModels/teacherModel_{model_name}.bin'))\n",
    "model.eval() # for 1k, 1.0 k data- teacher model -- less seq len\n",
    "test_metrics = engine.eval_fn(test_dataloader, model)\n",
    "# Thesis/src/code/models/MeanTeacher_models/TeacherModels/teacherModel_v1_rampup_const_cost_500_steps_to_1_withDropout_250_labeled_1750_unlabeled.bin\n",
    "teacher_mcc_test = test_metrics[4]\n",
    "teacher_mcc_test\n",
    "# Thesis/src/code/models/MeanTeacher_models/TeacherModels/teacherModel_v1_with_1000_labeled_1000_unlabeled_data_withdropout_alpha_0.995.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v1_with_500_labeled_1500_unlabeled_data_withdropout_alpha_0.99_noises_changed_t_0.5_s_0.3'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Consistency_lost_train :',consistency_cost_lst,'\\n',\n",
    "#       'Classificataion_lost_train : ',classification_cost_lst,'\\n',\n",
    "#       'Overall_loss_train :',overall_cost_lst,'\\n',\n",
    "#       'f1_class0_student_test :',f1_class0_student_test,'\\n',\n",
    "#       'f1_class1_student_test :',f1_class1_student_test,'\\n',\n",
    "#       'accuracy_student :',accuracy_student,'\\n',\n",
    "#       'mcc_cost_student :',mcc_cost_student,'\\n',\n",
    "#       'test_cost_student :',test_cost_student,'\\n',\n",
    "#       'f1_class0_teacher_test :',f1_class0_teacher_test,'\\n',\n",
    "#       'f1_class1_teacher_test :',f1_class1_teacher_test,'\\n',\n",
    "#     'accuracy_teacher :', accuracy_teacher,'\\n',\n",
    "#       'mcc_cost_teacher',mcc_cost_teacher,'\\n',\n",
    "#       'test_cost_teacher',test_cost_teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "writer_semisupervised.add_hparams(\n",
    "                    { \"lr\": float(config.lr),  \n",
    "                     \"labelled_data\": len_labelled_data, \n",
    "                     \"validation_data\":len(eval_data_ssl),\n",
    "                     \"alpha\":config.alpha, \n",
    "                     \"teacher_noise_gaussian_std\":config.gaussian_noise_std_teacher,\n",
    "                     \"student_noise_gaussian_std\":config.gaussian_noise_std_student,\n",
    "                     \"unlabelled_data\":len_unlabelled_data, \n",
    "                    },\n",
    "                    {\n",
    "                    \"mcc_score_test\" : float(0),\n",
    "                    \"mcc_score_teacher\": float(student_mcc_test) ,\n",
    "                    \"mcc_score_student\" : float(teacher_mcc_test),\n",
    "                    },\n",
    "                )\n",
    "writer_semisupervised.add_text('text', f'Ramup from 0 to 1 in 500 steps. Also, noise used in teacher is more than student 0.5 and 0.3 respectively for teacher and student models. {label_data_required} labeled and {unlabel_data_required} unlabeled. Alpha is 0.99')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.gaussian_noise_std_teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-67-2b3a9e4e0103>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-67-2b3a9e4e0103>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Run till this point\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Run till this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Experiment name : MeanTeacher_withConsistencyRampup_for_600_steps_to_1_1000_labeled_1000_unlabeled_alpha_adjusted_check\n",
      "Enter name for saving the models with version : v1_rampup_const_cost_600_steps_to_1_withDropout_1000_labeled_1000_unlabeled_alpha_adjusted_check\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/conda/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 1 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /codebuild/output/src811146734/src/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  app.launch_new_instance()\n",
      "  4%|▍         | 11/250 [00:06<02:12,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 21/250 [00:11<02:03,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9523809523809523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 31/250 [00:17<01:58,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.967741935483871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:51<00:27,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.6507706062775105\n",
      "Consistency Cost : 0.4398407321423292\n",
      "Classification Cost : 0.6353269508481025\n",
      "Consistency Weight is: 0.10836802322189582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:19<00:00,  1.80it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:07<00:00,  8.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.5888, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.34186596583442835\n",
      "Average F1 Validation score for whole sentence class 1 : 0.7450157825068731\n",
      "Average Accuracy Validation score whole sentence  : 0.6324379862028475\n",
      "Average mcc Validation score whole sentence : 0.26285807458677524\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.79      0.34      4122\n",
      "           1       0.95      0.61      0.75     29943\n",
      "\n",
      "    accuracy                           0.63     34065\n",
      "   macro avg       0.59      0.70      0.54     34065\n",
      "weighted avg       0.87      0.63      0.70     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:07<00:00,  8.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.6231, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.3331091915377851\n",
      "Average F1 Validation score for whole sentence class 1 : 0.7357829374756645\n",
      "Average Accuracy Validation score whole sentence  : 0.6215176867752825\n",
      "Average mcc Validation score whole sentence : 0.2497578093538212\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.78      0.33      4122\n",
      "           1       0.95      0.60      0.74     29943\n",
      "\n",
      "    accuracy                           0.62     34065\n",
      "   macro avg       0.58      0.69      0.53     34065\n",
      "weighted avg       0.86      0.62      0.69     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 2 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:54<00:28,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.6618445229530334\n",
      "Consistency Cost : 0.193541909083724\n",
      "Classification Cost : 0.5814238642156124\n",
      "Consistency Weight is: 0.7316156289466418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:22<00:00,  1.75it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:07<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.5723, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.3611275612933791\n",
      "Average F1 Validation score for whole sentence class 1 : 0.7894777925220513\n",
      "Average Accuracy Validation score whole sentence  : 0.6833113166006165\n",
      "Average mcc Validation score whole sentence : 0.279773841430625\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.74      0.36      4122\n",
      "           1       0.95      0.68      0.79     29943\n",
      "\n",
      "    accuracy                           0.68     34065\n",
      "   macro avg       0.59      0.71      0.58     34065\n",
      "weighted avg       0.86      0.68      0.74     34065\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-a6411e94364f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmetrices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_MeanTeacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen_labelled_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen_unlabelled_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-74161f607cb0>\u001b[0m in \u001b[0;36mtrain_MeanTeacher\u001b[0;34m(train_dataloader, val_dataloader, len_labelled_data, len_unlabelled_data, writer, early_stopping, dropout_layer, noise_layer)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_mcc_score_student\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m  \u001b[0mbest_mcc_student\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'../models/MeanTeacher_models/StudentModels/studentModel_{model_name}.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                     \u001b[0mbest_mcc_student\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_mcc_score_student\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    364\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(patience=8)\n",
    "metrices = train_MeanTeacher(train_dataloader,val_dataloader,len_labelled_data,len_unlabelled_data,writer, early_stopping,dropout_layer=True, noise_layer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Experiment name : MeanTeacher_withConsistencyRampup_for_600_steps_to_1_1000_labeled_1000_unlabeled_alpha_adjusted\n",
      "Enter name for saving the models with version : v1_rampup_const_cost_600_steps_to_1_withDropout_1000_labeled_1000_unlabeled_alpha_adjusted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/conda/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 1 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:55<00:29,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.6507706062775105\n",
      "Consistency Cost : 0.4398407321423292\n",
      "Classification Cost : 0.6353269508481025\n",
      "Consistency Weight is: 0.10836802322189582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:25<00:00,  1.72it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:07<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.5888, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.34186596583442835\n",
      "Average F1 Validation score for whole sentence class 1 : 0.7450157825068731\n",
      "Average Accuracy Validation score whole sentence  : 0.6324379862028475\n",
      "Average mcc Validation score whole sentence : 0.26285807458677524\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.79      0.34      4122\n",
      "           1       0.95      0.61      0.75     29943\n",
      "\n",
      "    accuracy                           0.63     34065\n",
      "   macro avg       0.59      0.70      0.54     34065\n",
      "weighted avg       0.87      0.63      0.70     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:07<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.6231, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.3331091915377851\n",
      "Average F1 Validation score for whole sentence class 1 : 0.7357829374756645\n",
      "Average Accuracy Validation score whole sentence  : 0.6215176867752825\n",
      "Average mcc Validation score whole sentence : 0.2497578093538212\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.78      0.33      4122\n",
      "           1       0.95      0.60      0.74     29943\n",
      "\n",
      "    accuracy                           0.62     34065\n",
      "   macro avg       0.58      0.69      0.53     34065\n",
      "weighted avg       0.86      0.62      0.69     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 2 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [02:01<00:29,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.6618445229530334\n",
      "Consistency Cost : 0.193541909083724\n",
      "Classification Cost : 0.5814238642156124\n",
      "Consistency Weight is: 0.7316156289466418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:31<00:00,  1.65it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:07<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.5723, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.3611275612933791\n",
      "Average F1 Validation score for whole sentence class 1 : 0.7894777925220513\n",
      "Average Accuracy Validation score whole sentence  : 0.6833113166006165\n",
      "Average mcc Validation score whole sentence : 0.279773841430625\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.74      0.36      4122\n",
      "           1       0.95      0.68      0.79     29943\n",
      "\n",
      "    accuracy                           0.68     34065\n",
      "   macro avg       0.59      0.71      0.58     34065\n",
      "weighted avg       0.86      0.68      0.74     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:07<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.5883, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.35459619546645404\n",
      "Average F1 Validation score for whole sentence class 1 : 0.7759234358933798\n",
      "Average Accuracy Validation score whole sentence  : 0.6673418464699838\n",
      "Average mcc Validation score whole sentence : 0.27390460591384436\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.76      0.35      4122\n",
      "           1       0.95      0.66      0.78     29943\n",
      "\n",
      "    accuracy                           0.67     34065\n",
      "   macro avg       0.59      0.71      0.57     34065\n",
      "weighted avg       0.86      0.67      0.72     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 3 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:55<00:26,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.6834059897065162\n",
      "Consistency Cost : 0.1498090098798275\n",
      "Classification Cost : 0.5370685669779778\n",
      "Consistency Weight is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:22<00:00,  1.76it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:06<00:00,  9.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.5714, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.359741355074501\n",
      "Average F1 Validation score for whole sentence class 1 : 0.7738206376005562\n",
      "Average Accuracy Validation score whole sentence  : 0.665727286070747\n",
      "Average mcc Validation score whole sentence : 0.28387618922460817\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.78      0.36      4122\n",
      "           1       0.95      0.65      0.77     29943\n",
      "\n",
      "    accuracy                           0.67     34065\n",
      "   macro avg       0.59      0.71      0.57     34065\n",
      "weighted avg       0.87      0.67      0.72     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:06<00:00,  9.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.5763, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.3688155922038981\n",
      "Average F1 Validation score for whole sentence class 1 : 0.806147116380799\n",
      "Average Accuracy Validation score whole sentence  : 0.7033905768383972\n",
      "Average mcc Validation score whole sentence : 0.28587170143879026\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.72      0.37      4122\n",
      "           1       0.95      0.70      0.81     29943\n",
      "\n",
      "    accuracy                           0.70     34065\n",
      "   macro avg       0.60      0.71      0.59     34065\n",
      "weighted avg       0.86      0.70      0.75     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 4 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:49<00:26,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.6329384277760982\n",
      "Consistency Cost : 0.13410346314311028\n",
      "Classification Cost : 0.49883496075868605\n",
      "Consistency Weight is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:16<00:00,  1.84it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:06<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.5521, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.39303991811668376\n",
      "Average F1 Validation score for whole sentence class 1 : 0.8475354440608243\n",
      "Average Accuracy Validation score whole sentence  : 0.7562894466461177\n",
      "Average mcc Validation score whole sentence : 0.30688499722525686\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.65      0.39      4122\n",
      "           1       0.94      0.77      0.85     29943\n",
      "\n",
      "    accuracy                           0.76     34065\n",
      "   macro avg       0.61      0.71      0.62     34065\n",
      "weighted avg       0.86      0.76      0.79     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:06<00:00,  9.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.5594, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.384313184919705\n",
      "Average F1 Validation score for whole sentence class 1 : 0.8334048227373603\n",
      "Average Accuracy Validation score whole sentence  : 0.7377660355203288\n",
      "Average mcc Validation score whole sentence : 0.2993655055140609\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.68      0.38      4122\n",
      "           1       0.94      0.75      0.83     29943\n",
      "\n",
      "    accuracy                           0.74     34065\n",
      "   macro avg       0.61      0.71      0.61     34065\n",
      "weighted avg       0.86      0.74      0.78     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 5 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:56<00:29,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.578609249740839\n",
      "Consistency Cost : 0.1259544885158539\n",
      "Classification Cost : 0.45265476159751417\n",
      "Consistency Weight is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:26<00:00,  1.71it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:07<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.5497, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.4086652007492466\n",
      "Average F1 Validation score for whole sentence class 1 : 0.8699933752305241\n",
      "Average Accuracy Validation score whole sentence  : 0.7868486716571261\n",
      "Average mcc Validation score whole sentence : 0.32101908556910497\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.61      0.41      4122\n",
      "           1       0.94      0.81      0.87     29943\n",
      "\n",
      "    accuracy                           0.79     34065\n",
      "   macro avg       0.62      0.71      0.64     34065\n",
      "weighted avg       0.86      0.79      0.81     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:07<00:00,  8.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.5622, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.40028155795401216\n",
      "Average F1 Validation score for whole sentence class 1 : 0.861448395490026\n",
      "Average Accuracy Validation score whole sentence  : 0.7749009247027742\n",
      "Average mcc Validation score whole sentence : 0.312232297079589\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.62      0.40      4122\n",
      "           1       0.94      0.80      0.86     29943\n",
      "\n",
      "    accuracy                           0.77     34065\n",
      "   macro avg       0.62      0.71      0.63     34065\n",
      "weighted avg       0.86      0.77      0.81     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 6 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:54<00:26,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.5105033388361335\n",
      "Consistency Cost : 0.11641626179218292\n",
      "Classification Cost : 0.3940870776027441\n",
      "Consistency Weight is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:21<00:00,  1.77it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:06<00:00,  9.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.5565, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.4083471662319184\n",
      "Average F1 Validation score for whole sentence class 1 : 0.8650840858703294\n",
      "Average Accuracy Validation score whole sentence  : 0.7802730074856892\n",
      "Average mcc Validation score whole sentence : 0.32224119948520497\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.63      0.41      4122\n",
      "           1       0.94      0.80      0.87     29943\n",
      "\n",
      "    accuracy                           0.78     34065\n",
      "   macro avg       0.62      0.71      0.64     34065\n",
      "weighted avg       0.86      0.78      0.81     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:06<00:00,  9.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.5538, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.41576182136602446\n",
      "Average F1 Validation score for whole sentence class 1 : 0.8823487921001587\n",
      "Average Accuracy Validation score whole sentence  : 0.8041391457507706\n",
      "Average mcc Validation score whole sentence : 0.32708248581371663\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.58      0.42      4122\n",
      "           1       0.93      0.84      0.88     29943\n",
      "\n",
      "    accuracy                           0.80     34065\n",
      "   macro avg       0.63      0.71      0.65     34065\n",
      "weighted avg       0.86      0.80      0.83     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 7 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:49<00:26,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.47228979177773\n",
      "Consistency Cost : 0.11324455369263887\n",
      "Classification Cost : 0.35904523827135565\n",
      "Consistency Weight is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:15<00:00,  1.84it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:06<00:00,  9.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.5694, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.42196163985092267\n",
      "Average F1 Validation score for whole sentence class 1 : 0.8886905074480562\n",
      "Average Accuracy Validation score whole sentence  : 0.8133274622046088\n",
      "Average mcc Validation score whole sentence : 0.333774732241063\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.56      0.42      4122\n",
      "           1       0.93      0.85      0.89     29943\n",
      "\n",
      "    accuracy                           0.81     34065\n",
      "   macro avg       0.64      0.71      0.66     34065\n",
      "weighted avg       0.86      0.81      0.83     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:06<00:00,  9.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.5710, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.42609365966467533\n",
      "Average F1 Validation score for whole sentence class 1 : 0.8968693724892643\n",
      "Average Accuracy Validation score whole sentence  : 0.8251577865844708\n",
      "Average mcc Validation score whole sentence : 0.3380102405478781\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.54      0.43      4122\n",
      "           1       0.93      0.86      0.90     29943\n",
      "\n",
      "    accuracy                           0.83     34065\n",
      "   macro avg       0.64      0.70      0.66     34065\n",
      "weighted avg       0.86      0.83      0.84     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 8 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:50<00:27,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.4151100990176201\n",
      "Consistency Cost : 0.10775427658110857\n",
      "Classification Cost : 0.3073558236286044\n",
      "Consistency Weight is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:17<00:00,  1.81it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:06<00:00,  9.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.6327, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.4102850754611515\n",
      "Average F1 Validation score for whole sentence class 1 : 0.9108726873363183\n",
      "Average Accuracy Validation score whole sentence  : 0.8451489798913842\n",
      "Average mcc Validation score whole sentence : 0.3231022588010173\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.45      0.41      4122\n",
      "           1       0.92      0.90      0.91     29943\n",
      "\n",
      "    accuracy                           0.85     34065\n",
      "   macro avg       0.65      0.67      0.66     34065\n",
      "weighted avg       0.86      0.85      0.85     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:07<00:00,  8.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.5871, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.4320213749871544\n",
      "Average F1 Validation score for whole sentence class 1 : 0.905357968458364\n",
      "Average Accuracy Validation score whole sentence  : 0.8377513576985175\n",
      "Average mcc Validation score whole sentence : 0.3454380086870494\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.51      0.43      4122\n",
      "           1       0.93      0.88      0.91     29943\n",
      "\n",
      "    accuracy                           0.84     34065\n",
      "   macro avg       0.65      0.70      0.67     34065\n",
      "weighted avg       0.86      0.84      0.85     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 9 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [02:00<00:29,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.36140915922820566\n",
      "Consistency Cost : 0.10124831279739738\n",
      "Classification Cost : 0.26016084644943477\n",
      "Consistency Weight is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:30<00:00,  1.66it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:07<00:00,  8.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.6364, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.41714846732846084\n",
      "Average F1 Validation score for whole sentence class 1 : 0.9093838508895409\n",
      "Average Accuracy Validation score whole sentence  : 0.843152796125055\n",
      "Average mcc Validation score whole sentence : 0.3298248006538421\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.46      0.42      4122\n",
      "           1       0.92      0.90      0.91     29943\n",
      "\n",
      "    accuracy                           0.84     34065\n",
      "   macro avg       0.65      0.68      0.66     34065\n",
      "weighted avg       0.86      0.84      0.85     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:07<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.6150, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.42143406171077197\n",
      "Average F1 Validation score for whole sentence class 1 : 0.910729715534622\n",
      "Average Accuracy Validation score whole sentence  : 0.845325113753119\n",
      "Average mcc Validation score whole sentence : 0.3350929373157651\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.47      0.42      4122\n",
      "           1       0.92      0.90      0.91     29943\n",
      "\n",
      "    accuracy                           0.85     34065\n",
      "   macro avg       0.65      0.68      0.67     34065\n",
      "weighted avg       0.86      0.85      0.85     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 10 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [02:02<00:29,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.32515245221555233\n",
      "Consistency Cost : 0.09873415146023035\n",
      "Classification Cost : 0.22641830025240778\n",
      "Consistency Weight is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:31<00:00,  1.65it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:07<00:00,  8.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.7178, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.40980225646002666\n",
      "Average F1 Validation score for whole sentence class 1 : 0.9187636715814784\n",
      "Average Accuracy Validation score whole sentence  : 0.8571847937766035\n",
      "Average mcc Validation score whole sentence : 0.3285659325308531\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.41      0.41      4122\n",
      "           1       0.92      0.92      0.92     29943\n",
      "\n",
      "    accuracy                           0.86     34065\n",
      "   macro avg       0.66      0.66      0.66     34065\n",
      "weighted avg       0.86      0.86      0.86     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:07<00:00,  8.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.6542, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.41705282669138094\n",
      "Average F1 Validation score for whole sentence class 1 : 0.9154257285959192\n",
      "Average Accuracy Validation score whole sentence  : 0.8522824012916483\n",
      "Average mcc Validation score whole sentence : 0.3331154400061646\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.44      0.42      4122\n",
      "           1       0.92      0.91      0.92     29943\n",
      "\n",
      "    accuracy                           0.85     34065\n",
      "   macro avg       0.66      0.67      0.67     34065\n",
      "weighted avg       0.86      0.85      0.86     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 11 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:57<00:26,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.30029949981719256\n",
      "Consistency Cost : 0.09844230867922306\n",
      "Classification Cost : 0.20185719151981174\n",
      "Consistency Weight is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:24<00:00,  1.73it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:06<00:00,  9.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.7548, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.3965811965811966\n",
      "Average F1 Validation score for whole sentence class 1 : 0.9175508842175509\n",
      "Average Accuracy Validation score whole sentence  : 0.8549244092176721\n",
      "Average mcc Validation score whole sentence : 0.31414497677364983\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.39      0.40      4122\n",
      "           1       0.92      0.92      0.92     29943\n",
      "\n",
      "    accuracy                           0.85     34065\n",
      "   macro avg       0.66      0.66      0.66     34065\n",
      "weighted avg       0.85      0.85      0.85     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:06<00:00,  9.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.7018, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.40841494377947046\n",
      "Average F1 Validation score for whole sentence class 1 : 0.9182579060792864\n",
      "Average Accuracy Validation score whole sentence  : 0.8563628357551739\n",
      "Average mcc Validation score whole sentence : 0.32667609614293824\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.41      0.41      4122\n",
      "           1       0.92      0.92      0.92     29943\n",
      "\n",
      "    accuracy                           0.86     34065\n",
      "   macro avg       0.66      0.66      0.66     34065\n",
      "weighted avg       0.86      0.86      0.86     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 12 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:50<00:27,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.2751480747386813\n",
      "Consistency Cost : 0.09618175240233541\n",
      "Classification Cost : 0.17896632244810462\n",
      "Consistency Weight is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:17<00:00,  1.81it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:06<00:00,  9.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.9175, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.3817829457364341\n",
      "Average F1 Validation score for whole sentence class 1 : 0.9266738909138674\n",
      "Average Accuracy Validation score whole sentence  : 0.8688976955819756\n",
      "Average mcc Validation score whole sentence : 0.3140109280596588\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.33      0.38      4122\n",
      "           1       0.91      0.94      0.93     29943\n",
      "\n",
      "    accuracy                           0.87     34065\n",
      "   macro avg       0.68      0.64      0.65     34065\n",
      "weighted avg       0.85      0.87      0.86     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:06<00:00,  9.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.7536, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.406288685318148\n",
      "Average F1 Validation score for whole sentence class 1 : 0.920135882235396\n",
      "Average Accuracy Validation score whole sentence  : 0.8592103331865552\n",
      "Average mcc Validation score whole sentence : 0.32655157633549337\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.40      0.41      4122\n",
      "           1       0.92      0.92      0.92     29943\n",
      "\n",
      "    accuracy                           0.86     34065\n",
      "   macro avg       0.67      0.66      0.66     34065\n",
      "weighted avg       0.86      0.86      0.86     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 13 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:58<00:28,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.24575058571994304\n",
      "Consistency Cost : 0.08938711406663061\n",
      "Classification Cost : 0.15636347279883922\n",
      "Consistency Weight is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:26<00:00,  1.71it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]9.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:06<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.8412, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.4022677490014173\n",
      "Average F1 Validation score for whole sentence class 1 : 0.9231559243982839\n",
      "Average Accuracy Validation score whole sentence  : 0.8638191692352855\n",
      "Average mcc Validation score whole sentence : 0.32656505228941407\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.38      0.40      4122\n",
      "           1       0.92      0.93      0.92     29943\n",
      "\n",
      "    accuracy                           0.86     34065\n",
      "   macro avg       0.67      0.65      0.66     34065\n",
      "weighted avg       0.86      0.86      0.86     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 14 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:50<00:26,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.21835778195410968\n",
      "Consistency Cost : 0.08650215949863195\n",
      "Classification Cost : 0.13185562253464014\n",
      "Consistency Weight is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:17<00:00,  1.82it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:06<00:00,  9.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(1.0128, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.38831659929586654\n",
      "Average F1 Validation score for whole sentence class 1 : 0.9224127950248922\n",
      "Average Accuracy Validation score whole sentence  : 0.8622926757669162\n",
      "Average mcc Validation score whole sentence : 0.3123356791186087\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.36      0.39      4122\n",
      "           1       0.91      0.93      0.92     29943\n",
      "\n",
      "    accuracy                           0.86     34065\n",
      "   macro avg       0.67      0.65      0.66     34065\n",
      "weighted avg       0.85      0.86      0.86     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:06<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(0.9391, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.3920988981456598\n",
      "Average F1 Validation score for whole sentence class 1 : 0.9254547851305035\n",
      "Average Accuracy Validation score whole sentence  : 0.8671950682518714\n",
      "Average mcc Validation score whole sentence : 0.3208743975375306\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.35      0.39      4122\n",
      "           1       0.91      0.94      0.93     29943\n",
      "\n",
      "    accuracy                           0.87     34065\n",
      "   macro avg       0.68      0.65      0.66     34065\n",
      "weighted avg       0.86      0.87      0.86     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 15 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:55<00:29,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.21577567676082254\n",
      "Consistency Cost : 0.0838872840628028\n",
      "Classification Cost : 0.13188839299138636\n",
      "Consistency Weight is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:24<00:00,  1.73it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:07<00:00,  8.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(1.1569, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.37494769144929563\n",
      "Average F1 Validation score for whole sentence class 1 : 0.9264939879595151\n",
      "Average Accuracy Validation score whole sentence  : 0.8684573609276384\n",
      "Average mcc Validation score whole sentence : 0.3076112804224331\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.33      0.37      4122\n",
      "           1       0.91      0.94      0.93     29943\n",
      "\n",
      "    accuracy                           0.87     34065\n",
      "   macro avg       0.68      0.63      0.65     34065\n",
      "weighted avg       0.85      0.87      0.86     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:07<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(1.0667, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.38393223117912284\n",
      "Average F1 Validation score for whole sentence class 1 : 0.9258522306819491\n",
      "Average Accuracy Validation score whole sentence  : 0.8676354029062087\n",
      "Average mcc Validation score whole sentence : 0.3142628805414049\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.34      0.38      4122\n",
      "           1       0.91      0.94      0.93     29943\n",
      "\n",
      "    accuracy                           0.87     34065\n",
      "   macro avg       0.68      0.64      0.65     34065\n",
      "weighted avg       0.85      0.87      0.86     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch is 16 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 201/250 [01:51<00:26,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  0.19508064774796366\n",
      "Consistency Cost : 0.08055738933384418\n",
      "Classification Cost : 0.11452325810678303\n",
      "Consistency Weight is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:18<00:00,  1.81it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for student -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:06<00:00,  9.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(1.1754, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.364396999422966\n",
      "Average F1 Validation score for whole sentence class 1 : 0.92800418314324\n",
      "Average Accuracy Validation score whole sentence  : 0.8706590341993248\n",
      "Average mcc Validation score whole sentence : 0.3019851510238544\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.31      0.36      4122\n",
      "           1       0.91      0.95      0.93     29943\n",
      "\n",
      "    accuracy                           0.87     34065\n",
      "   macro avg       0.68      0.63      0.65     34065\n",
      "weighted avg       0.85      0.87      0.86     34065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Running validation for teacher -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:06<00:00,  9.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation loss: tensor(1.1576, device='cuda:0')\n",
      "Average F1 Validation score for whole sentence class 0 : 0.38059598059598065\n",
      "Average F1 Validation score for whole sentence class 1 : 0.9266354756628087\n",
      "Average Accuracy Validation score whole sentence  : 0.8688096286511082\n",
      "Average mcc Validation score whole sentence : 0.3128832519403145\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.33      0.38      4122\n",
      "           1       0.91      0.94      0.93     29943\n",
      "\n",
      "    accuracy                           0.87     34065\n",
      "   macro avg       0.68      0.64      0.65     34065\n",
      "weighted avg       0.85      0.87      0.86     34065\n",
      "\n",
      "terminating because of early stopping!\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(patience=8)\n",
    "metrices = train_MeanTeacher(train_dataloader,val_dataloader,len_labelled_data,len_unlabelled_data,writer, early_stopping,dropout_layer=True, noise_layer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail hoga ya pe\n",
    "metrices = train_MeanTeacher(train_dataloader,val_dataloader,len_labelled_data,len_unlabelled_data,writer, early_stopping,dropout_layer=True, noise_layer=True) # alpha = 0.995 and full data w/o noise and dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consistency_cost_lst = get_items(metrices[0])\n",
    "classification_cost_lst = get_items(metrices[1])\n",
    "overall_cost_lst = get_items(metrices[2])\n",
    "f1_class0_student_test = metrices[3]\n",
    "f1_class1_student_test= metrices[4]\n",
    "accuracy_student = metrices[5]\n",
    "mcc_cost_student = metrices[6]\n",
    "test_cost_student = tensors_to_lst(metrices[7])\n",
    "f1_class1_teacher_test=metrices[8]\n",
    "f1_class0_teacher_test = metrices[9]\n",
    "accuracy_teacher = metrices[10]\n",
    "mcc_cost_teacher = metrices[11]\n",
    "test_cost_teacher=tensors_to_lst(metrices[12])\n",
    "\n",
    "print('Consistency_lost_train :',consistency_cost_lst,'\\n',\n",
    "      'Classificataion_lost_train : ',classification_cost_lst,'\\n',\n",
    "      'Overall_loss_train :',overall_cost_lst,'\\n',\n",
    "      'f1_class0_student_test :',f1_class0_student_test,'\\n',\n",
    "      'f1_class1_student_test :',f1_class1_student_test,'\\n',\n",
    "      'accuracy_student :',accuracy_student,'\\n',\n",
    "      'mcc_cost_student :',mcc_cost_student,'\\n',\n",
    "      'test_cost_student :',test_cost_student,'\\n',\n",
    "      'f1_class0_teacher_test :',f1_class0_teacher_test,'\\n',\n",
    "      'f1_class1_teacher_test :',f1_class1_teacher_test,'\\n',\n",
    "    'accuracy_teacher :', accuracy_teacher,'\\n',\n",
    "      'mcc_cost_teacher',mcc_cost_teacher,'\\n',\n",
    "      'test_cost_teacher',test_cost_teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model= EntityModel()\n",
    "# optimizer_parameters_model = list(model.named_parameters())\n",
    "  \n",
    "# no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "# optimizer_parameters = [\n",
    "# {\n",
    "#     \"params\": [\n",
    "#         p for n, p in optimizer_parameters_model if not any(nd in n for nd in no_decay)\n",
    "#     ],\n",
    "#     \"weight_decay\": 0.001,\n",
    "# },\n",
    "# {\n",
    "#     \"params\": [\n",
    "#         p for n, p in optimizer_parameters_model if any(nd in n for nd in no_decay)\n",
    "#     ],\n",
    "#     \"weight_decay\": 0.0,\n",
    "# }]\n",
    "# optimizer = AdamW(optimizer_parameters, lr=config.lr)\n",
    "# num_train_steps = int(len(train_dataloader) * config.EPOCHS)\n",
    "# #     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "# #         optimizer, T_max=num_train_steps)\n",
    "# scheduler = get_linear_schedule_with_warmup(\n",
    "# optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n",
    "# float(scheduler.get_last_lr()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items(lst):\n",
    "    new_lst = []\n",
    "    for items in lst:\n",
    "        for item in items :\n",
    "            new_lst.append(float(item))\n",
    "    return new_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensors_to_lst(lst):\n",
    "    new_lst = []\n",
    "    for items in lst:\n",
    "        new_lst.append(items.item())\n",
    "    return new_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_against_epochs(matrix1,matrix2,label1 , label2 ,y_label , title):\n",
    "    \n",
    "    epochs = np.arange(1,config.EPOCHS+1)\n",
    "    # print(epochs)\n",
    "    plt.plot(epochs,matrix1,label=label1)\n",
    "    if matrix2 is not None :\n",
    "        plt.plot(epochs,matrix2,label=label2)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Running without Noise embeddings and dropout </h2> -- <h4> No weighted Loss </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrices = train_MeanTeacher(train_dataloader,val_dataloader,noise_layer=False) # alpha = 0.995 and full data w/o noise and dropout\n",
    "\n",
    "consistency_cost_lst = get_items(metrices[0])\n",
    "classification_cost_lst = get_items(metrices[1])\n",
    "overall_cost_lst = get_items(metrices[2])\n",
    "f1_class0_student_test = metrices[3]\n",
    "f1_class1_student_test= metrices[4]\n",
    "accuracy_student = metrices[5]\n",
    "mcc_cost_student = metrices[6]\n",
    "test_cost_student = tensors_to_lst(metrices[7])\n",
    "f1_class1_teacher_test=metrices[8]\n",
    "f1_class0_teacher_test = metrices[9]\n",
    "accuracy_teacher = metrices[10]\n",
    "mcc_cost_teacher = metrices[11]\n",
    "test_cost_teacher=tensors_to_lst(metrices[12])\n",
    "\n",
    "print('Consistency_lost_train :',consistency_cost_lst,'\\n',\n",
    "      'Classificataion_lost_train : ',classification_cost_lst,'\\n',\n",
    "      'Overall_loss_train :',overall_cost_lst,'\\n',\n",
    "      'f1_class0_student_test :',f1_class0_student_test,'\\n',\n",
    "      'f1_class1_student_test :',f1_class1_student_test,'\\n',\n",
    "      'accuracy_student :',accuracy_student,'\\n',\n",
    "      'mcc_cost_student :',mcc_cost_student,'\\n',\n",
    "      'test_cost_student :',test_cost_student,'\\n',\n",
    "      'f1_class0_teacher_test :',f1_class0_teacher_test,'\\n',\n",
    "      'f1_class1_teacher_test :',f1_class1_teacher_test,'\\n',\n",
    "    'accuracy_teacher :', accuracy_teacher,'\\n',\n",
    "      'mcc_cost_teacher',mcc_cost_teacher,'\\n',\n",
    "      'test_cost_teacher',test_cost_teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consistency_cost_lst = get_items(metrices[0])\n",
    "classification_cost_lst = get_items(metrices[1])\n",
    "overall_cost_lst = get_items(metrices[2])\n",
    "f1_class0_student_test = metrices[3]\n",
    "f1_class1_student_test= metrices[4]\n",
    "accuracy_student = metrices[5]\n",
    "mcc_cost_student = metrices[6]\n",
    "test_cost_student = tensors_to_lst(metrices[7])\n",
    "f1_class1_teacher_test=metrices[8]\n",
    "f1_class0_teacher_test = metrices[9]\n",
    "accuracy_teacher = metrices[10]\n",
    "mcc_cost_teacher = metrices[11]\n",
    "test_cost_teacher=tensors_to_lst(metrices[12])\n",
    "\n",
    "print('Consistency_lost_train :',consistency_cost_lst,'\\n',\n",
    "      'Classificataion_lost_train : ',classification_cost_lst,'\\n',\n",
    "      'Overall_loss_train :',overall_cost_lst,'\\n',\n",
    "      'f1_class0_student_test :',f1_class0_student_test,'\\n',\n",
    "      'f1_class1_student_test :',f1_class1_student_test,'\\n',\n",
    "      'accuracy_student :',accuracy_student,'\\n',\n",
    "      'mcc_cost_student :',mcc_cost_student,'\\n',\n",
    "      'test_cost_student :',test_cost_student,'\\n',\n",
    "      'f1_class0_teacher_test :',f1_class0_teacher_test,'\\n',\n",
    "      'f1_class1_teacher_test :',f1_class1_teacher_test,'\\n',\n",
    "    'accuracy_teacher :', accuracy_teacher,'\\n',\n",
    "      'mcc_cost_teacher',mcc_cost_teacher,'\\n',\n",
    "      'test_cost_teacher',test_cost_teacher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plotting</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_against_epochs(test_cost_teacher,test_cost_student,label1 = 'ValidationLoss_teacher', \n",
    "                    label2 = 'ValidationLoss_student', y_label = 'loss', title ='Meanteacher_XLMRoberta' )\n",
    "plot_against_epochs(mcc_cost_teacher,mcc_cost_student,label1 = 'mcc_teacher', \n",
    "                    label2 = 'mcc_student', y_label = 'mcc_score_test', title ='Meanteacher_XLMRoberta' )\n",
    "plot_against_epochs(classification_cost_lst,None,label1 = 'classification_cost_train', \n",
    "                    label2 = None, y_label = 'loss', title ='Meanteacher_XLMRoberta' )\n",
    "plot_against_epochs(consistency_cost_lst,None,label1 = 'Consistency_lost_train', \n",
    "                    label2 = None, y_label = 'loss', title ='Meanteacher_XLMRoberta' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,param in model.named_parameters():\n",
    "    if 'classifier' not in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer.add_text('Text','The Labelled data used in this experiment was 3200 and unlabelled used was 6400')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"runs/MeanTeacherTraining/alpha_0.995_trail1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrices = train_MeanTeacher(train_dataloader,val_dataloader,len_labelled_data,len_unlabelled_data,writer, noise_layer=True) # alpha = 0.995 and full data w/o noise and dropout\n",
    "\n",
    "consistency_cost_lst = get_items(metrices[0])\n",
    "classification_cost_lst = get_items(metrices[1])\n",
    "overall_cost_lst = get_items(metrices[2])\n",
    "f1_class0_student_test = metrices[3]\n",
    "f1_class1_student_test= metrices[4]\n",
    "accuracy_student = metrices[5]\n",
    "mcc_cost_student = metrices[6]\n",
    "test_cost_student = tensors_to_lst(metrices[7])\n",
    "f1_class1_teacher_test=metrices[8]\n",
    "f1_class0_teacher_test = metrices[9]\n",
    "accuracy_teacher = metrices[10]\n",
    "mcc_cost_teacher = metrices[11]\n",
    "test_cost_teacher=tensors_to_lst(metrices[12])\n",
    "\n",
    "print('Consistency_lost_train :',consistency_cost_lst,'\\n',\n",
    "      'Classificataion_lost_train : ',classification_cost_lst,'\\n',\n",
    "      'Overall_loss_train :',overall_cost_lst,'\\n',\n",
    "      'f1_class0_student_test :',f1_class0_student_test,'\\n',\n",
    "      'f1_class1_student_test :',f1_class1_student_test,'\\n',\n",
    "      'accuracy_student :',accuracy_student,'\\n',\n",
    "      'mcc_cost_student :',mcc_cost_student,'\\n',\n",
    "      'test_cost_student :',test_cost_student,'\\n',\n",
    "      'f1_class0_teacher_test :',f1_class0_teacher_test,'\\n',\n",
    "      'f1_class1_teacher_test :',f1_class1_teacher_test,'\\n',\n",
    "    'accuracy_teacher :', accuracy_teacher,'\\n',\n",
    "      'mcc_cost_teacher',mcc_cost_teacher,'\\n',\n",
    "      'test_cost_teacher',test_cost_teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_against_epochs(test_cost_teacher,test_cost_student,label1 = 'ValidationLoss_teacher', \n",
    "                    label2 = 'ValidationLoss_student', y_label = 'loss', title ='Meanteacher_XLMRoberta' )\n",
    "plot_against_epochs(mcc_cost_teacher,mcc_cost_student,label1 = 'mcc_teacher', \n",
    "                    label2 = 'mcc_student', y_label = 'mcc_score_test', title ='Meanteacher_XLMRoberta' )\n",
    "plot_against_epochs(classification_cost_lst,None,label1 = 'classification_cost_train', \n",
    "                    label2 = None, y_label = 'loss', title ='Meanteacher_XLMRoberta' )\n",
    "plot_against_epochs(consistency_cost_lst,None,label1 = 'Consistency_lost_train', \n",
    "                    label2 = None, y_label = 'loss', title ='Meanteacher_XLMRoberta' )"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/pytorch-1.6-gpu-py36-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
