{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.6/site-packages (4.12.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers) (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.51.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.1.2)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (0.1.96)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (3.6.5)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.6/site-packages (from nltk) (2021.11.10)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from nltk) (4.51.0)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "# from DataAugmentation \n",
    "from DataAugmentation import DataAugmentation\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from importlib import reload\n",
    "import config\n",
    "from transformers import XLMRobertaForTokenClassification, XLMRobertaConfig ,BertModel, XLMRobertaTokenizer, XLMRobertaModel, BertForTokenClassification\n",
    "from data_utils import loadDatafromFile,createTokenizedDf,CompDataset,createkfoldData,createDataloaders\n",
    "from seqeval.metrics import accuracy_score, classification_report\n",
    "import engine\n",
    "from model_new import EntityModel\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "def process_data(filePath_src,filePath_tar, filePath_srcTags,filePath_tarTags):\n",
    "    \n",
    "    dataObj = loadDatafromFile(filePath_src,filePath_tar, filePath_srcTags,filePath_tarTags)\n",
    "    df= dataObj.createDf() # get dataframe from files\n",
    "    obj_tokenized = createTokenizedDf(df)\n",
    "    df_new= obj_tokenized.convertDf()\n",
    "    enc_label = preprocessing.LabelEncoder()\n",
    "    df_new['labels']= enc_label.fit_transform(df_new['labels'])\n",
    "    train_data = CompDataset(df_new)\n",
    "    return train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if loading of the non augmented (noise-free) data is needed, then run this\n",
    "# dataset_train = process_data(config.filePath_src,config.filePath_tar, config.filePath_srcTags,config.filePath_tarTags)\n",
    "# len(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading dev/eval data\n",
    "dataset_eval = process_data(config.filePath_src_eval,config.filePath_tar_eval, config.filePath_srcTags_eval,config.filePath_tarTags_eval)\n",
    "len(dataset_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'DataAugmentation' has no attribute 'DataAugmentation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-3e8f837d3939>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDataAugmentation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataAugmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataAugmentation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'DataAugmentation' has no attribute 'DataAugmentation'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>src_tokens</th>\n",
       "      <th>tar_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>José Ortega y Gasset visited Husserl at Freibu...</td>\n",
       "      <td>1934 besuchte José Ortega y Gasset Husserl in ...</td>\n",
       "      <td>OK OK OK OK BAD OK OK OK BAD BAD OK</td>\n",
       "      <td>OK BAD OK BAD OK OK OK OK OK OK OK OK BAD OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>however , a disappointing ninth in China meant...</td>\n",
       "      <td>eine enttäuschende Neunte in China bedeutete j...</td>\n",
       "      <td>OK BAD BAD BAD BAD OK OK OK OK OK OK OK OK OK ...</td>\n",
       "      <td>BAD BAD OK BAD OK BAD OK OK OK OK OK OK OK OK ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in his diary , Chase wrote that the release of...</td>\n",
       "      <td>in seinem Tagebuch , Chase schrieb , dass die ...</td>\n",
       "      <td>OK OK OK OK OK BAD OK OK BAD OK OK BAD OK BAD ...</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heavy arquebuses mounted on wagons were called...</td>\n",
       "      <td>schwere Arquebuses auf Waggons montiert wurden...</td>\n",
       "      <td>BAD BAD OK OK BAD BAD BAD OK OK OK OK</td>\n",
       "      <td>OK OK BAD BAD OK OK OK BAD OK OK BAD OK BAD OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>once North Pacific salmon die off after spawni...</td>\n",
       "      <td>sobald der nordpazifische Lachs nach dem Laich...</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK BAD BAD BAD BAD OK...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK BAD ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>some may also discourage or disallow unsanitar...</td>\n",
       "      <td>einige können auch unhygienische Praktiken wie...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK BAD ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>in the late 1860s , the crinolines disappeared...</td>\n",
       "      <td>in den späten 1860er Jahren verschwanden die K...</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK BAD BAD BAD OK OK ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>disco was criticized as mindless , consumerist...</td>\n",
       "      <td>Disco wurde als geistlos , konsumistisch , übe...</td>\n",
       "      <td>OK OK OK OK BAD BAD OK OK OK BAD OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>planters would then fill large hogsheads with ...</td>\n",
       "      <td>die Pflanzer würden dann große Heuschrecken mi...</td>\n",
       "      <td>OK OK BAD OK OK BAD OK OK OK OK BAD BAD BAD BA...</td>\n",
       "      <td>OK OK OK OK OK BAD OK OK OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>he slew Krishna 's most dangerous enemy , Jara...</td>\n",
       "      <td>er tötete Krishnas gefährlichsten Feind Jarasa...</td>\n",
       "      <td>OK BAD OK OK OK OK OK OK OK OK OK BAD BAD OK O...</td>\n",
       "      <td>OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 source  \\\n",
       "0     José Ortega y Gasset visited Husserl at Freibu...   \n",
       "1     however , a disappointing ninth in China meant...   \n",
       "2     in his diary , Chase wrote that the release of...   \n",
       "3     Heavy arquebuses mounted on wagons were called...   \n",
       "4     once North Pacific salmon die off after spawni...   \n",
       "...                                                 ...   \n",
       "6995  some may also discourage or disallow unsanitar...   \n",
       "6996  in the late 1860s , the crinolines disappeared...   \n",
       "6997  disco was criticized as mindless , consumerist...   \n",
       "6998  planters would then fill large hogsheads with ...   \n",
       "6999  he slew Krishna 's most dangerous enemy , Jara...   \n",
       "\n",
       "                                                 target  \\\n",
       "0     1934 besuchte José Ortega y Gasset Husserl in ...   \n",
       "1     eine enttäuschende Neunte in China bedeutete j...   \n",
       "2     in seinem Tagebuch , Chase schrieb , dass die ...   \n",
       "3     schwere Arquebuses auf Waggons montiert wurden...   \n",
       "4     sobald der nordpazifische Lachs nach dem Laich...   \n",
       "...                                                 ...   \n",
       "6995  einige können auch unhygienische Praktiken wie...   \n",
       "6996  in den späten 1860er Jahren verschwanden die K...   \n",
       "6997  Disco wurde als geistlos , konsumistisch , übe...   \n",
       "6998  die Pflanzer würden dann große Heuschrecken mi...   \n",
       "6999  er tötete Krishnas gefährlichsten Feind Jarasa...   \n",
       "\n",
       "                                             src_tokens  \\\n",
       "0                   OK OK OK OK BAD OK OK OK BAD BAD OK   \n",
       "1     OK BAD BAD BAD BAD OK OK OK OK OK OK OK OK OK ...   \n",
       "2     OK OK OK OK OK BAD OK OK BAD OK OK BAD OK BAD ...   \n",
       "3                 BAD BAD OK OK BAD BAD BAD OK OK OK OK   \n",
       "4     OK OK OK OK OK OK OK BAD OK BAD BAD BAD BAD OK...   \n",
       "...                                                 ...   \n",
       "6995  OK OK OK OK OK OK OK OK OK OK OK OK OK OK BAD ...   \n",
       "6996  OK OK OK OK OK OK OK BAD OK BAD BAD BAD OK OK ...   \n",
       "6997             OK OK OK OK BAD BAD OK OK OK BAD OK OK   \n",
       "6998  OK OK BAD OK OK BAD OK OK OK OK BAD BAD BAD BA...   \n",
       "6999  OK BAD OK OK OK OK OK OK OK OK OK BAD BAD OK O...   \n",
       "\n",
       "                                             tar_tokens  \n",
       "0     OK BAD OK BAD OK OK OK OK OK OK OK OK BAD OK O...  \n",
       "1     BAD BAD OK BAD OK BAD OK OK OK OK OK OK OK OK ...  \n",
       "2     OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...  \n",
       "3     OK OK BAD BAD OK OK OK BAD OK OK BAD OK BAD OK...  \n",
       "4     OK OK OK OK OK OK OK OK OK OK OK OK OK OK BAD ...  \n",
       "...                                                 ...  \n",
       "6995  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "6996  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK B...  \n",
       "6997  OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...  \n",
       "6998  OK OK OK OK OK BAD OK OK OK OK OK BAD OK OK OK...  \n",
       "6999  OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...  \n",
       "\n",
       "[7000 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataObj = loadDatafromFile(config.filePath_src,config.filePath_tar, config.filePath_srcTags,config.filePath_tarTags)\n",
    "df= dataObj.createDf() \n",
    "df\n",
    "# list(df.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (3.6.5)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.6/site-packages (from nltk) (2021.11.10)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from nltk) (4.51.0)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "reload(DataAugmentation)\n",
    "from DataAugmentation import DataAugmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>src_tokens</th>\n",
       "      <th>tar_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>José Ortega y Gasset visited Husserl at Freibu...</td>\n",
       "      <td>1934 besuchte José Ortega y Gasset Husserl in ...</td>\n",
       "      <td>OK OK OK OK BAD OK OK OK BAD BAD OK</td>\n",
       "      <td>OK BAD OK BAD OK OK OK OK OK OK OK OK BAD OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>however , a disappointing ninth in China meant...</td>\n",
       "      <td>eine enttäuschende Neunte in China bedeutete j...</td>\n",
       "      <td>OK BAD BAD BAD BAD OK OK OK OK OK OK OK OK OK ...</td>\n",
       "      <td>BAD BAD OK BAD OK BAD OK OK OK OK OK OK OK OK ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in his diary , Chase wrote that the release of...</td>\n",
       "      <td>in seinem Tagebuch , Chase schrieb , dass die ...</td>\n",
       "      <td>OK OK OK OK OK BAD OK OK BAD OK OK BAD OK BAD ...</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heavy arquebuses mounted on wagons were called...</td>\n",
       "      <td>schwere Arquebuses auf Waggons montiert wurden...</td>\n",
       "      <td>BAD BAD OK OK BAD BAD BAD OK OK OK OK</td>\n",
       "      <td>OK OK BAD BAD OK OK OK BAD OK OK BAD OK BAD OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>once North Pacific salmon die off after spawni...</td>\n",
       "      <td>sobald der nordpazifische Lachs nach dem Laich...</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK BAD BAD BAD BAD OK...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK BAD ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76995</th>\n",
       "      <td>he mass Krishna 's most grave enemy , Jarasand...</td>\n",
       "      <td>er tötete Krishnas gefährlichsten Feind Jarasa...</td>\n",
       "      <td>OK BAD OK OK OK OK OK OK OK OK OK BAD BAD OK O...</td>\n",
       "      <td>OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76996</th>\n",
       "      <td>he great deal Krishna 's most dangerous enemy ...</td>\n",
       "      <td>er tötete Krishnas gefährlichsten Feind Jarasa...</td>\n",
       "      <td>OK BAD BAD OK OK OK OK OK OK OK OK OK BAD BAD ...</td>\n",
       "      <td>OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76997</th>\n",
       "      <td>he slew Krishna 's most dangerous enemy , Jara...</td>\n",
       "      <td>er tötete Krishnas gefährlichsten Feind Jarasa...</td>\n",
       "      <td>OK BAD OK OK OK OK OK OK OK OK OK BAD BAD OK O...</td>\n",
       "      <td>OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76998</th>\n",
       "      <td>he slue Krishna 's most dangerous enemy , Jara...</td>\n",
       "      <td>er tötete Krishnas gefährlichsten Feind Jarasa...</td>\n",
       "      <td>OK BAD OK OK OK OK OK OK OK OK OK BAD BAD OK O...</td>\n",
       "      <td>OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76999</th>\n",
       "      <td>he slew krishna 's most dangerous enemy , Jara...</td>\n",
       "      <td>er tötete Krishnas gefährlichsten Feind Jarasa...</td>\n",
       "      <td>OK BAD OK OK OK OK OK OK OK OK OK BAD BAD OK O...</td>\n",
       "      <td>OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>217000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  source  \\\n",
       "0      José Ortega y Gasset visited Husserl at Freibu...   \n",
       "1      however , a disappointing ninth in China meant...   \n",
       "2      in his diary , Chase wrote that the release of...   \n",
       "3      Heavy arquebuses mounted on wagons were called...   \n",
       "4      once North Pacific salmon die off after spawni...   \n",
       "...                                                  ...   \n",
       "76995  he mass Krishna 's most grave enemy , Jarasand...   \n",
       "76996  he great deal Krishna 's most dangerous enemy ...   \n",
       "76997  he slew Krishna 's most dangerous enemy , Jara...   \n",
       "76998  he slue Krishna 's most dangerous enemy , Jara...   \n",
       "76999  he slew krishna 's most dangerous enemy , Jara...   \n",
       "\n",
       "                                                  target  \\\n",
       "0      1934 besuchte José Ortega y Gasset Husserl in ...   \n",
       "1      eine enttäuschende Neunte in China bedeutete j...   \n",
       "2      in seinem Tagebuch , Chase schrieb , dass die ...   \n",
       "3      schwere Arquebuses auf Waggons montiert wurden...   \n",
       "4      sobald der nordpazifische Lachs nach dem Laich...   \n",
       "...                                                  ...   \n",
       "76995  er tötete Krishnas gefährlichsten Feind Jarasa...   \n",
       "76996  er tötete Krishnas gefährlichsten Feind Jarasa...   \n",
       "76997  er tötete Krishnas gefährlichsten Feind Jarasa...   \n",
       "76998  er tötete Krishnas gefährlichsten Feind Jarasa...   \n",
       "76999  er tötete Krishnas gefährlichsten Feind Jarasa...   \n",
       "\n",
       "                                              src_tokens  \\\n",
       "0                    OK OK OK OK BAD OK OK OK BAD BAD OK   \n",
       "1      OK BAD BAD BAD BAD OK OK OK OK OK OK OK OK OK ...   \n",
       "2      OK OK OK OK OK BAD OK OK BAD OK OK BAD OK BAD ...   \n",
       "3                  BAD BAD OK OK BAD BAD BAD OK OK OK OK   \n",
       "4      OK OK OK OK OK OK OK BAD OK BAD BAD BAD BAD OK...   \n",
       "...                                                  ...   \n",
       "76995  OK BAD OK OK OK OK OK OK OK OK OK BAD BAD OK O...   \n",
       "76996  OK BAD BAD OK OK OK OK OK OK OK OK OK BAD BAD ...   \n",
       "76997  OK BAD OK OK OK OK OK OK OK OK OK BAD BAD OK O...   \n",
       "76998  OK BAD OK OK OK OK OK OK OK OK OK BAD BAD OK O...   \n",
       "76999  OK BAD OK OK OK OK OK OK OK OK OK BAD BAD OK O...   \n",
       "\n",
       "                                              tar_tokens  \n",
       "0      OK BAD OK BAD OK OK OK OK OK OK OK OK BAD OK O...  \n",
       "1      BAD BAD OK BAD OK BAD OK OK OK OK OK OK OK OK ...  \n",
       "2      OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...  \n",
       "3      OK OK BAD BAD OK OK OK BAD OK OK BAD OK BAD OK...  \n",
       "4      OK OK OK OK OK OK OK OK OK OK OK OK OK OK BAD ...  \n",
       "...                                                  ...  \n",
       "76995  OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...  \n",
       "76996  OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...  \n",
       "76997  OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...  \n",
       "76998  OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...  \n",
       "76999  OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...  \n",
       "\n",
       "[217000 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataaug_obj = DataAugmentation(df,swap_words=2,syn_words=2,del_words_prob=0.2,num_sentences=10)  \n",
    "swapDataset = dataaug_obj.random_swap()\n",
    "del_augDataset = dataaug_obj.random_deletion()\n",
    "del_augDataset = del_augDataset[7000:]\n",
    "syn_dataset = dataaug_obj.synonym_replacement()\n",
    "syn_dataset = syn_dataset[7000:]\n",
    "frames = [swapDataset , del_augDataset,syn_dataset]\n",
    "aug_df  = pd.concat(frames)\n",
    "aug_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>src_tokens</th>\n",
       "      <th>tar_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Germany invaded France , , Luxembourg and the ...</td>\n",
       "      <td>Deutschland marschierte am 10. Mai 1940 in Fra...</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD BAD OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the enzyme chitinase grow in the stomach helps...</td>\n",
       "      <td>das im Magen produzierte Enzym Chitinase hilft...</td>\n",
       "      <td>OK OK OK OK OK BAD OK BAD BAD BAD BAD BAD BAD ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK BAD ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Roger Rousseau , The French Foreign Legion in ...</td>\n",
       "      <td>Roger Rousseau , Die französische Fremdenlegio...</td>\n",
       "      <td>OK OK OK BAD OK BAD BAD OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK OK BAD BAD OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in 1927 , Lieutenant Jaime Sabater , from San ...</td>\n",
       "      <td>1927 absolvierte Lieutenant Jaime Sabater aus ...</td>\n",
       "      <td>BAD OK OK BAD OK OK OK OK OK OK OK OK OK OK OK...</td>\n",
       "      <td>BAD OK OK OK OK BAD OK OK OK OK OK OK OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackmails Sharon into meeting him at a motel</td>\n",
       "      <td>Cameron erpresst Sharon , ihn in einem Motel z...</td>\n",
       "      <td>BAD BAD BAD OK OK OK OK OK</td>\n",
       "      <td>OK OK OK BAD OK OK BAD OK OK OK OK OK OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216995</th>\n",
       "      <td>maxilla baleen exclusively the in the sit jaw ...</td>\n",
       "      <td>die Zähne oder Baleen im Oberkiefer sitzen aus...</td>\n",
       "      <td>OK OK OK OK BAD OK OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK OK OK OK OK OK OK ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216996</th>\n",
       "      <td>the with clock with mainspring either 's motor...</td>\n",
       "      <td>die Hauptfeder der elektrischen Uhr wird entwe...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK BAD OK OK OK ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216997</th>\n",
       "      <td>he accidentally frivol away Jane during an alt...</td>\n",
       "      <td>er schießt Jane versehentlich während einer Au...</td>\n",
       "      <td>OK OK BAD BAD OK OK OK OK OK OK OK OK OK OK OK...</td>\n",
       "      <td>OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216998</th>\n",
       "      <td>Paul Collier disability advocate , Australian ...</td>\n",
       "      <td>Paul Collier , 46 , australischer Behindertena...</td>\n",
       "      <td>OK OK BAD BAD OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK BAD BAD OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216999</th>\n",
       "      <td>; republish with many errors by Jacques st pau...</td>\n",
       "      <td>; wiederveröffentlicht mit vielen Fehlern von ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>217000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   source  \\\n",
       "0       Germany invaded France , , Luxembourg and the ...   \n",
       "1       the enzyme chitinase grow in the stomach helps...   \n",
       "2       Roger Rousseau , The French Foreign Legion in ...   \n",
       "3       in 1927 , Lieutenant Jaime Sabater , from San ...   \n",
       "4           blackmails Sharon into meeting him at a motel   \n",
       "...                                                   ...   \n",
       "216995  maxilla baleen exclusively the in the sit jaw ...   \n",
       "216996  the with clock with mainspring either 's motor...   \n",
       "216997  he accidentally frivol away Jane during an alt...   \n",
       "216998  Paul Collier disability advocate , Australian ...   \n",
       "216999  ; republish with many errors by Jacques st pau...   \n",
       "\n",
       "                                                   target  \\\n",
       "0       Deutschland marschierte am 10. Mai 1940 in Fra...   \n",
       "1       das im Magen produzierte Enzym Chitinase hilft...   \n",
       "2       Roger Rousseau , Die französische Fremdenlegio...   \n",
       "3       1927 absolvierte Lieutenant Jaime Sabater aus ...   \n",
       "4       Cameron erpresst Sharon , ihn in einem Motel z...   \n",
       "...                                                   ...   \n",
       "216995  die Zähne oder Baleen im Oberkiefer sitzen aus...   \n",
       "216996  die Hauptfeder der elektrischen Uhr wird entwe...   \n",
       "216997  er schießt Jane versehentlich während einer Au...   \n",
       "216998  Paul Collier , 46 , australischer Behindertena...   \n",
       "216999  ; wiederveröffentlicht mit vielen Fehlern von ...   \n",
       "\n",
       "                                               src_tokens  \\\n",
       "0                   OK OK OK OK OK OK OK BAD BAD OK OK OK   \n",
       "1       OK OK OK OK OK BAD OK BAD BAD BAD BAD BAD BAD ...   \n",
       "2                  OK OK OK BAD OK BAD BAD OK OK OK OK OK   \n",
       "3       BAD OK OK BAD OK OK OK OK OK OK OK OK OK OK OK...   \n",
       "4                              BAD BAD BAD OK OK OK OK OK   \n",
       "...                                                   ...   \n",
       "216995         OK OK OK OK BAD OK OK OK OK OK OK OK OK OK   \n",
       "216996  OK OK OK OK OK OK OK OK OK OK OK BAD OK OK OK ...   \n",
       "216997  OK OK BAD BAD OK OK OK OK OK OK OK OK OK OK OK...   \n",
       "216998              OK OK BAD BAD OK OK OK OK OK OK OK OK   \n",
       "216999             OK OK OK OK OK OK OK OK OK OK OK OK OK   \n",
       "\n",
       "                                               tar_tokens  \n",
       "0       OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "1       OK OK OK OK OK OK OK OK OK OK OK OK OK OK BAD ...  \n",
       "2       OK OK OK OK OK OK OK BAD OK OK BAD BAD OK OK O...  \n",
       "3       BAD OK OK OK OK BAD OK OK OK OK OK OK OK OK OK...  \n",
       "4       OK OK OK BAD OK OK BAD OK OK OK OK OK OK OK OK...  \n",
       "...                                                   ...  \n",
       "216995  OK OK OK OK OK OK OK BAD OK OK OK OK OK OK OK ...  \n",
       "216996  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "216997  OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...  \n",
       "216998  OK OK OK OK OK OK OK OK OK OK OK OK BAD BAD OK...  \n",
       "216999  OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...  \n",
       "\n",
       "[217000 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_df =aug_df.sample(frac=1).reset_index(drop=True)\n",
    "aug_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DataAugmentation methods\n",
    "# def random_swap(dataframe,n): # n is number of times to swap randomly 2 words\n",
    "    \n",
    "#     source_sentences  = list(dataframe.source)\n",
    "#     target_sentences = list(dataframe.target)\n",
    "#     labels_src = list(dataframe.src_tokens)\n",
    "#     labels_tar = list(dataframe.tar_tokens)\n",
    "#     source_sentences_temp =[]\n",
    "#     labels_sec_temp=[]\n",
    "#     i=0\n",
    "    \n",
    "#     for sentences, labels in zip(source_sentences,labels_src):\n",
    "        \n",
    "#         sentences = sentences.split()\n",
    "#         labels = labels.split()\n",
    "        \n",
    "#         for _ in range(5):\n",
    "#             for _ in range(n):\n",
    "#                 sentences, labels = swap_word(sentences,labels)\n",
    "#             assert(len(sentences) == len(labels))\n",
    "#             sentences_str = ' '.join(sentences)\n",
    "#             labels_str = ' '.join(labels)\n",
    "#             target_sentences.append(target_sentences[i])\n",
    "#             labels_tar.append(labels_tar[i])\n",
    "#             source_sentences_temp.append(sentences_str)\n",
    "#             labels_sec_temp.append(labels_str)\n",
    "        \n",
    "# #         break\n",
    "#         i+=1\n",
    "    \n",
    "#     source_sentences.extend(source_sentences_temp)\n",
    "#     labels_src.extend(labels_sec_temp)\n",
    "# #     print(source_sentences)\n",
    "# #     print(labels_src)\n",
    "#     column_names = [\"source\",\"target\",\"src_tokens\",\"tar_tokens\"]\n",
    "#     df = pd.DataFrame(columns=column_names,dtype=object)\n",
    "#     df = df.assign(source=source_sentences)\n",
    "#     df = df.assign(target = target_sentences)\n",
    "#     df = df.assign(src_tokens = labels_src)\n",
    "#     df = df.assign(tar_tokens = labels_tar)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# def swap_word(new_words,labels_src):\n",
    "\n",
    "#     random_idx_1 = random.randint(0, len(new_words)-1)\n",
    "#     random_idx_2 = random_idx_1\n",
    "#     counter = 0\n",
    "#     while random_idx_2 == random_idx_1:\n",
    "#         random_idx_2 = random.randint(0, len(new_words)-1)\n",
    "#         counter += 1\n",
    "#         if counter > 3:\n",
    "#             return (new_words,labels_src)\n",
    "#     new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n",
    "#     labels_src[random_idx_1], labels_src[random_idx_2] = labels_src[random_idx_2], labels_src[random_idx_1]\n",
    "# #     print(labels_src)\n",
    "# #     new_str = ''.join(new_words)\n",
    "#     return (new_words, labels_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df = random_swap(df,2) # tune how much words you want to swap\n",
    "# # new_df = new_df.sample(frac=1).reset_index(drop=True) # shuffling the dafaframe with resetting the index\n",
    "# print(new_df.iloc[0].values)\n",
    "# print(new_df.iloc[7000].values)\n",
    "# print(new_df.iloc[7001].values)\n",
    "# print(new_df.iloc[7002].values)\n",
    "# print(new_df.iloc[7003].values)\n",
    "# print(new_df.iloc[7004].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def random_deletion(dataframe, p):\n",
    "    \n",
    "#     source_sentences  = list(dataframe.source)\n",
    "#     target_sentences = list(dataframe.target)\n",
    "#     labels_src = list(dataframe.src_tokens)\n",
    "#     labels_tar = list(dataframe.tar_tokens)\n",
    "#     senetences_temp=[]\n",
    "#     labels_temp= []\n",
    "#     #randomly delete words with probability p\n",
    "#     i=0\n",
    "#     for sentences, labels in zip(source_sentences,labels_src):\n",
    "            \n",
    "#         sentences = sentences.split()\n",
    "#         labels = labels.split() \n",
    "        \n",
    "        \n",
    "        \n",
    "#         if len(sentences) == 1:\n",
    "#             i+=1\n",
    "#             continue\n",
    "            \n",
    "        \n",
    "               \n",
    "#         for _ in range(5):\n",
    "            \n",
    "#             source_sentences_temp=[]\n",
    "#             labels_sec_temp=[]\n",
    "#             for word,label in zip(sentences,labels):\n",
    "#                 r = random.uniform(0, 1)\n",
    "#                 if r > p:\n",
    "#                     source_sentences_temp.append(word)\n",
    "#                     labels_sec_temp.append(label)\n",
    "#             if len(source_sentences_temp) == 0: #if you end up deleting all words, just return a random word\n",
    "#                 rand_int = random.randint(0, len(source_sentences_temp)-1)\n",
    "#                 source_sentences_temp.append(sentences[rand_int])\n",
    "#                 labels_sec_temp.append(labels[rand_int])\n",
    "#             assert(len(source_sentences_temp) == len(labels_sec_temp))\n",
    "#             sentences_str = ' '.join(source_sentences_temp)\n",
    "#             labels_str = ' '.join(labels_sec_temp)\n",
    "#             senetences_temp.append(sentences_str)\n",
    "#             labels_temp.append(labels_str)\n",
    "#             target_sentences.append(target_sentences[i])\n",
    "#             labels_tar.append(labels_tar[i])\n",
    "# #         break\n",
    "#         i+=1\n",
    "#     source_sentences.extend(senetences_temp)\n",
    "#     labels_src.extend(labels_temp)    \n",
    "#     column_names = [\"source\",\"target\",\"src_tokens\",\"tar_tokens\"]\n",
    "#     df = pd.DataFrame(columns=column_names,dtype=object)\n",
    "#     df = df.assign(source=source_sentences)\n",
    "#     df = df.assign(target = target_sentences)\n",
    "#     df = df.assign(src_tokens = labels_src)\n",
    "#     df = df.assign(tar_tokens = labels_tar)  \n",
    "\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, test_data= model_selection.train_test_split(dataset, random_state = 34, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# augmented_dataset = random_deletion(df,0.2) # probabality of deleting the tokens\n",
    "# augmented_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(augmented_dataset.iloc[0].values)\n",
    "# print(augmented_dataset.iloc[0].values)\n",
    "\n",
    "# print(augmented_dataset.iloc[7003].values)\n",
    "# print(augmented_dataset.iloc[7004].values)\n",
    "# print(augmented_dataset.iloc[7002].values)\n",
    "# print(augmented_dataset.iloc[7001].values)\n",
    "# print(augmented_dataset.iloc[7000].values)\n",
    "# assert(augmented_dataset.iloc[7002].values[0] == augmented_dataset.iloc[7003].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_tokenized = createTokenizedDf(aug_df)\n",
    "df_new= obj_tokenized.convertDf()\n",
    "enc_label = preprocessing.LabelEncoder()\n",
    "df_new['labels']= enc_label.fit_transform(df_new['labels'])\n",
    "train_data = CompDataset(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([     0, 102126, 205491,     71,   9942,      6,      4,      6,      4,\n",
       "         211546,    136,     70, 231118,     98,   4347,  27712,      2,      2,\n",
       "            656,  12154,    656,  10283,    206,  32223,    656,    444,    656,\n",
       "           4068,    656,   3596,    656,  27712,    656,     23,    656, 118742,\n",
       "            656,      6,      4,    656,  41267,     33,    656,      6,      4,\n",
       "            656, 157436,    656,    165,    656,     68,    656,  82237,  30766,\n",
       "            656,    599,    656,      6,      5,    656,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f830b720160>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataloaders\n",
    "loader_obj = createDataloaders(train_data,config.TRAIN_BATCH_SIZE)\n",
    "train_dataloader = loader_obj.createDataloaders()\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21700"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_obj = createDataloaders(dataset_eval,config.VALID_BATCH_SIZE)\n",
    "val_dataloader = loader_obj.createDataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  7103,    70,  ...,     1,     1,     1],\n",
       "        [    0,  6044, 39395,  ...,     1,     1,     1],\n",
       "        [    0,   903, 36049,  ...,     1,     1,     1],\n",
       "        [    0,   678,    70,  ...,     1,     1,     1]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(val_dataloader))\n",
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(output,target,mask):\n",
    "    lfn = nn.CrossEntropyLoss()\n",
    "    active_loss = mask.view(-1) == 1 #loss calculation for non padded tokens only (mask =1)\n",
    "    active_logits = output.view(-1,2)\n",
    "    active_labels = torch.where(\n",
    "        active_loss,\n",
    "        target.view(-1),\n",
    "        torch.tensor(lfn.ignore_index).type_as(target)    \n",
    "    )\n",
    "    loss = lfn(active_logits,active_labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(EntityModel, self).__init__()\n",
    "        self.bert = XLMRobertaForTokenClassification.from_pretrained(config.BASE_MODEL,output_attentions = False, output_hidden_states = False)\n",
    "#         self.bert_drop_1 = nn.Dropout(0.3)\n",
    "#         self.out_tag = nn.Linear(768, 2)\n",
    "    \n",
    "    def forward(self, ids, attention_mask, labels):\n",
    "        \n",
    "        outputs = self.bert(ids,\n",
    "                                attention_mask = attention_mask,\n",
    "                                labels = labels,return_dict=False)\n",
    "#         bo_tag = self.bert_drop_1(output_1)\n",
    "        \n",
    "#         tag = self.out_tag(bo_tag)  \n",
    "        \n",
    "        loss_tag = loss_fn(outputs[1],labels,attention_mask)\n",
    "        \n",
    "#         return bo_tag,loss\n",
    "        return loss_tag, outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_input_ids = batch[0].cuda()\n",
    "# b_input_mask = batch[1].cuda()\n",
    "# b_labels = batch[2].cuda()\n",
    "# outputs = model(b_input_ids, \n",
    "#                 b_input_mask,\n",
    "#                 labels=b_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EntityModel(\n",
       "  (bert): XLMRobertaForTokenClassification(\n",
       "    (roberta): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EntityModel()\n",
    "model.cuda()\n",
    "# model = nn.DataParallel(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "# torch.cuda.empty_cache()\n",
    "# outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.001,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "num_train_steps = int(len(train_data) / config.TRAIN_BATCH_SIZE * config.EPOCHS) #10 is the batchsize\n",
    "optimizer = AdamW(optimizer_parameters, lr=5e-5) # used 3e-5\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.6/site-packages (4.12.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.51.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.19.1)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.1.2)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers) (4.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (0.1.96)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "config = reload(config) # config file that has been changed after first import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss = np.inf\n",
    "# train_loss_lst = []\n",
    "# val_loss_lst = []\n",
    "# with open('loss_values.txt', 'w') as f:\n",
    "#     for epoch in range(config.EPOCHS):\n",
    "#             print(f'Epoch {epoch} of {config.EPOCHS}')\n",
    "#             train_loss = engine.train_fn(train_dataloader, model, optimizer, scheduler)\n",
    "#             test_loss = engine.eval_fn(val_dataloader, model)\n",
    "#             print(f\"Train Loss = {train_loss} Valid Loss = {test_loss}\")\n",
    "#             train_loss_lst.append(train_loss)\n",
    "#             val_loss_lst.append(test_loss)\n",
    "#             f.write(f\"Train_loss {epoch} : {str(train_loss)}\" + '\\n')\n",
    "#             f.write(f\"val_loss {epoch} : {str(val_loss_lst)}\" + '\\n')\n",
    "#             if test_loss < best_loss:\n",
    "#                 torch.save(model.state_dict(), 'model_xlmrobertatokenclassificationmodel_augmentedData_2.bin')\n",
    "#                 best_loss = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loss_lst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cc5f9b23adcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loss_lst' is not defined"
     ]
    }
   ],
   "source": [
    "print(train_loss_lst)\n",
    "print(val_loss_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = np.arange(1,6)\n",
    "plt.plot(epochs,train_loss_lst,label='train_loss')\n",
    "plt.plot(epochs,val_loss_lst,label='validation_loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.title('XMLRobertaForTokenClassification')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained('xlm-roberta-base')\n",
    "config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): EntityModel(\n",
      "    (bert): XLMRobertaForTokenClassification(\n",
      "      (roberta): RobertaModel(\n",
      "        (embeddings): RobertaEmbeddings(\n",
      "          (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
      "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "          (token_type_embeddings): Embedding(1, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): RobertaEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.2xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
