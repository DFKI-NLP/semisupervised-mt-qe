{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.19.1)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Using cached huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Using cached tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2021.11.10-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (748 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.51.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers) (4.4.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.25.1)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.4.1-py3-none-any.whl (9.9 kB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Installing collected packages: regex, filelock, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.4.1 huggingface-hub-0.2.1 regex-2021.11.10 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.15.0\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.96-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.96\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.6.7-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.6/site-packages (from nltk) (2021.11.10)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from nltk) (4.51.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from nltk) (1.0.1)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.6.7\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-7.6.5-py2.py3-none-any.whl (121 kB)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.6/site-packages (from ipywidgets) (4.3.3)\n",
      "Collecting jupyterlab-widgets>=1.0.0\n",
      "  Using cached jupyterlab_widgets-1.0.2-py3-none-any.whl (243 kB)\n",
      "Collecting nbformat>=4.2.0\n",
      "  Using cached nbformat-5.1.3-py3-none-any.whl (178 kB)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.6/site-packages (from ipywidgets) (5.5.6)\n",
      "Collecting widgetsnbextension~=3.5.0\n",
      "  Using cached widgetsnbextension-3.5.2-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.6/site-packages (from ipywidgets) (7.16.2)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.6/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: tornado>=4.2 in /opt/conda/lib/python3.6/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: jupyter-client in /opt/conda/lib/python3.6/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.1.0)\n",
      "Collecting jedi<=0.17.2,>=0.10\n",
      "  Using cached jedi-0.17.2-py2.py3-none-any.whl (1.4 MB)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.24)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (59.6.0)\n",
      "Collecting parso<0.8.0,>=0.7.0\n",
      "  Using cached parso-0.7.1-py2.py3-none-any.whl (109 kB)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.6/site-packages (from nbformat>=4.2.0->ipywidgets) (4.9.1)\n",
      "Collecting jsonschema!=2.5.0,>=2.4\n",
      "  Using cached jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: six>=1.11.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (1.16.0)\n",
      "Collecting pyrsistent>=0.14.0\n",
      "  Using cached pyrsistent-0.18.0-cp36-cp36m-manylinux1_x86_64.whl (117 kB)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (4.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Collecting notebook>=4.4.1\n",
      "  Using cached notebook-6.4.6-py3-none-any.whl (9.9 MB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.0.1)\n",
      "Collecting terminado>=0.8.3\n",
      "  Using cached terminado-0.12.1-py3-none-any.whl (15 kB)\n",
      "Collecting Send2Trash>=1.8.0\n",
      "  Using cached Send2Trash-1.8.0-py3-none-any.whl (18 kB)\n",
      "Collecting prometheus-client\n",
      "  Using cached prometheus_client-0.12.0-py2.py3-none-any.whl (57 kB)\n",
      "Collecting argon2-cffi\n",
      "  Using cached argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.4)\n",
      "Collecting nbconvert\n",
      "  Using cached nbconvert-6.0.7-py3-none-any.whl (552 kB)\n",
      "Requirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (22.1.0)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.6/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: ptyprocess in /opt/conda/lib/python3.6/site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.10.0.0)\n",
      "Collecting argon2-cffi-bindings\n",
      "  Using cached argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.20)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.6/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.1)\n",
      "Collecting nbclient<0.6.0,>=0.5.0\n",
      "  Using cached nbclient-0.5.9-py3-none-any.whl (69 kB)\n",
      "Collecting mistune<2,>=0.8.1\n",
      "  Using cached mistune-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting jupyterlab-pygments\n",
      "  Using cached jupyterlab_pygments-0.1.2-py2.py3-none-any.whl (4.6 kB)\n",
      "Collecting testpath\n",
      "  Using cached testpath-0.5.0-py3-none-any.whl (84 kB)\n",
      "Collecting defusedxml\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting pandocfilters>=1.4.1\n",
      "  Using cached pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting bleach\n",
      "  Using cached bleach-4.1.0-py2.py3-none-any.whl (157 kB)\n",
      "Collecting async-generator\n",
      "  Using cached async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.9)\n",
      "Collecting webencodings\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.4.7)\n",
      "Installing collected packages: pyrsistent, parso, jsonschema, webencodings, nbformat, jedi, async-generator, testpath, pandocfilters, nbclient, mistune, jupyterlab-pygments, defusedxml, bleach, argon2-cffi-bindings, terminado, Send2Trash, prometheus-client, nbconvert, argon2-cffi, notebook, widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "  Attempting uninstall: parso\n",
      "    Found existing installation: parso 0.8.0\n",
      "    Uninstalling parso-0.8.0:\n",
      "      Successfully uninstalled parso-0.8.0\n",
      "  Attempting uninstall: jedi\n",
      "    Found existing installation: jedi 0.18.0\n",
      "    Uninstalling jedi-0.18.0:\n",
      "      Successfully uninstalled jedi-0.18.0\n",
      "Successfully installed Send2Trash-1.8.0 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 async-generator-1.10 bleach-4.1.0 defusedxml-0.7.1 ipywidgets-7.6.5 jedi-0.17.2 jsonschema-3.2.0 jupyterlab-pygments-0.1.2 jupyterlab-widgets-1.0.2 mistune-0.8.4 nbclient-0.5.9 nbconvert-6.0.7 nbformat-5.1.3 notebook-6.4.6 pandocfilters-1.5.0 parso-0.7.1 prometheus-client-0.12.0 pyrsistent-0.18.0 terminado-0.12.1 testpath-0.5.0 webencodings-0.5.1 widgetsnbextension-3.5.2\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install nltk\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in /opt/conda/lib/python3.6/site-packages (1.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.6/site-packages (from seqeval) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.6/site-packages (from seqeval) (1.19.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.6/site-packages (from scikit-learn>=0.21.3->seqeval) (1.5.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn>=0.21.3->seqeval) (2.1.0)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.6/site-packages (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.19.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers) (3.4.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.51.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers) (4.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (0.1.96)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.6/site-packages (4.15.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.19.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers) (4.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers) (3.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.51.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (0.1.96)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (3.6.7)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from nltk) (4.51.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.6/site-packages (from nltk) (2021.11.10)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "# from DataAugmentation \n",
    "from data_utils import loadDatafromFile,createTokenizedDf,CompDataset,createkfoldData,createDataloaders\n",
    "\n",
    "from DataAugmentation import DataAugmentation\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from importlib import reload\n",
    "import config\n",
    "from transformers import XLMRobertaForTokenClassification, XLMRobertaConfig ,BertModel, XLMRobertaTokenizer, XLMRobertaModel, BertForTokenClassification\n",
    "from seqeval.metrics import accuracy_score, classification_report,f1_score\n",
    "from sklearn.metrics import matthews_corrcoef,classification_report\n",
    "import engine\n",
    "# from model_new import EntityModel\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "def process_data(filePath_src,filePath_tar, filePath_srcTags,filePath_tarTags,model_type):\n",
    "    \n",
    "    dataObj = loadDatafromFile(filePath_src,filePath_tar, filePath_srcTags,filePath_tarTags)\n",
    "    df= dataObj.createDf() # get dataframe from files\n",
    "    obj_tokenized = createTokenizedDf(df,model_type)\n",
    "    df_new= obj_tokenized.convertDf()\n",
    "    train_data = CompDataset(df_new,model_type)\n",
    "    return train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading dev/eval data\n",
    "dataset_eval = process_data(config.filePath_src_eval,config.filePath_tar_eval, config.filePath_srcTags_eval,config.filePath_tarTags_eval,model_type = 'xlm')\n",
    "len(dataset_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>src_tokens</th>\n",
       "      <th>tar_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>José Ortega y Gasset visited Husserl at Freibu...</td>\n",
       "      <td>1934 besuchte José Ortega y Gasset Husserl in ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>However , a disappointing ninth in China meant...</td>\n",
       "      <td>Eine enttäuschende Neunte in China bedeutete j...</td>\n",
       "      <td>OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...</td>\n",
       "      <td>OK BAD OK BAD OK BAD OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In his diary , Chase wrote that the release of...</td>\n",
       "      <td>In seinem Tagebuch , Chase schrieb , dass die ...</td>\n",
       "      <td>OK OK OK OK BAD BAD OK OK OK OK OK OK OK OK BA...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK BAD OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heavy arquebuses mounted on wagons were called...</td>\n",
       "      <td>Schwere Arquebuses auf Waggons montiert wurden...</td>\n",
       "      <td>OK BAD BAD OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK BAD OK OK OK OK BAD BAD OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once North Pacific salmon die off after spawni...</td>\n",
       "      <td>Sobald der nordpazifische Lachs nach dem Laich...</td>\n",
       "      <td>OK OK OK OK BAD OK OK OK OK OK BAD OK BAD OK O...</td>\n",
       "      <td>OK OK OK BAD OK OK OK BAD OK OK OK OK OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>Some may also discourage or disallow unsanitar...</td>\n",
       "      <td>Einige können auch unhygienische Praktiken wie...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>In the late 1860s , the crinolines disappeared...</td>\n",
       "      <td>In den späten 1860er Jahren verschwanden die K...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK BAD BAD OK OK OK...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>Disco was criticized as mindless , consumerist...</td>\n",
       "      <td>Disco wurde als geistlos , konsumistisch , übe...</td>\n",
       "      <td>OK OK OK OK BAD OK BAD OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>Planters would then fill large hogsheads with ...</td>\n",
       "      <td>Die Pflanzer würden dann große Heuschrecken mi...</td>\n",
       "      <td>OK OK OK BAD OK BAD OK OK BAD BAD OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK BAD OK OK OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>He slew Krishna 's most dangerous enemy , Jara...</td>\n",
       "      <td>Er tötete Krishnas gefährlichsten Feind Jarasa...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK BAD OK OK ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 source  \\\n",
       "0     José Ortega y Gasset visited Husserl at Freibu...   \n",
       "1     However , a disappointing ninth in China meant...   \n",
       "2     In his diary , Chase wrote that the release of...   \n",
       "3     Heavy arquebuses mounted on wagons were called...   \n",
       "4     Once North Pacific salmon die off after spawni...   \n",
       "...                                                 ...   \n",
       "6995  Some may also discourage or disallow unsanitar...   \n",
       "6996  In the late 1860s , the crinolines disappeared...   \n",
       "6997  Disco was criticized as mindless , consumerist...   \n",
       "6998  Planters would then fill large hogsheads with ...   \n",
       "6999  He slew Krishna 's most dangerous enemy , Jara...   \n",
       "\n",
       "                                                 target  \\\n",
       "0     1934 besuchte José Ortega y Gasset Husserl in ...   \n",
       "1     Eine enttäuschende Neunte in China bedeutete j...   \n",
       "2     In seinem Tagebuch , Chase schrieb , dass die ...   \n",
       "3     Schwere Arquebuses auf Waggons montiert wurden...   \n",
       "4     Sobald der nordpazifische Lachs nach dem Laich...   \n",
       "...                                                 ...   \n",
       "6995  Einige können auch unhygienische Praktiken wie...   \n",
       "6996  In den späten 1860er Jahren verschwanden die K...   \n",
       "6997  Disco wurde als geistlos , konsumistisch , übe...   \n",
       "6998  Die Pflanzer würden dann große Heuschrecken mi...   \n",
       "6999  Er tötete Krishnas gefährlichsten Feind Jarasa...   \n",
       "\n",
       "                                             src_tokens  \\\n",
       "0                      OK OK OK OK OK OK OK OK OK OK OK   \n",
       "1     OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...   \n",
       "2     OK OK OK OK BAD BAD OK OK OK OK OK OK OK OK BA...   \n",
       "3                    OK BAD BAD OK OK OK OK OK OK OK OK   \n",
       "4     OK OK OK OK BAD OK OK OK OK OK BAD OK BAD OK O...   \n",
       "...                                                 ...   \n",
       "6995  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...   \n",
       "6996  OK OK OK OK OK OK OK OK OK OK BAD BAD OK OK OK...   \n",
       "6997              OK OK OK OK BAD OK BAD OK OK OK OK OK   \n",
       "6998   OK OK OK BAD OK BAD OK OK BAD BAD OK OK OK OK OK   \n",
       "6999  OK OK OK OK OK OK OK OK OK OK OK OK BAD OK OK ...   \n",
       "\n",
       "                                             tar_tokens  \n",
       "0     OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "1     OK BAD OK BAD OK BAD OK OK OK OK OK OK OK OK O...  \n",
       "2     OK OK OK OK OK OK OK OK OK BAD OK BAD OK OK OK...  \n",
       "3     OK OK OK BAD OK OK OK OK BAD BAD OK OK OK OK O...  \n",
       "4     OK OK OK BAD OK OK OK BAD OK OK OK OK OK OK OK...  \n",
       "...                                                 ...  \n",
       "6995  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "6996  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "6997  OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...  \n",
       "6998  OK OK OK OK OK BAD OK OK OK OK OK BAD OK OK OK...  \n",
       "6999  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "\n",
       "[7000 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataObj = loadDatafromFile(config.filePath_src,config.filePath_tar, config.filePath_srcTags,config.filePath_tarTags)\n",
    "df= dataObj.createDf() \n",
    "df\n",
    "# list(df.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (3.6.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from nltk) (4.51.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.6/site-packages (from nltk) (2021.11.10)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from nltk) (7.1.2)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import DataAugmentation\n",
    "DataAugmentation = reload(DataAugmentation)\n",
    "from DataAugmentation import DataAugmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>src_tokens</th>\n",
       "      <th>tar_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>José Ortega y Gasset visited Husserl at Freibu...</td>\n",
       "      <td>1934 besuchte José Ortega y Gasset Husserl in ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>However , a disappointing ninth in China meant...</td>\n",
       "      <td>Eine enttäuschende Neunte in China bedeutete j...</td>\n",
       "      <td>OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...</td>\n",
       "      <td>OK BAD OK BAD OK BAD OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In his diary , Chase wrote that the release of...</td>\n",
       "      <td>In seinem Tagebuch , Chase schrieb , dass die ...</td>\n",
       "      <td>OK OK OK OK BAD BAD OK OK OK OK OK OK OK OK BA...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK BAD OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heavy arquebuses mounted on wagons were called...</td>\n",
       "      <td>Schwere Arquebuses auf Waggons montiert wurden...</td>\n",
       "      <td>OK BAD BAD OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK BAD OK OK OK OK BAD BAD OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once North Pacific salmon die off after spawni...</td>\n",
       "      <td>Sobald der nordpazifische Lachs nach dem Laich...</td>\n",
       "      <td>OK OK OK OK BAD OK OK OK OK OK BAD OK BAD OK O...</td>\n",
       "      <td>OK OK OK BAD OK OK OK BAD OK OK OK OK OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20995</th>\n",
       "      <td>was criticized as mindless , , overproduced an...</td>\n",
       "      <td>Disco wurde als geistlos , konsumistisch , übe...</td>\n",
       "      <td>OK OK OK BAD OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20996</th>\n",
       "      <td>Planters would then fill hogsheads and them to...</td>\n",
       "      <td>Die Pflanzer würden dann große Heuschrecken mi...</td>\n",
       "      <td>OK OK OK BAD BAD BAD OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK BAD OK OK OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20997</th>\n",
       "      <td>Planters would then fill with tobacco and conv...</td>\n",
       "      <td>Die Pflanzer würden dann große Heuschrecken mi...</td>\n",
       "      <td>OK OK OK BAD OK OK BAD BAD OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK BAD OK OK OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20998</th>\n",
       "      <td>He slew Krishna dangerous enemy Jarasandha , a...</td>\n",
       "      <td>Er tötete Krishnas gefährlichsten Feind Jarasa...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK BAD OK OK OK OK OK OK ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20999</th>\n",
       "      <td>He slew Krishna 's dangerous enemy , Jarasandh...</td>\n",
       "      <td>Er tötete Krishnas gefährlichsten Feind Jarasa...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK BAD OK OK OK OK ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  source  \\\n",
       "0      José Ortega y Gasset visited Husserl at Freibu...   \n",
       "1      However , a disappointing ninth in China meant...   \n",
       "2      In his diary , Chase wrote that the release of...   \n",
       "3      Heavy arquebuses mounted on wagons were called...   \n",
       "4      Once North Pacific salmon die off after spawni...   \n",
       "...                                                  ...   \n",
       "20995  was criticized as mindless , , overproduced an...   \n",
       "20996  Planters would then fill hogsheads and them to...   \n",
       "20997  Planters would then fill with tobacco and conv...   \n",
       "20998  He slew Krishna dangerous enemy Jarasandha , a...   \n",
       "20999  He slew Krishna 's dangerous enemy , Jarasandh...   \n",
       "\n",
       "                                                  target  \\\n",
       "0      1934 besuchte José Ortega y Gasset Husserl in ...   \n",
       "1      Eine enttäuschende Neunte in China bedeutete j...   \n",
       "2      In seinem Tagebuch , Chase schrieb , dass die ...   \n",
       "3      Schwere Arquebuses auf Waggons montiert wurden...   \n",
       "4      Sobald der nordpazifische Lachs nach dem Laich...   \n",
       "...                                                  ...   \n",
       "20995  Disco wurde als geistlos , konsumistisch , übe...   \n",
       "20996  Die Pflanzer würden dann große Heuschrecken mi...   \n",
       "20997  Die Pflanzer würden dann große Heuschrecken mi...   \n",
       "20998  Er tötete Krishnas gefährlichsten Feind Jarasa...   \n",
       "20999  Er tötete Krishnas gefährlichsten Feind Jarasa...   \n",
       "\n",
       "                                              src_tokens  \\\n",
       "0                       OK OK OK OK OK OK OK OK OK OK OK   \n",
       "1      OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...   \n",
       "2      OK OK OK OK BAD BAD OK OK OK OK OK OK OK OK BA...   \n",
       "3                     OK BAD BAD OK OK OK OK OK OK OK OK   \n",
       "4      OK OK OK OK BAD OK OK OK OK OK BAD OK BAD OK O...   \n",
       "...                                                  ...   \n",
       "20995                     OK OK OK BAD OK OK OK OK OK OK   \n",
       "20996                   OK OK OK BAD BAD BAD OK OK OK OK   \n",
       "20997          OK OK OK BAD OK OK BAD BAD OK OK OK OK OK   \n",
       "20998  OK OK OK OK OK OK OK OK BAD OK OK OK OK OK OK ...   \n",
       "20999  OK OK OK OK OK OK OK OK OK OK BAD OK OK OK OK ...   \n",
       "\n",
       "                                              tar_tokens  \n",
       "0      OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "1      OK BAD OK BAD OK BAD OK OK OK OK OK OK OK OK O...  \n",
       "2      OK OK OK OK OK OK OK OK OK BAD OK BAD OK OK OK...  \n",
       "3      OK OK OK BAD OK OK OK OK BAD BAD OK OK OK OK O...  \n",
       "4      OK OK OK BAD OK OK OK BAD OK OK OK OK OK OK OK...  \n",
       "...                                                  ...  \n",
       "20995  OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...  \n",
       "20996  OK OK OK OK OK BAD OK OK OK OK OK BAD OK OK OK...  \n",
       "20997  OK OK OK OK OK BAD OK OK OK OK OK BAD OK OK OK...  \n",
       "20998  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "20999  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "\n",
       "[21000 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataaug_obj = DataAugmentation(df,swap_words=2,syn_words=4,del_words_prob=0.2,num_sentences=2)  \n",
    "# swapDataset = dataaug_obj.random_swap()\n",
    "del_augDataset = dataaug_obj.random_deletion()\n",
    "# del_augDataset = del_augDataset[7000:]\n",
    "# syn_dataset = dataaug_obj.synonym_replacement()\n",
    "# syn_dataset = syn_dataset[7000:]\n",
    "# frames = [swapDataset , del_augDataset,syn_dataset]\n",
    "# aug_df  = pd.concat(frames)\n",
    "del_augDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Slovak partisans in the Crimea Odessa assisted Soviet partisans .',\n",
       "        'Slowakische Partisanen auf der Krim und Odessa unterstützten sowjetische Partisanen .',\n",
       "        'OK OK OK OK OK OK OK OK OK OK',\n",
       "        'OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK'],\n",
       "       ['Assisting Sukarno as supreme commander ABRI , would be an ABRI chief of staff',\n",
       "        'Sukarno als Oberbefehlshaber von ABRI zu unterstützen , wäre ein Stabschef von ABRI .',\n",
       "        'OK OK OK OK OK OK OK OK OK OK OK OK OK OK',\n",
       "        'OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK'],\n",
       "       [', the pleadings between Jean de from and Arnaud de Laforcade , inhabitant of Fourré .',\n",
       "        ', das Gerichtsverfahren zwischen Jean de Pee von Laborde und Arnaud de Laforcade , Einwohner von Fourré .',\n",
       "        'OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK',\n",
       "        'OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK'],\n",
       "       ['Classical antiquity and the ancient Armenian Kingdom',\n",
       "        'Klassische Antike und das antike armenische Königreich',\n",
       "        'OK OK OK OK OK OK OK',\n",
       "        'OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK'],\n",
       "       ['2016 , Farage praised Trump for dominating \" Hillary Clinton , to a silverback gorilla .',\n",
       "        'Im Oktober 2016 lobte Farage Trump für die \" Dominanz \" Hillary Clintons und verglich ihn mit einem Silberrückengorilla .',\n",
       "        'OK OK OK OK OK OK OK BAD OK OK OK OK OK OK OK OK',\n",
       "        'OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK BAD OK OK BAD BAD OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del_augDataset.iloc[41995:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>src_tokens</th>\n",
       "      <th>tar_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B covered convoy MW 16 of one tanker escorted ...</td>\n",
       "      <td>Die Operation Quadrangle B umfasste den Konvoi...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sun Quan continued to recognize his jure to We...</td>\n",
       "      <td>Sun Quan erkannte weiterhin seine De-jure-Ober...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK BAD OK OK OK ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK BAD OK OK ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Hornet and the Gremlin shared platforms .</td>\n",
       "      <td>Die Hornet und die späteren Gremlin teilten Pl...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK BAD OK OK OK OK OK OK OK OK OK OK OK OK BAD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Both the Union Confederacy soon up with plans ...</td>\n",
       "      <td>Sowohl die Union als auch die Konföderation ka...</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK OK OK BAD</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The peasants many the scattered plunderers .</td>\n",
       "      <td>Die Bauern töteten viele der zerstreuten Plünd...</td>\n",
       "      <td>OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20995</th>\n",
       "      <td>Reversals and compensation the survivors and t...</td>\n",
       "      <td>Reversals des Erreichers und Entschädigung für...</td>\n",
       "      <td>BAD OK OK BAD OK OK BAD OK</td>\n",
       "      <td>OK BAD OK BAD OK BAD OK OK OK OK OK BAD OK BAD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20996</th>\n",
       "      <td>The Creek Association within the .</td>\n",
       "      <td>Die Darby Creek Valley Association ist innerha...</td>\n",
       "      <td>OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK BAD OK ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20997</th>\n",
       "      <td>Electric trains to via the line commenced on 2...</td>\n",
       "      <td>Elektrische Züge nach Hornsby über die Hauptli...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20998</th>\n",
       "      <td>The Group painted figures combined or palmette...</td>\n",
       "      <td>Die Phantomgruppe malte hauptsächlich verhüllt...</td>\n",
       "      <td>OK OK OK OK OK OK BAD OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK BAD OK OK OK OK OK ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20999</th>\n",
       "      <td>The 1st and 2nd Regiments number of and redesi...</td>\n",
       "      <td>Das 1. und 2. Regiment durchlaufen eine Reihe ...</td>\n",
       "      <td>OK OK OK OK OK BAD BAD BAD BAD BAD BAD OK BAD ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK BAD BAD OK BAD O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  source  \\\n",
       "0      B covered convoy MW 16 of one tanker escorted ...   \n",
       "1      Sun Quan continued to recognize his jure to We...   \n",
       "2          The Hornet and the Gremlin shared platforms .   \n",
       "3      Both the Union Confederacy soon up with plans ...   \n",
       "4           The peasants many the scattered plunderers .   \n",
       "...                                                  ...   \n",
       "20995  Reversals and compensation the survivors and t...   \n",
       "20996                 The Creek Association within the .   \n",
       "20997  Electric trains to via the line commenced on 2...   \n",
       "20998  The Group painted figures combined or palmette...   \n",
       "20999  The 1st and 2nd Regiments number of and redesi...   \n",
       "\n",
       "                                                  target  \\\n",
       "0      Die Operation Quadrangle B umfasste den Konvoi...   \n",
       "1      Sun Quan erkannte weiterhin seine De-jure-Ober...   \n",
       "2      Die Hornet und die späteren Gremlin teilten Pl...   \n",
       "3      Sowohl die Union als auch die Konföderation ka...   \n",
       "4      Die Bauern töteten viele der zerstreuten Plünd...   \n",
       "...                                                  ...   \n",
       "20995  Reversals des Erreichers und Entschädigung für...   \n",
       "20996  Die Darby Creek Valley Association ist innerha...   \n",
       "20997  Elektrische Züge nach Hornsby über die Hauptli...   \n",
       "20998  Die Phantomgruppe malte hauptsächlich verhüllt...   \n",
       "20999  Das 1. und 2. Regiment durchlaufen eine Reihe ...   \n",
       "\n",
       "                                              src_tokens  \\\n",
       "0                 OK OK OK OK OK OK OK OK OK OK OK OK OK   \n",
       "1      OK OK OK OK OK OK OK OK OK OK OK BAD OK OK OK ...   \n",
       "2                                OK OK OK OK OK OK OK OK   \n",
       "3                  OK OK OK OK OK OK OK BAD OK OK OK BAD   \n",
       "4                                   OK OK OK OK OK OK OK   \n",
       "...                                                  ...   \n",
       "20995                         BAD OK OK BAD OK OK BAD OK   \n",
       "20996                                  OK OK OK OK OK OK   \n",
       "20997                OK OK OK OK OK OK OK OK OK OK OK OK   \n",
       "20998                        OK OK OK OK OK OK BAD OK OK   \n",
       "20999  OK OK OK OK OK BAD BAD BAD BAD BAD BAD OK BAD ...   \n",
       "\n",
       "                                              tar_tokens  \n",
       "0      OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "1      OK OK OK OK OK OK OK OK OK OK OK OK BAD OK OK ...  \n",
       "2      OK BAD OK OK OK OK OK OK OK OK OK OK OK OK BAD...  \n",
       "3      OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK B...  \n",
       "4      OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "...                                                  ...  \n",
       "20995  OK BAD OK BAD OK BAD OK OK OK OK OK BAD OK BAD...  \n",
       "20996  OK OK OK OK OK OK OK OK OK OK OK OK OK BAD OK ...  \n",
       "20997  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "20998  OK OK OK OK OK OK OK OK OK BAD OK OK OK OK OK ...  \n",
       "20999  OK OK OK OK OK OK OK OK OK OK BAD BAD OK BAD O...  \n",
       "\n",
       "[21000 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del_augDataset =del_augDataset.sample(frac=1).reset_index(drop=True)\n",
    "del_augDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in /opt/conda/lib/python3.6/site-packages (1.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.6/site-packages (from seqeval) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.6/site-packages (from seqeval) (1.19.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn>=0.21.3->seqeval) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.6/site-packages (from scikit-learn>=0.21.3->seqeval) (1.5.4)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.6/site-packages (4.15.0)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers) (4.4.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.51.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.19.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (0.1.96)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import data_utils\n",
    "data_utils=reload(data_utils)\n",
    "from data_utils import loadDatafromFile,createTokenizedDf,CompDataset,createkfoldData,createDataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_tokenized = createTokenizedDf(del_augDataset,model_type = 'xlm')\n",
    "df_new= obj_tokenized.convertDf()\n",
    "# enc_label = preprocessing.LabelEncoder()\n",
    "# df_new['labels']= enc_label.fit_transform(df_new['labels'])\n",
    "train_data = CompDataset(df_new, model_type = 'xlm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([     0,  17151,  12426,   2765,    113,    527, 110896,  36997,     71,\n",
       "          20387,   2189,    141,     99, 183124,     23,  58020,      6,      5,\n",
       "              2,      2,    656,  58020,    656, 138438,     13,    656,  17151,\n",
       "            656,  12426,   2765,    656,    113,    656,    527, 110896,    656,\n",
       "          20387,   2189,    141,    656,     23,    656, 183124,    656,      6,\n",
       "              5,    656,      2,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([-100,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1, -100, -100,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1313"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataloaders\n",
    "loader_obj = createDataloaders(train_data,config.TRAIN_BATCH_SIZE)\n",
    "train_dataloader = loader_obj.createDataloaders()\n",
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_obj = createDataloaders(dataset_eval,config.VALID_BATCH_SIZE)\n",
    "val_dataloader = loader_obj.createDataloaders()\n",
    "len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-01-10 20:30:55.834 pytorch-1-6-gpu-py-ml-g4dn-8xlarge-d01940f40012030c1b9f0e178db4:31 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2022-01-10 20:30:55.867 pytorch-1-6-gpu-py-ml-g4dn-8xlarge-d01940f40012030c1b9f0e178db4:31 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([     0,    581,  13579,    721,   1799,    927,      7,   5117,  43780,\n",
       "           297,    390,    884,   1505,   6431, 177592,     23,    543,  12977,\n",
       "             6,      5,      2,      2,    656,   1310,    656, 106971,    656,\n",
       "         26842,    150,  92733,    656,   4180,    656,  94756,      7,    656,\n",
       "           543,  12977,    656,    542,    656,    884,   1505,   6431,    656,\n",
       "        177592,    656, 232307,    656,      6,      5,    656,      2,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(val_dataloader))\n",
    "batch[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(output,target,mask):\n",
    "    lfn = nn.CrossEntropyLoss()\n",
    "    active_loss = mask.view(-1) == 1 #loss calculation for non padded tokens only (mask =1)\n",
    "    active_logits = output.view(-1,2)\n",
    "    active_labels = torch.where(\n",
    "        active_loss,\n",
    "        target.view(-1),\n",
    "        torch.tensor(lfn.ignore_index).type_as(target)    \n",
    "    )\n",
    "    loss = lfn(active_logits,active_labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(EntityModel, self).__init__()\n",
    "        self.bert = XLMRobertaForTokenClassification.from_pretrained(config.BASE_MODEL,output_attentions = False, output_hidden_states = False)\n",
    "#         self.bert_drop_1 = nn.Dropout(0.3)\n",
    "#         self.out_tag = nn.Linear(768, 2)\n",
    "    \n",
    "    def forward(self, ids, attention_mask, labels):\n",
    "        \n",
    "        outputs = self.bert(ids,\n",
    "                                attention_mask = attention_mask,\n",
    "                                labels = labels,return_dict=False)\n",
    "#         bo_tag = self.bert_drop_1(output_1)\n",
    "        \n",
    "#         tag = self.out_tag(bo_tag)  \n",
    "        \n",
    "#         loss_tag = loss_fn(outputs[1],labels,attention_mask)\n",
    "        \n",
    "#         return bo_tag,loss\n",
    "        return outputs[0], outputs[1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_input_ids = batch[0].cuda()\n",
    "# b_input_mask = batch[1].cuda()\n",
    "# b_labels = batch[2].cuda()\n",
    "# outputs = model(b_input_ids, \n",
    "#                 b_input_mask,\n",
    "#                 labels=b_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EntityModel(\n",
       "  (bert): XLMRobertaForTokenClassification(\n",
       "    (roberta): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EntityModel()\n",
    "model.cuda()\n",
    "# model = nn.DataParallel(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "# torch.cuda.empty_cache()\n",
    "# outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    def __init__(self, mode='max', min_delta=0, patience=10, percentage=False):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = 0\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "\n",
    "        if np.isnan(metrics):\n",
    "            return True\n",
    "\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            print('terminating because of early stopping!')\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best \n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best \n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (\n",
    "                            best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (\n",
    "                            best * min_delta / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.001,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "num_train_steps = int(len(del_augDataset) / config.TRAIN_BATCH_SIZE * config.EPOCHS) #10 is the batchsize\n",
    "optimizer = AdamW(optimizer_parameters, lr=2e-5) # used 3e-5\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",
    ")\n",
    "early_stopping = EarlyStopping(patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:13<00:00,  9.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.0\n",
      "Average F1 Validation score for class 1 : 0.9294044211680166\n",
      "Average Accuracy Validation score  : 0.8681190540521324\n",
      "Average mcc Validation score  : 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      9274\n",
      "           1       0.87      1.00      0.93     61047\n",
      "\n",
      "    accuracy                           0.87     70321\n",
      "   macro avg       0.43      0.50      0.46     70321\n",
      "weighted avg       0.75      0.87      0.81     70321\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "loss_test,f1_score_0_test, f1_score_1_test, accuracy_score_test, mcc_score_test,labels_test,preds_test = engine.eval_fn(val_dataloader, model)\n",
    "print(classification_report(labels_test,preds_test)) # validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1313/1313 [04:41<00:00,  4.67it/s]\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.0\n",
      "Average F1 Validation score for class 1 : 0.9329219141459792\n",
      "Average Accuracy Validation score  : 0.8742770810435381\n",
      "Average mcc Validation score  : 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00    175584\n",
      "           1       0.87      1.00      0.93   1221011\n",
      "\n",
      "    accuracy                           0.87   1396595\n",
      "   macro avg       0.44      0.50      0.47   1396595\n",
      "weighted avg       0.76      0.87      0.82   1396595\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "loss_test,f1_score_0_test, f1_score_1_test, accuracy_score_test, mcc_score_test,labels_test,preds_test = engine.eval_fn(train_dataloader, model)\n",
    "print(classification_report(labels_test,preds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.6/site-packages (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.19.1)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.51.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers) (4.4.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (0.1.96)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "config = reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,param in model.named_parameters():\n",
    "    if 'classifier' not in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1313/1313 [17:21<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Training score for class 0 : 0.2615500336504836\n",
      "Average F1 Training score for class 1 : 0.9345597811447877\n",
      "Average Accuracy Training score  : 0.8797738091572056\n",
      "Average mcc Training score  : 0.2651824495268958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.17      0.26    175469\n",
      "           1       0.89      0.98      0.93   1220874\n",
      "\n",
      "    accuracy                           0.88   1396343\n",
      "   macro avg       0.73      0.58      0.60   1396343\n",
      "weighted avg       0.85      0.88      0.85   1396343\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:14<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.19965325303403597\n",
      "Average F1 Validation score for class 1 : 0.9323658459474256\n",
      "Average Accuracy Validation score  : 0.8752719671221968\n",
      "Average mcc Validation score  : 0.239580020040288\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.12      0.20      9274\n",
      "           1       0.88      0.99      0.93     61047\n",
      "\n",
      "    accuracy                           0.88     70321\n",
      "   macro avg       0.77      0.55      0.57     70321\n",
      "weighted avg       0.85      0.88      0.84     70321\n",
      "\n",
      "Train Loss = 0.30186314803675124 Valid Loss = 0.36236758852005\n",
      "Epoch 2 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1313/1313 [17:26<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Training score for class 0 : 0.582702842835604\n",
      "Average F1 Training score for class 1 : 0.9510571595917309\n",
      "Average Accuracy Training score  : 0.9123897208637133\n",
      "Average mcc Training score  : 0.5492335688726299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.49      0.58    175469\n",
      "           1       0.93      0.97      0.95   1220874\n",
      "\n",
      "    accuracy                           0.91   1396343\n",
      "   macro avg       0.83      0.73      0.77   1396343\n",
      "weighted avg       0.90      0.91      0.90   1396343\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:14<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.2952108063855915\n",
      "Average F1 Validation score for class 1 : 0.9329658093703038\n",
      "Average Accuracy Validation score  : 0.8775756886278636\n",
      "Average mcc Validation score  : 0.29710170815769477\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.19      0.30      9274\n",
      "           1       0.89      0.98      0.93     61047\n",
      "\n",
      "    accuracy                           0.88     70321\n",
      "   macro avg       0.75      0.59      0.61     70321\n",
      "weighted avg       0.85      0.88      0.85     70321\n",
      "\n",
      "Train Loss = 0.21957505939234692 Valid Loss = 0.4146628511548042\n",
      "Epoch 3 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1313/1313 [17:23<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Training score for class 0 : 0.7260718851487996\n",
      "Average F1 Training score for class 1 : 0.9645590209155037\n",
      "Average Accuracy Training score  : 0.9372382000697537\n",
      "Average mcc Training score  : 0.6951744789457047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.66      0.73    175469\n",
      "           1       0.95      0.98      0.96   1220874\n",
      "\n",
      "    accuracy                           0.94   1396343\n",
      "   macro avg       0.88      0.82      0.85   1396343\n",
      "weighted avg       0.93      0.94      0.93   1396343\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:14<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.3250078051826413\n",
      "Average F1 Validation score for class 1 : 0.9323476492216225\n",
      "Average Accuracy Validation score  : 0.8770210890061291\n",
      "Average mcc Validation score  : 0.31059104690040285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.22      0.33      9274\n",
      "           1       0.89      0.98      0.93     61047\n",
      "\n",
      "    accuracy                           0.88     70321\n",
      "   macro avg       0.74      0.60      0.63     70321\n",
      "weighted avg       0.85      0.88      0.85     70321\n",
      "\n",
      "Train Loss = 0.16070287873555664 Valid Loss = 0.5265298784971237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1313/1313 [17:21<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Training score for class 0 : 0.799406326297758\n",
      "Average F1 Training score for class 1 : 0.9729199188625804\n",
      "Average Accuracy Training score  : 0.9522817817685196\n",
      "Average mcc Training score  : 0.7740163537097449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.76      0.80    175469\n",
      "           1       0.97      0.98      0.97   1220874\n",
      "\n",
      "    accuracy                           0.95   1396343\n",
      "   macro avg       0.91      0.87      0.89   1396343\n",
      "weighted avg       0.95      0.95      0.95   1396343\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:14<00:00,  8.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.3845727738540369\n",
      "Average F1 Validation score for class 1 : 0.9295721077654517\n",
      "Average Accuracy Validation score  : 0.873608168256993\n",
      "Average mcc Validation score  : 0.3374939146539046\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.30      0.38      9274\n",
      "           1       0.90      0.96      0.93     61047\n",
      "\n",
      "    accuracy                           0.87     70321\n",
      "   macro avg       0.72      0.63      0.66     70321\n",
      "weighted avg       0.85      0.87      0.86     70321\n",
      "\n",
      "Train Loss = 0.12241293019093771 Valid Loss = 0.5541496854424477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1313/1313 [17:19<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Training score for class 0 : 0.8419569096947277\n",
      "Average F1 Training score for class 1 : 0.978205012235997\n",
      "Average Accuracy Training score  : 0.9616927932463585\n",
      "Average mcc Training score  : 0.8209190438044313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84    175469\n",
      "           1       0.97      0.98      0.98   1220874\n",
      "\n",
      "    accuracy                           0.96   1396343\n",
      "   macro avg       0.92      0.90      0.91   1396343\n",
      "weighted avg       0.96      0.96      0.96   1396343\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:14<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.37849370599530613\n",
      "Average F1 Validation score for class 1 : 0.9309612027081473\n",
      "Average Accuracy Validation score  : 0.8757270232220816\n",
      "Average mcc Validation score  : 0.33867498018607184\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.29      0.38      9274\n",
      "           1       0.90      0.97      0.93     61047\n",
      "\n",
      "    accuracy                           0.88     70321\n",
      "   macro avg       0.73      0.63      0.65     70321\n",
      "weighted avg       0.85      0.88      0.86     70321\n",
      "\n",
      "Train Loss = 0.09792534601984593 Valid Loss = 0.6254125117063523\n",
      "Epoch 6 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1313/1313 [17:21<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Training score for class 0 : 0.8738781349449651\n",
      "Average F1 Training score for class 1 : 0.9823694297879011\n",
      "Average Accuracy Training score  : 0.9690634750917217\n",
      "Average mcc Training score  : 0.8565966383231011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.85      0.87    175469\n",
      "           1       0.98      0.99      0.98   1220874\n",
      "\n",
      "    accuracy                           0.97   1396343\n",
      "   macro avg       0.94      0.92      0.93   1396343\n",
      "weighted avg       0.97      0.97      0.97   1396343\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:14<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.35037921453780885\n",
      "Average F1 Validation score for class 1 : 0.9320557628117023\n",
      "Average Accuracy Validation score  : 0.8769784274967648\n",
      "Average mcc Validation score  : 0.32494383063665994\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.25      0.35      9274\n",
      "           1       0.90      0.97      0.93     61047\n",
      "\n",
      "    accuracy                           0.88     70321\n",
      "   macro avg       0.74      0.61      0.64     70321\n",
      "weighted avg       0.85      0.88      0.86     70321\n",
      "\n",
      "Train Loss = 0.0788113628709171 Valid Loss = 0.7241875883340836\n",
      "Epoch 7 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1313/1313 [17:28<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Training score for class 0 : 0.8995994868334554\n",
      "Average F1 Training score for class 1 : 0.9858342332756799\n",
      "Average Accuracy Training score  : 0.97517157317364\n",
      "Average mcc Training score  : 0.885591342589789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90    175469\n",
      "           1       0.98      0.99      0.99   1220874\n",
      "\n",
      "    accuracy                           0.98   1396343\n",
      "   macro avg       0.95      0.94      0.94   1396343\n",
      "weighted avg       0.97      0.98      0.97   1396343\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:14<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.3753669889008235\n",
      "Average F1 Validation score for class 1 : 0.9311398280666577\n",
      "Average Accuracy Validation score  : 0.875954551272024\n",
      "Average mcc Validation score  : 0.3372676134943797\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.28      0.38      9274\n",
      "           1       0.90      0.97      0.93     61047\n",
      "\n",
      "    accuracy                           0.88     70321\n",
      "   macro avg       0.73      0.62      0.65     70321\n",
      "weighted avg       0.85      0.88      0.86     70321\n",
      "\n",
      "Train Loss = 0.06316519014094726 Valid Loss = 0.7973662937879562\n",
      "Epoch 8 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1313/1313 [17:27<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Training score for class 0 : 0.9200131456806637\n",
      "Average F1 Training score for class 1 : 0.9886556731447\n",
      "Average Accuracy Training score  : 0.9801295240496067\n",
      "Average mcc Training score  : 0.9087515834130034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92    175469\n",
      "           1       0.99      0.99      0.99   1220874\n",
      "\n",
      "    accuracy                           0.98   1396343\n",
      "   macro avg       0.96      0.95      0.95   1396343\n",
      "weighted avg       0.98      0.98      0.98   1396343\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:14<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.35878483258186117\n",
      "Average F1 Validation score for class 1 : 0.9317536365281285\n",
      "Average Accuracy Validation score  : 0.8766371354218512\n",
      "Average mcc Validation score  : 0.3288943931752005\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.26      0.36      9274\n",
      "           1       0.90      0.97      0.93     61047\n",
      "\n",
      "    accuracy                           0.88     70321\n",
      "   macro avg       0.73      0.62      0.65     70321\n",
      "weighted avg       0.85      0.88      0.86     70321\n",
      "\n",
      "Train Loss = 0.05119086278925926 Valid Loss = 0.8605518705844879\n",
      "Epoch 9 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1313/1313 [17:19<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Training score for class 0 : 0.936250768426435\n",
      "Average F1 Training score for class 1 : 0.9909219282557437\n",
      "Average Accuracy Training score  : 0.9841070567904877\n",
      "Average mcc Training score  : 0.9272131968318944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.94    175469\n",
      "           1       0.99      0.99      0.99   1220874\n",
      "\n",
      "    accuracy                           0.98   1396343\n",
      "   macro avg       0.97      0.96      0.96   1396343\n",
      "weighted avg       0.98      0.98      0.98   1396343\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:14<00:00,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.3712626995645864\n",
      "Average F1 Validation score for class 1 : 0.9317053175891914\n",
      "Average Accuracy Validation score  : 0.8767935609561867\n",
      "Average mcc Validation score  : 0.3370126981431409\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.28      0.37      9274\n",
      "           1       0.90      0.97      0.93     61047\n",
      "\n",
      "    accuracy                           0.88     70321\n",
      "   macro avg       0.73      0.62      0.65     70321\n",
      "weighted avg       0.85      0.88      0.86     70321\n",
      "\n",
      "Train Loss = 0.0413221878755727 Valid Loss = 0.8877145833969117\n",
      "Epoch 10 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1313/1313 [17:30<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Training score for class 0 : 0.9493748836409985\n",
      "Average F1 Training score for class 1 : 0.9927666744013119\n",
      "Average Accuracy Training score  : 0.9873419353267786\n",
      "Average mcc Training score  : 0.9421581881985379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95    175469\n",
      "           1       0.99      0.99      0.99   1220874\n",
      "\n",
      "    accuracy                           0.99   1396343\n",
      "   macro avg       0.97      0.97      0.97   1396343\n",
      "weighted avg       0.99      0.99      0.99   1396343\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:14<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.35547103569338795\n",
      "Average F1 Validation score for class 1 : 0.9305977790029141\n",
      "Average Accuracy Validation score  : 0.8746889264942194\n",
      "Average mcc Validation score  : 0.3211012915451525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.26      0.36      9274\n",
      "           1       0.90      0.97      0.93     61047\n",
      "\n",
      "    accuracy                           0.87     70321\n",
      "   macro avg       0.72      0.61      0.64     70321\n",
      "weighted avg       0.85      0.87      0.85     70321\n",
      "\n",
      "Train Loss = 0.03282821915514614 Valid Loss = 0.9630370870828628\n",
      "terminating because of early stopping!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_accuracy = -1\n",
    "train_loss_lst = []\n",
    "val_loss_lst = []\n",
    "with open('metrics_xlmroberta__DataAugmentation_deletion.txt', 'w') as f:\n",
    "    for epoch in range(config.EPOCHS):\n",
    "            print(f'Epoch {epoch+1} of {config.EPOCHS}')\n",
    "            train_metrics = engine.train_fn(train_dataloader, model, optimizer, scheduler)\n",
    "            print(classification_report(train_metrics[5],train_metrics[6]))\n",
    "            test_metrics = engine.eval_fn(val_dataloader, model)\n",
    "            print(classification_report(test_metrics[5],test_metrics[6]))\n",
    "            print(f\"Train Loss = {train_metrics[0]} Valid Loss = {test_metrics[0]}\")\n",
    "            train_loss_lst.append(train_metrics[:-2])\n",
    "            val_loss_lst.append(test_metrics[:-2])\n",
    "            f.write(f\"Train_loss {epoch+1} : {str(train_loss_lst)}\" + '\\n')\n",
    "            f.write(f\"val_loss {epoch+1} : {str(val_loss_lst)}\" + '\\n')\n",
    "            if early_stopping.step(test_metrics[4]): #mcc score for early stopping\n",
    "                  break  # early stop criterion is met, we can stop now\n",
    "            if test_metrics[4] >  best_accuracy:\n",
    "                torch.save(model.state_dict(), './models/training_data/model_xlmrobertatokenclassificationmodel_DataAugmentationdeletion_earlystopping.bin')\n",
    "                best_accuracy = test_metrics[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_train : [0.30186314803675124, 0.21957505939234692, 0.16070287873555664, 0.12241293019093771, 0.09792534601984593, 0.0788113628709171, 0.06316519014094726, 0.05119086278925926, 0.0413221878755727, 0.03282821915514614] \n",
      " loss_test :  [0.36236758852005, 0.4146628511548042, 0.5265298784971237, 0.5541496854424477, 0.6254125117063523, 0.7241875883340836, 0.7973662937879562, 0.8605518705844879, 0.8877145833969117, 0.9630370870828628] \n",
      " f1_class0_train : [0.2615500336504836, 0.582702842835604, 0.7260718851487996, 0.799406326297758, 0.8419569096947277, 0.8738781349449651, 0.8995994868334554, 0.9200131456806637, 0.936250768426435, 0.9493748836409985] \n",
      " f1_class0_test : [0.19965325303403597, 0.2952108063855915, 0.3250078051826413, 0.3845727738540369, 0.37849370599530613, 0.35037921453780885, 0.3753669889008235, 0.35878483258186117, 0.3712626995645864, 0.35547103569338795] \n",
      " f1_class1_train : [0.9345597811447877, 0.9510571595917309, 0.9645590209155037, 0.9729199188625804, 0.978205012235997, 0.9823694297879011, 0.9858342332756799, 0.9886556731447, 0.9909219282557437, 0.9927666744013119] \n",
      " f1_class1_test : [0.9323658459474256, 0.9329658093703038, 0.9323476492216225, 0.9295721077654517, 0.9309612027081473, 0.9320557628117023, 0.9311398280666577, 0.9317536365281285, 0.9317053175891914, 0.9305977790029141] \n",
      " accuracy_score_train : [0.8797738091572056, 0.9123897208637133, 0.9372382000697537, 0.9522817817685196, 0.9616927932463585, 0.9690634750917217, 0.97517157317364, 0.9801295240496067, 0.9841070567904877, 0.9873419353267786] \n",
      " accuracy_score_test : [0.8752719671221968, 0.8775756886278636, 0.8770210890061291, 0.873608168256993, 0.8757270232220816, 0.8769784274967648, 0.875954551272024, 0.8766371354218512, 0.8767935609561867, 0.8746889264942194] \n",
      " mcc_score_train : [0.2651824495268958, 0.5492335688726299, 0.6951744789457047, 0.7740163537097449, 0.8209190438044313, 0.8565966383231011, 0.885591342589789, 0.9087515834130034, 0.9272131968318944, 0.9421581881985379] \n",
      " mcc_score_test : [0.239580020040288, 0.29710170815769477, 0.31059104690040285, 0.3374939146539046, 0.33867498018607184, 0.32494383063665994, 0.3372676134943797, 0.3288943931752005, 0.3370126981431409, 0.3211012915451525]\n"
     ]
    }
   ],
   "source": [
    "val_loss_lst = [(0.36236758852005, 0.19965325303403597, 0.9323658459474256, 0.8752719671221968, 0.239580020040288), (0.4146628511548042, 0.2952108063855915, 0.9329658093703038, 0.8775756886278636, 0.29710170815769477), (0.5265298784971237, 0.3250078051826413, 0.9323476492216225, 0.8770210890061291, 0.31059104690040285), (0.5541496854424477, 0.3845727738540369, 0.9295721077654517, 0.873608168256993, 0.3374939146539046), (0.6254125117063523, 0.37849370599530613, 0.9309612027081473, 0.8757270232220816, 0.33867498018607184), (0.7241875883340836, 0.35037921453780885, 0.9320557628117023, 0.8769784274967648, 0.32494383063665994), (0.7973662937879562, 0.3753669889008235, 0.9311398280666577, 0.875954551272024, 0.3372676134943797), (0.8605518705844879, 0.35878483258186117, 0.9317536365281285, 0.8766371354218512, 0.3288943931752005), (0.8877145833969117, 0.3712626995645864, 0.9317053175891914, 0.8767935609561867, 0.3370126981431409), (0.9630370870828628, 0.35547103569338795, 0.9305977790029141, 0.8746889264942194, 0.3211012915451525)]\n",
    "train_loss_lst = [(0.30186314803675124, 0.2615500336504836, 0.9345597811447877, 0.8797738091572056, 0.2651824495268958), (0.21957505939234692, 0.582702842835604, 0.9510571595917309, 0.9123897208637133, 0.5492335688726299), (0.16070287873555664, 0.7260718851487996, 0.9645590209155037, 0.9372382000697537, 0.6951744789457047), (0.12241293019093771, 0.799406326297758, 0.9729199188625804, 0.9522817817685196, 0.7740163537097449), (0.09792534601984593, 0.8419569096947277, 0.978205012235997, 0.9616927932463585, 0.8209190438044313), (0.0788113628709171, 0.8738781349449651, 0.9823694297879011, 0.9690634750917217, 0.8565966383231011), (0.06316519014094726, 0.8995994868334554, 0.9858342332756799, 0.97517157317364, 0.885591342589789), (0.05119086278925926, 0.9200131456806637, 0.9886556731447, 0.9801295240496067, 0.9087515834130034), (0.0413221878755727, 0.936250768426435, 0.9909219282557437, 0.9841070567904877, 0.9272131968318944), (0.03282821915514614, 0.9493748836409985, 0.9927666744013119, 0.9873419353267786, 0.9421581881985379)]\n",
    "\n",
    "\n",
    "train_metrices = np.array(train_loss_lst)\n",
    "test_metrices = np.array(val_loss_lst)\n",
    "loss_train = train_metrices[:,0]\n",
    "loss_test = test_metrices[:,0]\n",
    "f1_class0_train = train_metrices[:,1]\n",
    "f1_class0_test= test_metrices[:,1]\n",
    "f1_class1_train = train_metrices[:,2]\n",
    "f1_class1_test = test_metrices[:,2]\n",
    "accuracy_score_train = train_metrices[:,3]\n",
    "accuracy_score_test = test_metrices[:,3]\n",
    "mcc_score_train = train_metrices[:,4]\n",
    "mcc_score_test = test_metrices[:,4]\n",
    "print('loss_train :',list(loss_train),'\\n','loss_test : ',list(loss_test),'\\n','f1_class0_train :',list(f1_class0_train),'\\n','f1_class0_test :',list(f1_class0_test),'\\n','f1_class1_train :',list(f1_class1_train),'\\n','f1_class1_test :',list(f1_class1_test),'\\n','accuracy_score_train :',list(accuracy_score_train),'\\n','accuracy_score_test :',list(accuracy_score_test),'\\n','mcc_score_train :',list(mcc_score_train),'\\n',\n",
    "    'mcc_score_test :', list(mcc_score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ff4676d27c1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max mcc score achieved at : 4 epoch\n"
     ]
    }
   ],
   "source": [
    "print('Max mcc score achieved at :',np.argmax(mcc_score_test),'epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch = 9\n",
    "epochs = np.arange(1,epoch+1)\n",
    "# print(epochs)\n",
    "plt.plot(epochs,loss_train[:-1],label='train_loss')\n",
    "plt.plot(epochs,loss_test[:-1],label='validation_loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.title('XMLRobertaForTokenClassification_earlyStopping_DataAugmentation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA2f0lEQVR4nO3deXxU5fX48c/JQnaysibs+44aAQUt7lvdd60Fa7VatdpqrbUqVm2/trV1K7+27rvWahW0Vq0LAiICrmwCIQQIINkgIYSQ7fz+uE/CMCRkAplMkjnv12teM3c/c2fmOc997r3PiKpijDEmfEWEOgBjjDGhZYnAGGPCnCUCY4wJc5YIjDEmzFkiMMaYMGeJwBhjwpwlgnZOROaIyI9DHUcoiMhUEcnvbNsVkb+LyB0+w9eIyFYRKReRdPc8MAjbXS4iU1t7vcHSEb/7InKbiDwe6jhaKuwTgYgkikieiFzqMy5JRDaIyHki8rSIqIic6bfcA278dDc8XUTmN7GNOSJS6X7gRSLybxHpFdQ31oSW/rhE5C4RqXax1z9uOYDt+i5fJyK7fIYvbX4NrUtEJojI2yKyXURKRGSRiFzeFttW1atV9R4XRzTwF+BEVU1U1WL3nHsw23Df23v9tjtKVecczHrbK1cAr3Pfp3wR+afPtDZLKKr6e1XtUMkLLBGgquXAT4AHRaSbG/1HYImqvuqGVwM/rF9GRKKAC4C1LdjUdaqaCAwGEoH7Dzb2lhDPgX7e/3SFU/3jjy3cdpTv8sAG4HSfcS8cYFwHRESOAD4EPsb7PNKBa4BT2jIOpwcQCywPwbbbpZZ+V0VkGnAZcLz7fmUDHwQrvs4o7BMBgKq+C/wHeNgdOl8A/NRnljeBKSKS6oZPBr4BvjuAbW0H3gDG148TkSNFZLGIlLrnI/0WG+RqrGUiMktE0nyWnSQiC1zN9mvfQ39XE/qdiHwCVADPAUcBf3U1p7+6+R4SkY1u/Z+LyFGBvBcROcM1N2x32xrhMy1PRH4lIt8AO13ybGwdMSLyoIhsdo8HRSSmiXl/JiIrRCTLLXe/O3LbKl5zS5ybb6qrFd4kIgUissWvtv8n4BlV/YOqFqnnc1W9oInt3ioia0Vkh9v+2T7TBovIx+6zK6qvibrC7AG3/TIRWSoio920p0XkXhEZCqxyq9ouIh+66Soig93rOBH5s4isd9uY7/M+/yUi37nxc0VklBt/FXApcIv7nN/0+UyOb26/B7D/GtXMZ5IqIm+JSKGIbHOvs3yW9f+uDvSZ1kW8o7YxPuO6i0iFeJW3w4F3VXUtgKp+p6qPuvl+R+Pf+SZ/cy6W/5NGfnMi0t99Ple5/bZFRG72WfYuEXneb95pbp8UichvfOaNE5Fn3P5YKSK3SAiaQgFQVXt43WykAluAIuByn/FPA/cCjwLXuHGvABcD84Hpbtx0YH4T654D/Ni9TgfeB2a54TRgG16NJsqtdxuQ7rPsJmA0kAC8BjzvpmUCxcCpeEn9BDfczWfZDcAot+5o31h84vuBiysKuAkvwcW6aXfVb89vmaHATrfNaOAWIAfo4qbnAV8BfYA4v2Xz8GpvAHcDC4HuQDdgAXCPmzYVyHev7wS+8HlvDwCz3f5LwkvW/+ezXI1bd7TbPxXuM44HaoFj9vNdaNiuGz4f6O328YXuffdy014CfuOmxQJT3PiTgM+BFECAET7LPA3c6173BxSI8tmeAoPd65nuM8sEIoEjgRg37UfuvccADwJf+X9vD2K/N7r/mvkN7e8zSQfOdfs/CfgX8Ibfb6TJ7yrw/4A/+Mx/A/Cmz/e3BPgl3tFAZFO/v1b4zdV/Xi+5aWOAQp/9elcj8z4GxAHjgN3ACDf9Pryj0lQgC69ymb+/fRy08i8UG22vD7wCugJI9v9BAVOAT/F+2FvdB9uSRFABlLovxldAXzftMmCR3/yf+qx3DnCfz7SRQBVeofAr4Dm/Zd8Fpvkse/f+fhRNxLsNGKd7vthVwHafR2/gDuAVn2Ui3I9nqhvOA37UxPrzfH44a4FTfaadBOS511PdOv/i9nWyGy94hfEgn+WOANb5LLeLvQvXAmASXoGqwPD9vP+p+/tBus/vTPf6WbxKQpbfPMfiNSlOAiL8pj1NAInA7dNd9Z9FM59Zilsu2X8bB7jfG91/+9n+fj+TRuYfD2zz+142+V0FJuIlCnHDS4ALfOa9FO/3uxOvMvSrpr7zHNxvrv7zGu4z/Y/AEz6/F/9EkOUz7yLgIvc6FzjJZ9qPCVEisKYhR0R+gPfBvQ/8wX+6qs7Hqzn9BnhLVXe1cBM/U9VkYCx7agDgFarr/eZdj1dg1dvoNy0ayAD6AeeL1zSzXUS24yWsXk0s2ygRudkdmpa6dSS79dd7RVVTfB6b/eNW1Tq3rabibor/+1/vxtVLAa7Cq1mWunHd8GqWn/u873fc+HrFqlrjM1yBd25mG1DH3vtov0TkhyLylc+2RrNn/9yCVwguEq+Z7EcAqvoh8Fe8Gn2BiDwqIl0D3aaTgXeUsc+5KBGJFJH7xGuyKsMr5OuXCURz+72p/deU/X4mIhIvIv9wTVxlwFwgRUQifdbR5PdFVT9zMUwVkeF4iXK2z/QXVPV4vO/L1cA9InJSE6s7mN9cU9N9950/3yZk3/3Y2289gfxegsISAV57I95h7ZV4J44vkMbbyZ/Hazp59kC3papL8Y4wZoqIAJvxCnRfffFqwvX6+E2rxmvC2oh3ROBbSCeo6n2+m/QPwXfAvc9b8M6LpKpqCt6RizTzVvaK272XPn5x+2+72fXgvb/NPsPbgO8DT4nIZDeuCK/GOsrnfSerd6Jwv1S1Aq/2d24AsSEi/fAO7a/DazpIAZbh9o967dFXqmpvvO/O/xPXvq+qD6vqYXg1yqF4TRctUQRUAoMamXYJcCZwPF7i7l8fsntubt83t99bqrnP5CZgGDBRVbsCR/vFG0jMz+A1A10GvKqqlf4zqGq1qv4Lr5lldBPrPZjfXFPTD2TfbWFPhdB/nW3KEoHnr3jtlR+p6ha8gvEx2fek5cN4beJzm1iPiEis76OJ+Z7Bu1rkDOBtYKiIXCIiUSJyIV7B8ZbP/D8QkZEiEo/XbvuqqtbiJabTReQkV0OMdSf6smjaVnxOxOG119bgtXNGicidQCA111eA00TkOPEugbwJr/1zQQDL+noJuF1EuolIBt65gOd9Z1DvksdLgX+LyAR39PEY8IBL4ohI5n5qgP5uAaaLyC9FJN0tP05EXm5k3gS8gqTQzXc5ewoYROR8n/29zc1bJyKHi8hEt2924hXodQHGV/++64Angb+ISG/3GR/hvpdJePu7GK8m/nu/xf0/Z3/N7vcDiHV/n0kSXqLY7k68zjiAzTwPnI2XDBoqY+Jdun2aeJd9R4jIKXjnGj5zs/jvi4P5zdW7wx3ljAIuB/5Jy70C/Fq8E+mZeJWNkAj7RCAiZ+E1pzTU1lT1cbwMf6fvvKpaoqofqGvQa8SReF/2hoc0crWMqlYBDwF3qGoxXo33Jrwf9S3A91XVt/bxHF6b73d4TQU/c+vZiFcrvA2voNro3sf+PteHgPPclQoP451TeAevPXs9XoHV7CGqqq7C+0E+gldTOh3vktCq5pb1cy9ee+83wFK8E8L3+s+kqv/DOzn6pogcind+JAdY6Joa3sercTZLVRfgteEfC+SKSAleO//bjcy7Avgz3lHEVryTg5/4zHI48JmIlOM1Vdyg3j0AXfEKxm14+7UY72qllroZb78sxjsh+ge8z/dZt95NwAq8E7++ngBGumaaNxpZb0D7vYX295k8iHdercjF+k5LV+6+71/gJdt5PpPK8H4DG/DOYf0R78KO+vt69vrOH8xvzsfH7r1+ANyvqu+19P3gJZh8YB3evnoVL7m3OWm6TDPGmPZFRJ4ENqvq7UHcxhy8E7773CEsIv3xCu5ov3MorbHda/BOJH+vNdcbiEav7TbGmPbGFcLnAIeEOJRWIV7vAgPxjjaH4B2h/DUUsQStaUhEnhTvZpRlTUwXEXlYRHJE5Bt3uG+MacfclVHljTyC2k2IiNyDd5L+T6q6LpjbakNdgH8AO/DudJ+Fd79Emwta05CIHA2UA8+q6uhGpp8KXI93s8pE4CFVnRiUYIwxxjQpaEcEqjoX7+RWU87ESxKqqgvxrikOSUdsxhgTzkJ5jiCTva9OyXfjtvjPKF7fKVcBJCQkHDZ8+PA2CdAYYzqLzz//vEhVuzU2rUOcLFavA6lHAbKzs3XJkiUhjsgYYzoWEfG/m7pBKO8j2MTed9JlsfedfcYYY9pAKBPBbOCH7uqhSUCpu6vXGGNMGwpa05CIvITXi2GGeH1sz8DruAlV/TveXZyn4t2dV4F3m7Yxxpg2FrREoKoXNzNdgWtbY1vV1dXk5+dTWblPH1TmAMTGxpKVlUV0dHSoQzHGtIEOcbK4Ofn5+SQlJdG/f3+8TjDNgVJViouLyc/PZ8CAAaEOxxjTBjpFp3OVlZWkp6dbEmgFIkJ6erodXRkTRjpFIgAsCbQi25fGhJdOkwiMMcYcGEsExhgT5iwRdCBz5sxhwYKW/gEYLFmyhJ/9zP9/NYwxxtMprhoKF3PmzCExMZEjjzxyn2k1NTVERTX+cWZnZ5OdnR3s8IwxHVSnSwS/fXM5KzaXteo6R/buyozTR+13nry8PE4++WQmTZrEggULOPzww7n88suZMWMGBQUFvPDCC4wcOZLrr7+eJUuWICLMmDGDc889l3feeYfbbruN2tpaMjIy+OCDDxpd/9///nciIyN5/vnneeSRR3jiiSeIjY3lyy+/ZPLkyVx00UXccMMNVFZWEhcXx1NPPcWwYcOYM2cO999/P2+99RZ33XUXGzZsIDc3lw0bNnDjjTfa0YIxYa7TJYJQysnJ4V//+hdPPvkkhx9+OC+++CLz589n9uzZ/P73v2fYsGEkJyezdOlSALZt20ZhYSFXXnklc+fOZcCAAZSUNN5zd//+/bn66qtJTEzk5ptvBuCJJ54gPz+fBQsWEBkZSVlZGfPmzSMqKor333+f2267jddee22fdX377bd89NFH7Nixg2HDhnHNNdfYzWPGhLFOlwiaq7kH04ABAxgzZgwAo0aN4rjjjkNEGDNmDHl5eWzcuJGXX365Yf7U1FTefPNNjj766Iabt9LS0lq0zfPPP5/IyEgASktLmTZtGmvWrEFEqK6ubnSZ0047jZiYGGJiYujevTtbt24lKyvrQN6yMSYIKqtrKSjbzXdllXxXVsnWUu/51DG9OKxfaqtvr9MlglCKiYlpeB0REdEwHBERQU1NTUOB3ZoSEhIaXt9xxx0cc8wxvP766+Tl5TF16tRm44yMjKSmplX/g9sY0wRVZVtFNd+VVrLVFfKNvd5WsW8lLi46kqE9Ei0RdHQnnHACM2fO5MEHHwS8pqFJkybx05/+lHXr1jU0DTV1VJCUlERZWdPnP0pLS8nMzATg6aefbu3wjTH7sbvGpxZfX7i7mnx9Qb+1bDdVNXV7LScC6Qkx9EyOISs1jsP6pdKzayw9kmPp2TWWnsmx9OgaS9fYqKDd7GmJoA3dfvvtXHvttYwePZrIyEhmzJjBOeecw6OPPso555xDXV0d3bt353//+1+jy59++umcd955zJo1i0ceeWSf6bfccgvTpk3j3nvv5bTTTgv22zEmLKgq2yuq92mm2VPQ72ZrWSUlO6v2WTY2OsIr1LvGcmjf1IbX9YV7z+RYuifFEB0Z2iv5g/bn9cHS2D+UrVy5khEjRoQoos7J9qkJF1U1dWzavosNJRVsKKkgv6SCLX6F/W6/WjxARmIXrzD3rb37ve4aF7xafEuJyOeq2uh15HZEYIzp1FSV4p1VbCipYGNJBRuKK/YU+tt2sbl0F7714S5REfRyNfZxWSmcNCq2ocDvmRxDj66xdE+KpUtU57kf1xJBO/TUU0/x0EMP7TVu8uTJzJw5M0QRGdO+VVbXkr/NFfDFFWzctmtPwV9SQUVV7V7zd0+KoW9aPBMHpNEnLZ6+afH0TfeeuyXGEBHRPmrxbcUSQTt0+eWXc/nl9odtxtSrq1MKy3fvVbj7vt5atnuv+eOiI+mbFk+ftHiOHJRB37S4hoI+KzWe2OjWv4KvI7NEYIxpF3burmHjtgo2luxqtMD3bacXgV5dY+mTFs/RQ7o11Oj7pMXTJzWejMQu7aZtviOwRGCMaTOV1bXkFu5kTcEO1haUs96noC8q3/uqm8SYKPqmxTO4WyLHDOvWUMPvmxZPZmocMVFWq28tlgiMMa2usrqWtYXlrNlazpqCHazeWk5OQTnri3dS507MRgj0Tomjb1o8x4/osaet3j1S4qOtVt9GLBF0IHPmzKFLly6N9j7anLy8PBYsWMAll1wShMhMuNpV5Qp8V9iv2VpOTsEONpRUNBT4URFC/4wEhvdM4vRxvRnSPZGhPZLonxFvtfp2whJBB7K/bqibk5eXx4svvmiJwByQiqoa1hbsZPXWHawpKGeNe964raLh0suoCGFgtwRG9U7mzPGZDO2RxJAeifRPT+hUl1p2RpYIWkkouqEePnw4V199NRs2bADgwQcfZPLkyXz88cfccMMNgPf/w3PnzuXWW29l5cqVjB8/nmnTpvHzn/+8TfeP6Rh27q4hp6B8r8J+TcEO8rftudY+OlIYmJHI2Kxkzj00iyE9EhnaI5F+6Qkhv0PWHJjOlwj+eyt8t7R119lzDJxyX7OztXU31Jdccgk///nPmTJlChs2bOCkk05i5cqV3H///cycOZPJkydTXl5ObGws9913X8N/EhhT7gr81Vt3eAX/Vq9pZ9P2XQ3zdImMYGC3BMb3SeWCw/owpEcig7sn0T89nigr8DuVzpcIQqitu6F+//33WbFiRcNwWVkZ5eXlTJ48mV/84hdceumlnHPOOdbFdBirrK5l5ZYy1mwtb2jWySnwK/CjIhjULZHs/qlc3L0Pg7snMbRHIn3TrMAPF50vEQRQcw+Wtu6Guq6ujoULFxIbG7vX+FtvvZXTTjuNt99+m8mTJ/Puu++26nZN+7S7ppZV3+3gm/xSluaX8s2mUlZv3UGtO2sbExXB4O6JHN4/lUt69G04adsnLZ7IMLuT1uyt8yWCdqy1u6E+8cQTeeSRR/jlL38JwFdffcX48eNZu3YtY8aMYcyYMSxevJhvv/2WPn36sGPHjqC/R9M2amrrWL21nKWbtnsF/6ZSvt2yg6pa76ar1PhoxmalcPyI7ozOTGZ4zySyUq3AN42zRNCGWrsb6ocffphrr72WsWPHUlNTw9FHH83f//53HnzwQT766CMiIiIYNWoUp5xyChEREURGRjJu3DimT59uJ4s7kNo6JbewvKHA/yZ/O8s3lzXcaZsUG8XYrGR+NGUAY7OSGZOZTFZqnF2DbwJm3VCbRtk+DQ1VJa+4gm/ytzc07yzfVMpO12lafJdIRmcmMzYzmTFZyYzNSqFfWnzYdZJmWs66oTamHVJV8rftcrX80oZmnh2V3l+HxkRFMKp3V87P7sOYzGTGZiUzsFuiNe+YVmeJoB2ybqg7p+9KK72afkPBX9rwr1bRkcLwnl05Y1xv17yTwpAeiXZdvmkTlgjaIeuGuuMrKt/tNe341PQLdnhdJUdGCEO6J3LCiB6ueSeZYT2TrLsFEzKdJhGoqp0cayUd7bxRqNXU1vHNplIW5hbzzUavpl9/nb4IDOqWyJTBGV5NPyuFkb26EtfFCn3TfnSKRBAbG0txcTHp6emWDA6SqlJcXLzPvQlmj/oTuvPXFDJvTRGf5hY3tOv3T4/n0H6pTD+yP2OzkhmVmUxiTKf4mZlOrFN8Q7OyssjPz6ewsDDUoXQKsbGxdjeyn+Ly3SxYW8z8NUXMzylqqPFnpsRx2pheTB6cwZGD0klPjGlmTca0P0FNBCJyMvAQEAk8rqr3+U3vCzwDpLh5blXVt1u6nejo6IYuGoxpDZXVtSzOK2ko+Jdv9m7kS4qN4shB6Vw9dRBHDc6gX3q8HYWaDi9oiUBEIoGZwAlAPrBYRGar6gqf2W4HXlHVv4nISOBtoH+wYjKmKXV1yvLNZczPKWJ+TiGL87ZRVVNHdKRwaN9Ubj5xKJMHZzAmM9n63zGdTjCPCCYAOaqaCyAiLwNnAr6JQIGu7nUysDmI8Rizl40lFa7gL2JBThHbKqoBGNYjicsm9WPKkAwm9E8jwdr4TScXzG94JrDRZzgfmOg3z13AeyJyPZAAHN/YikTkKuAqgL59+7Z6oCY8lFZU82luEfNcc8/64goAenSN4djhPZgyJJ3JgzPonmQnyk14CXVV52LgaVX9s4gcATwnIqNVtc53JlV9FHgUvC4mQhCn6YB219TyxfrtfJJTxLycIpbmb6dOIaFLJJMGpjP9yP5MGZzB4O6J1s5vwlowE8EmoI/PcJYb5+sK4GQAVf1URGKBDKAgiHGZTkpVWbV1B/PXeLX+RetK2FVdS2SEML5PCtcdO4SjhmQwvk+K3bFrjI9gJoLFwBARGYCXAC4C/P8wdwNwHPC0iIwAYgG7BtQE7LvSSuatKeSTnCLm5xRTVO7dvTuwWwIXZGcxeXAGkwal0zU2OsSRGtN+BS0RqGqNiFwHvIt3aeiTqrpcRO4GlqjqbOAm4DER+TneiePpare1mv2oqqljnruRa35OETkF5QBkJHZh8uAMJg/OYMrgDHqnxIU4UmM6jk7RDbXp/Ep3VfPSog08/Uke35VVEhsdwYQB6RzlCv/hPZOsK2Zj9sO6oTYd1saSCp76JI9/Lt7AzqpajhyUzr1njWbKkAxio62/HmNagyUC0y59vXE7j83L5e2lW4gQ4ftje/HjowYyOjM51KEZ0+lYIjDtRl2d8sG3BTw2N5dFeSUkxURx5VEDmXZkf2vzNyaILBGYkNtVVctrX+Tz5Px15BbtJDMljttPG8GFh/chya72MSboLBGYkCkq382zn67nuU/z2FZRzdisZB6++BBOHd3T+vMxpg1ZIjBtLqegnCfm5/LaF5uoqqnj+BHdufKogUwYkGZ3+BoTApYITJtQVRbmlvDYvFw+/LaAmKgIzj00iyumDGBw98RQh2dMWLNEYIKquraOt5du4bF5uSzbVEZ6QhduPH4Il03qZ3/iYkw7YYnABMWOympeXrSRpz5Zx+bSSgZ2S+D3Z4/hnEMz7fp/Y9oZSwSmVW3evounPlnHy4s2smN3DRMHpHHPWaM5Zlh3u/PXmHbKEoFpFcs2lfLYvFze+mYLAKeO6cWVRw1gbFZKaAMzxjTLEoE5YHV1ypzVBTw6N5eFuSUkxkRx+ZH9mT65P1mp8aEOzxgTIEsEpsUqq2t5/ctNPD4vl7WFO+mVHMttpw7nogl9rbtnYzogSwQmYCU7q3ju0/U8tzCPovIqRvXuykMXjefUMb3sj16M6cAsEZhm5RaW88T8dbz6eT67a+o4Zlg3rjx6IEcMTLcbwIzpBCwRmCYtWufdAPb+yq1ER0RwzqGZXDFlAEN6JIU6NGNMK7JEYPZRVVPHjNnLeGnRRlLjo7n+mMFcdkR/uiXZDWDGdEaWCMxetu2s4urnP+ezdSVcM3UQPzt2CHFd7AYwYzozSwSmQU7BDq54ZglbSit58MLxnHVIZqhDMsa0AUsEBoA5qwq4/sUviYmO5OWrJnFo39RQh2SMaSOWCMKcqvLUJ3nc+58VDOvZlcenZZNp/wZmTFixRBDGqmvruHPWcl5atIETR/bggQvHkxBjXwljwo396sPUtp1VXPPC5yzMLeHaYwZx0wnDrFM4Y8KUJYIw5HtS+IELx3H2IVmhDskYE0KWCMLMx6sLue6FL4iJjuClKydxWD87KWxMuLNEECZUlWcW5HH3WysY2iOJx6dlWw+hxhjAEkFYqK6tY8bs5bz42QZOGNmDB+2ksDHGh5UGndz2iip++sIXLFhbzDVTB/HLE+2ksDFmb5YIOrGcgnJ+/MxiNm+v5C8XjOOcQ+2ksDFmX5YIOqm5qwu59sUviImK4KWrJnJYv7RQh2SMaacsEXQyqsqzn67n7rdWMKR7op0UNsY0yxJBJ1JdW8dds5fzwmcbOH5EDx66yE4KG2OaZ6VEJ+F7Uvjq7w3ilpPspLAxJjCWCDqBtYXlXPG0d1L4z+eP49zD7KSwMSZwQf3HcRE5WURWiUiOiNzaxDwXiMgKEVkuIi8GM57OaN6aQs6a+Qk7Kmt48cqJlgSMMS0WtCMCEYkEZgInAPnAYhGZraorfOYZAvwamKyq20Ske7Di6Yye/TSP377pnRR+7IfZ9Emzk8LGmJZr9ohARHqIyBMi8l83PFJErghg3ROAHFXNVdUq4GXgTL95rgRmquo2AFUtaFn44am6to7b31jKnbOWc8ywbrx6zZGWBIwxByyQpqGngXeB3m54NXBjAMtlAht9hvPdOF9DgaEi8omILBSRkxtbkYhcJSJLRGRJYWFhAJvuvLZXVDH9qUU8v3ADP/neQP5xWTaJdmWQMeYgBJIIMlT1FaAOQFVrgNpW2n4UMASYClwMPCYiKf4zqeqjqpqtqtndunVrpU13PLmF5Zz9/xawaF0JfzpvLL8+ZQSRdmWQMeYgBVKV3Cki6YACiMgkoDSA5TYBfXyGs9w4X/nAZ6paDawTkdV4iWFxAOsPK/PXFPHTFz4nKjKCF6+cxOH97U5hY0zrCOSI4BfAbGCQiHwCPAtcH8Byi4EhIjJARLoAF7n1+HoD72gAEcnAayrKDSjyMPLcp3lMe2oRvZLjmHXtZEsCxphWtd8jAnflz/fcYxggwCpXg98vVa0Rkevwzi9EAk+q6nIRuRtYoqqz3bQTRWQFXnPTL1W1+KDeUSdSU1vH3W+t4NlP13Pc8O48dPEhdj7AGNPqRFX3P4PIIlWd0EbxNCs7O1uXLFkS6jCCrrSimmtf/IL5OUX85OiB3HLycDsfYIw5YCLyuapmNzYtkOrlJyLyV+CfwM76kar6RSvFZ/zkFpbz42eWsHFbBX86byznZ/dpfiFjjDlAgSSC8e75bp9xChzb6tEYPskp4prn7aSwMabtNJsIVPWYtgjEwHML13PX7OUM6pbAE9MOt5vEjDFtotlEICLJwAzgaDfqY+BuVQ3kElITAN+TwscO785DF40nKTY61GEZY8JEIJePPgnsAC5wjzLgqWAGFU7KKqu5/OnFPPvpeq48agCP/TDbkoAxpk0Fco5gkKqe6zP8WxH5KkjxhJ3fzl7Bp2uL+eO5Y7ngcDspbIxpe4EcEewSkSn1AyIyGdgVvJDCx8LcYl77Ip+rjh5oScAYEzKBHBFcAzzjzhUAbAOmBy2iMFFVU8ftbywjKzWO648dEupwjDFhLJCrhr4CxolIVzdcFuygwsFj83LJKSjnyenZxHWJDHU4xpgwFsj/EfxeRFJUtUxVy0QkVUTubYvgOquNJRU88uEaTh7Vk2OH9wh1OMaYMBfIOYJTVHV7/YD7E5lTgxZRJ6eq3DlrGREi3Hn6yFCHY4wxASWCSBGJqR8QkTggZj/zm/14d/l3fLSqkF+cMJTeKXGhDscYYwI6WfwC8IGI1N87cDnwTPBC6rzKd9fw2zdXMKJXV6Yf2T/U4RhjDBDYyeI/iMjXwPFu1D2q+m5ww+qcHvzfaraUVvLXSw4lKjKQgzFjjAm+QLqYSADeU9V3RGQYMExEogP5TwKzx4rNZTy1II+LJ/TlsH6poQ7HGGMaBFItnQvEikgm8A5wGd4f2psA1dUpv3ljKSlx0fzq5GGhDscYY/YSSCIQVa0AzgH+pqrnA6OCG1bn8vLijXy5YTu3nTqClPguoQ7HGGP2ElAiEJEjgEuB/7hxdgdUgIrKd3Pff1cycUAa5xyaGepwjDFmH4EkghuAXwOvu/8cHgh8FNywOo/f/2clu6pr+d3ZoxGxv5o0xrQ/gVw1NBfvPEH9cC7ws/phEXlEVa8PTngd24K1Rfz7y01ce8wgBndPCnU4xhjTqNa4hnFyK6yj09ldU8vtbyyjT1oc1x1jncoZY9qvQG4oMwfgsbm55Bbu5Knph1uncsaYds3uagqCDcUVPPJhDqeM7skxw7uHOhxjjNmv1kgEdgbUh6pyx6xlREVYp3LGmI6hNRLBQ62wjk7jv8u+4+PVhfzixGH0SrZO5Ywx7V8g/0fwPxFJ8RlOFZGGvoZU9enghNbxeJ3KLWdkr65MO6JfqMMxxpiABHJEkNHI/xFYw3cj/vLeagp27OZ3Z4+2TuWMMR1GIKVVnYj0rR8QkX6ABi+kjmnZplKeXrCOSyb05ZC+1qmcMabjCOTy0d8A80XkY7wTw0cBVwU1qg6mtk75zRvLSEvowi0nDQ91OMYY0yKB3Fn8jogcCkxyo25U1aLghtWxvLRoA19v3M4DF44jOT461OEYY0yLBHKy+GygWlXfUtW3gBoROSvokXUQhTt284d3vuWIgemcNd46lTPGdDyBnCOYoaql9QPuxPGMoEXUwfz+7ZXsrq7jXutUzhjTQQWSCBqbx7qmABbkFPH6l5v4yfcGMqhbYqjDMcaYAxJIIlgiIn8RkUHu8QDwebADa+/qO5XrmxbPtccMDnU4xhhzwAJJBNcDVcDL7rEL+Gkwg+oIHv04l9yindx95ihio61TOWNMxxVIIhgBDMNrDooFTgcWBrJyETlZRFaJSI6I3Lqf+c4VERWR7EDWG2p5RTt55KMcThvTi6nD7N46Y0zHFkhb/wvAzcAyoC7QFYtIJDATOAHIBxaLyGxVXeE3XxLev6B9Fui6Q6m+U7kukRHc8X3rVM4Y0/EFckRQqKpvquo6VV1f/whguQlAjqrmqmp909KZjcx3D/AHoDLwsEPnP0u3MG9NETedOJSeybGhDscYYw5aIEcEM0TkceADYHf9SFX9dzPLZQIbfYbzgYm+M7gb1fqo6n9E5JdNrUhErsLdzdy3b9+mZgu6HZXV3P3mCkb17splk6xTOWNM5xBIIrgcGA5Es6dpSIHmEsF+iUgE8BdgenPzquqjwKMA2dnZIevn6M/vraawfDeP/jDbOpUzxnQagSSCw1V12AGsexPQx2c4y42rlwSMBua4G7F6ArNF5AxVXXIA2wuqZZtKefbTPH4wsR/j+6SEOhxjjGk1gVRrF4jIgZwVXQwMEZEBItIFuAiYXT9RVUtVNUNV+6tqf7wrkdplEqitU37z+lLSEmK4+aQDyYnGGNN+BXJEMAn4SkTW4Z0jEEBVdez+FlLVGhG5DngXiASeVNXlInI3sERVZ+9v+fbkxc/W83V+KQ9dNJ7kOOtUzhjTuQSSCE4+0JWr6tvA237j7mxi3qkHup1gKthRyR/fWcXkwemcMa53qMMxxphWF0g31IFcKtpp/e4/K9ldU8c9Z1qncsaYzskufdmP+WuKmPXVZq6eOoiB1qmcMaaTskTQhMrqWu6YtYx+6fH8dOqgUIdjjDFBY91JN+EfH+eyrmgnz/5ognUqZ4zp1OyIoBHrinYyc04O3x/bi6OHdgt1OMYYE1SWCPyoKnfOWkaMdSpnjAkT1jTk561vvE7lfnvGKHp0tU7l2pXdO6BoDWzfAHU1oApa5z3wee07Xl2vKE1O8x2mifF+y+21Pp9pIhCXCvHp7pHm8zodouO9ecJVXS1UlsKubd6jshSiYqBLonskuEciRIR5HVUVqiugaufej9R+kNSz1TdnicBHWWU1d7+1gjGZyfzAOpULjbo6KN3oFfjFa7znotVQnAM7trRtLBLhPZA9r8Xvte80dQVdfbLwFxXbeIJo8pHmFZTtTU0VVG7fU6AH+qgsbXbVDaLjfRJD0p7XMY0kDd/hmKTGp0XHBy+51FZDVblPgV0OVRU+r3f6TXOvq5sYX/+gkW7VTvsLHH5Fq78FSwQ+/vzuKorKd/PEtGwiI8K45tYW6mv3exX4a6BkLdT49EgemwIZQ2DQsZA+2Hud2h8iY3wKZgmg0HaFQJPT/Jc7wM+/rs4rJCtKoKK4kYfP+O0boaJo/wVkl6QmEoffuIQM7zkuFSICvLihurKJQrukifHbveeq8qbXKRHeZxaX6o6OMiB9yJ5h30dsV6jZ7VMQ+hSGu3f4FYw7vP1atslnerl3ZBio6PpE0lgSSdyTaKLjvGS3TwFd7ldLd9NqqwKPIbLL3turT1Jds/ziSmj80W1E4NtqAUsEzjf523l24Xp+OKkfY7NSQh1O51BX62r3Oa5W71Pgl3+3Zz6J9Ar3jCEw6BjIGOq9Th/iFXAdqTklIsIV0mlAgP9lXVvtFbDNJY6KIiha5Y1rsjAWiEvZO0nEdPXmry/I6x81u/bzPqIgLm1Pod01C3qM8SnIUxov3GO6tm2zTk1VM0mkfN/CfHf5ntcVxbB9/d7TtBYQVyDH711Ax6VCctbeRxr7FNz+ycatIzoBorq03b5pAUsE1Hcqt4yMxBhusk7lWq6yzBXyOe55tfe6qdr94OP21O4zhkLqgHb7A2kTkdGQ2N17BKp6VyNHHY0chWzf6B1xxLhCLG0AxB3SeCHe8EjzCq6OkICjukBUfeJtBapeYo6M7hjvv5VYIgCeX7iepZtKefjiQ+gaa53KNaqhdt9Ic04gtfuMoV7tNIx+XEEVHQfJmd7DtB6RsKyUhH0iKCir5P53V3HUkAxOH9sr1OGERvWuRmqVJV4BX5zjCv61ULt7zzKxKV7h3lC7dwV+uNfujemAwj4R3POfleyurePuztKpXEOh7tNMsE/7c8nez021FfvW7gcf57XZW+3emE4nrBPB3NWFvPn1Zm48fggDMhJCHc6+/NuBd5XsW4j7F/bVFU2vLzZ5zwnErr2hx2ifq0/8rkKpP1EYGdZfEWPCQtj+yiura7lz1jIGZCRw9fdC1KlcVQXkvA/rP4GdRa6g9ynkAynU49IgqZdPoZ62d2He8NoKdWNM48K2ZPjbnLXkFVfw/BUT27ZTuaqdsOY9WDELVr/n3VQSneBdMRKfDok9ofvIvWvpcf61dSvUjTGtJyxLk9zCcv42Zy1njOvNlCEZwd/g7h2w+l1Y8Qased9rk0/oBuMuhJFnQr8pVrAbY0Im7EofVeWOWcuIiY7g9u8H5y49wLt2e9U7Xs0/533vipvEnnDoZV7h3/eIwO8ANcaYIAq7RDD76818klPMPWeOontSK3cqV1ECq/7rFf5rP4S6akjqDdk/8gr/PhOtMy1jTLsTVomgdFc197y1krFZyVwysZU6ldtZDKv+A8vfgHUfe32fJPeFiT+BkWdB5mFW+Btj2rWwSgT3v7uKkp27eWr64QfXqVx5IXz7plfzXzfP65sktT8cca1X8+99qF1jb4zpMMImEXy9cTvPf7aeaUf0Z0xWcstXsOM7WOkK//WfeF0Npw2CKTd6hX/PsVb4G2M6pLBJBN9sKqV3chw3nTg08IVKN+0p/Dd8Cqh3V+1RN3uFf49RVvgbYzq8sEkEl03qx/mHZTV/z8D2jbBytlf4b/zMG9d9JEz9tVf4dx8e/GCNMaYNhU0iAJpOAiXr9hT+mz73xvUcA8feDiPOhG4tOIowxpgOJqwSwV6K13oF/4o3YMvX3rhe4+H4u2DEGZAeom4njDGmjYVXIihc7Qr/WbB1qTcuMxtOuAdGnuFd+WOMMWEmfBLB3D/Bh/d6r/tMgpP+D0acDil9QhuXMcaEWPgkgiEnef+nOuJ0rwtmY4wxQDglgl5jvYcxxpi9WN8HxhgT5iwRGGNMmLNEYIwxYS6oiUBEThaRVSKSIyK3NjL9FyKyQkS+EZEPRKSVugQ1xhgTqKAlAhGJBGYCpwAjgYtFZKTfbF8C2ao6FngV+GOw4jHGGNO4YB4RTAByVDVXVauAl4EzfWdQ1Y9Utf4f2hcCWUGMxxhjTCOCmQgygY0+w/luXFOuAP7b2AQRuUpElojIksLCwlYM0RhjTLs4WSwiPwCygT81Nl1VH1XVbFXN7tatW9sGZ4wxnVwwbyjbBPj235Dlxu1FRI4HfgN8T1V3BzEeY4wxjQjmEcFiYIiIDBCRLsBFwGzfGUTkEOAfwBmqWhDEWIwxxjQhaIlAVWuA64B3gZXAK6q6XETuFpEz3Gx/AhKBf4nIVyIyu4nVGWOMCZKg9jWkqm8Db/uNu9Pn9fHB3L4xxpjmtYuTxcYYY0LHEoExxoQ5SwTGGBPmLBEYY0yYs0RgjDFhzhKBMcaEOUsExhgT5iwRGGNMmLNEYIwxYc4SgTHGhDlLBMYYE+YsERhjTJizRGCMMWHOEoExxoQ5SwTGGBPmLBEYY0yYs0RgjDFhzhKBMcaEOUsExhgT5iwRGGNMmLNEYIwxYc4SgTHGhDlLBMYYE+YsERhjTJizRGCMMWHOEoExxoQ5SwTGGBPmLBEYY0yYs0RgjDFhzhKBMcaEOUsExhgT5iwRGGNMmLNEYIwxYc4SgTHGhDlLBMYYE+aCmghE5GQRWSUiOSJyayPTY0Tkn276ZyLSP5jxGGOM2VfQEoGIRAIzgVOAkcDFIjLSb7YrgG2qOhh4APhDsOIxxhjTuGAeEUwAclQ1V1WrgJeBM/3mORN4xr1+FThORCSIMRljjPETFcR1ZwIbfYbzgYlNzaOqNSJSCqQDRb4zichVwFVusFxEVh1gTBn+624nLK6Wsbharr3GZnG1zMHE1a+pCcFMBK1GVR8FHj3Y9YjIElXNboWQWpXF1TIWV8u119gsrpYJVlzBbBraBPTxGc5y4xqdR0SigGSgOIgxGWOM8RPMRLAYGCIiA0SkC3ARMNtvntnANPf6POBDVdUgxmSMMcZP0JqGXJv/dcC7QCTwpKouF5G7gSWqOht4AnhORHKAErxkEUwH3bwUJBZXy1hcLddeY7O4WiYocYlVwI0xJrzZncXGGBPmLBEYY0yYC4tEICJPikiBiCwLdSy+RKSPiHwkIitEZLmI3BDqmABEJFZEFonI1y6u34Y6Jl8iEikiX4rIW6GOpZ6I5InIUhH5SkSWhDqeeiKSIiKvisi3IrJSRI5oBzENc/up/lEmIjeGOi4AEfm5+84vE5GXRCQ21DEBiMgNLqblwdhXYXGOQESOBsqBZ1V1dKjjqScivYBeqvqFiCQBnwNnqeqKEMclQIKqlotINDAfuEFVF4Yyrnoi8gsgG+iqqt8PdTzgJQIgW1Xb1U1IIvIMME9VH3dX78Wr6vYQh9XAdUWzCZioqutDHEsm3nd9pKruEpFXgLdV9ekQxzUar2eGCUAV8A5wtarmtNY2wuKIQFXn4l2V1K6o6hZV/cK93gGsxLvbOqTUU+4Go92jXdQYRCQLOA14PNSxtHcikgwcjXd1Hqpa1Z6SgHMcsDbUScBHFBDn7muKBzaHOB6AEcBnqlqhqjXAx8A5rbmBsEgEHYHrefUQ4LMQhwI0NL98BRQA/1PVdhEX8CBwC1AX4jj8KfCeiHzuukRpDwYAhcBTrintcRFJCHVQfi4CXgp1EACqugm4H9gAbAFKVfW90EYFwDLgKBFJF5F44FT2vln3oFkiaAdEJBF4DbhRVctCHQ+Aqtaq6ni8O8InuMPTkBKR7wMFqvp5qGNpxBRVPRSvt91rXXNkqEUBhwJ/U9VDgJ3APt3Bh4prqjoD+FeoYwEQkVS8jjAHAL2BBBH5QWijAlVdidcz83t4zUJfAbWtuQ1LBCHm2uBfA15Q1X+HOh5/rinhI+DkEIcCMBk4w7XHvwwcKyLPhzYkj6tNoqoFwOt47bmhlg/k+xzNvYqXGNqLU4AvVHVrqANxjgfWqWqhqlYD/waODHFMAKjqE6p6mKoeDWwDVrfm+i0RhJA7KfsEsFJV/xLqeOqJSDcRSXGv44ATgG9DGhSgqr9W1SxV7Y/XpPChqoa8xiYiCe5kP67p5US8w/mQUtXvgI0iMsyNOg4I6YUIfi6mnTQLORuASSIS736bx+Gdtws5EenunvvinR94sTXX3yF6Hz1YIvISMBXIEJF8YIaqPhHaqACvhnsZsNS1xwPcpqpvhy4kAHoBz7grOiKAV1S13Vyq2Q71AF53f6URBbyoqu+ENqQG1wMvuGaYXODyEMcDNCTME4CfhDqWeqr6mYi8CnwB1ABf0n66mnhNRNKBauDa1j7pHxaXjxpjjGmaNQ0ZY0yYs0RgjDFhzhKBMcaEOUsExhgT5iwRGGNMmLNEYEyQicjU9tRTqjH+LBEYY0yYs0RgjCMiP3D/w/CViPzDdbxXLiIPuH7gPxCRbm7e8SKyUES+EZHXXT81iMhgEXnf/ZfDFyIyyK0+0ed/AV5wd64iIve5/6P4RkTuD9FbN2HOEoExgIiMAC4EJrvO9mqBS4EEYImqjsLr/neGW+RZ4FeqOhZY6jP+BWCmqo7D66dmixt/CHAjMBIYCEx2d4qeDYxy67k3mO/RmKZYIjDGcxxwGLDYdfdxHF6BXQf8083zPDDF9fOfoqofu/HPAEe7/oYyVfV1AFWtVNUKN88iVc1X1Tq83iP7A6VAJfCEiJwD1M9rTJuyRGCMR4BnVHW8ewxT1bsame9A+2TZ7fO6FohyfzIyAa9X0O/jdTFsTJuzRGCM5wPgPJ9eHtNEpB/eb+Q8N88lwHxVLQW2ichRbvxlwMfuX+byReQst44Y90cijXL/Q5HsOhn8OTAuCO/LmGaFRe+jxjRHVVeIyO14/zIWgevlEe/PXCa4aQV45xEApgF/dwW9b6+elwH/EJG73TrO389mk4BZ7g/SBfhFK78tYwJivY8asx8iUq6qiaGOw5hgsqYhY4wJc3ZEYIwxYc6OCIwxJsxZIjDGmDBnicAYY8KcJQJjjAlzlgiMMSbM/X/M1ghspg3TywAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch = 9\n",
    "epochs = np.arange(1,epoch+1)\n",
    "plt.plot(epochs,mcc_score_train[:-1],label='mcc_train')\n",
    "plt.plot(epochs,mcc_score_test[:-1],label='mcc_test')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('mcc_score')\n",
    "plt.ylim(0.0,1.0)\n",
    "\n",
    "plt.title('XMLRobertaForTokenClassification_earlyStopping')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained('xlm-roberta-base')\n",
    "config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): EntityModel(\n",
      "    (bert): XLMRobertaForTokenClassification(\n",
      "      (roberta): RobertaModel(\n",
      "        (embeddings): RobertaEmbeddings(\n",
      "          (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
      "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "          (token_type_embeddings): Embedding(1, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): RobertaEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/pytorch-1.6-gpu-py36-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
