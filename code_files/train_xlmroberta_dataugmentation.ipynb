{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.6/site-packages (4.18.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers) (3.4.1)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.49)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.19.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers) (4.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.51.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (0.1.96)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (3.6.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.6/site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from nltk) (4.51.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from nltk) (7.1.2)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.6/site-packages (7.7.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.6/site-packages (from ipywidgets) (4.3.3)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.6/site-packages (from ipywidgets) (5.5.6)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.6/site-packages (from ipywidgets) (5.1.3)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.6/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.6/site-packages (from ipywidgets) (3.6.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.6/site-packages (from ipywidgets) (1.1.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.6/site-packages (from ipywidgets) (7.16.3)\n",
      "Requirement already satisfied: tornado>=4.2 in /opt/conda/lib/python3.6/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: jupyter-client in /opt/conda/lib/python3.6/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.1.2)\n",
      "Requirement already satisfied: jedi<=0.17.2,>=0.10 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (0.17.2)\n",
      "Requirement already satisfied: pexpect in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (59.3.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (2.11.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.28)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /opt/conda/lib/python3.6/site-packages (from jedi<=0.17.2,>=0.10->ipython>=4.0.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.6/site-packages (from nbformat>=4.2.0->ipywidgets) (4.9.2)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.6/site-packages (from nbformat>=4.2.0->ipywidgets) (3.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (4.4.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.6/site-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.4.10)\n",
      "Requirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (22.1.0)\n",
      "Requirement already satisfied: nbconvert>=5 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.0.7)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.14.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.12.1)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.3.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.6/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.6/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.6/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.6/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.6/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.6/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.9)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.6/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.1.0)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.6/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.6/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: async-generator in /opt/conda/lib/python3.6/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.10)\n",
      "Requirement already satisfied: ptyprocess in /opt/conda/lib/python3.6/site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.6/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.10.0.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.20)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (20.9)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.6/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.4.7)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install nltk\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "# from DataAugmentation \n",
    "from data_utils import loadDatafromFile,createTokenizedDf,CompDataset,createkfoldData,createDataloaders\n",
    "\n",
    "from DataAugmentation import DataAugmentation\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from importlib import reload\n",
    "import config\n",
    "from transformers import XLMRobertaForTokenClassification, XLMRobertaConfig ,BertModel, XLMRobertaTokenizer, XLMRobertaModel, BertForTokenClassification\n",
    "from seqeval.metrics import accuracy_score, classification_report,f1_score\n",
    "from sklearn.metrics import matthews_corrcoef,classification_report\n",
    "import engine\n",
    "# from model_new import EntityModel\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "def process_data(filePath_src,filePath_tar, filePath_srcTags,filePath_tarTags,model_type):\n",
    "    \n",
    "    dataObj = loadDatafromFile(filePath_src,filePath_tar, filePath_srcTags,filePath_tarTags)\n",
    "    df= dataObj.createDf() # get dataframe from files\n",
    "    obj_tokenized = createTokenizedDf(df,model_type)\n",
    "    df_new= obj_tokenized.convertDf()\n",
    "    train_data = CompDataset(df_new,model_type)\n",
    "    return train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if loading of the non augmented (noise-free) data is needed, then run this\n",
    "# dataset_train = process_data(config.filePath_src,config.filePath_tar, config.filePath_srcTags,config.filePath_tarTags,model_type = 'xlm')\n",
    "# len(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading dev/eval data\n",
    "dataset_eval = process_data(config.filePath_src_eval,config.filePath_tar_eval, config.filePath_srcTags_eval,config.filePath_tarTags_eval,model_type = 'xlm')\n",
    "len(dataset_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>src_tokens</th>\n",
       "      <th>tar_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>José Ortega y Gasset visited Husserl at Freibu...</td>\n",
       "      <td>1934 besuchte José Ortega y Gasset Husserl in ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>However , a disappointing ninth in China meant...</td>\n",
       "      <td>Eine enttäuschende Neunte in China bedeutete j...</td>\n",
       "      <td>OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...</td>\n",
       "      <td>OK BAD OK BAD OK BAD OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In his diary , Chase wrote that the release of...</td>\n",
       "      <td>In seinem Tagebuch , Chase schrieb , dass die ...</td>\n",
       "      <td>OK OK OK OK BAD BAD OK OK OK OK OK OK OK OK BA...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK BAD OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heavy arquebuses mounted on wagons were called...</td>\n",
       "      <td>Schwere Arquebuses auf Waggons montiert wurden...</td>\n",
       "      <td>OK BAD BAD OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK BAD OK OK OK OK BAD BAD OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once North Pacific salmon die off after spawni...</td>\n",
       "      <td>Sobald der nordpazifische Lachs nach dem Laich...</td>\n",
       "      <td>OK OK OK OK BAD OK OK OK OK OK BAD OK BAD OK O...</td>\n",
       "      <td>OK OK OK BAD OK OK OK BAD OK OK OK OK OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>Some may also discourage or disallow unsanitar...</td>\n",
       "      <td>Einige können auch unhygienische Praktiken wie...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>In the late 1860s , the crinolines disappeared...</td>\n",
       "      <td>In den späten 1860er Jahren verschwanden die K...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK BAD BAD OK OK OK...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>Disco was criticized as mindless , consumerist...</td>\n",
       "      <td>Disco wurde als geistlos , konsumistisch , übe...</td>\n",
       "      <td>OK OK OK OK BAD OK BAD OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>Planters would then fill large hogsheads with ...</td>\n",
       "      <td>Die Pflanzer würden dann große Heuschrecken mi...</td>\n",
       "      <td>OK OK OK BAD OK BAD OK OK BAD BAD OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK BAD OK OK OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>He slew Krishna 's most dangerous enemy , Jara...</td>\n",
       "      <td>Er tötete Krishnas gefährlichsten Feind Jarasa...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK BAD OK OK ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 source  \\\n",
       "0     José Ortega y Gasset visited Husserl at Freibu...   \n",
       "1     However , a disappointing ninth in China meant...   \n",
       "2     In his diary , Chase wrote that the release of...   \n",
       "3     Heavy arquebuses mounted on wagons were called...   \n",
       "4     Once North Pacific salmon die off after spawni...   \n",
       "...                                                 ...   \n",
       "6995  Some may also discourage or disallow unsanitar...   \n",
       "6996  In the late 1860s , the crinolines disappeared...   \n",
       "6997  Disco was criticized as mindless , consumerist...   \n",
       "6998  Planters would then fill large hogsheads with ...   \n",
       "6999  He slew Krishna 's most dangerous enemy , Jara...   \n",
       "\n",
       "                                                 target  \\\n",
       "0     1934 besuchte José Ortega y Gasset Husserl in ...   \n",
       "1     Eine enttäuschende Neunte in China bedeutete j...   \n",
       "2     In seinem Tagebuch , Chase schrieb , dass die ...   \n",
       "3     Schwere Arquebuses auf Waggons montiert wurden...   \n",
       "4     Sobald der nordpazifische Lachs nach dem Laich...   \n",
       "...                                                 ...   \n",
       "6995  Einige können auch unhygienische Praktiken wie...   \n",
       "6996  In den späten 1860er Jahren verschwanden die K...   \n",
       "6997  Disco wurde als geistlos , konsumistisch , übe...   \n",
       "6998  Die Pflanzer würden dann große Heuschrecken mi...   \n",
       "6999  Er tötete Krishnas gefährlichsten Feind Jarasa...   \n",
       "\n",
       "                                             src_tokens  \\\n",
       "0                      OK OK OK OK OK OK OK OK OK OK OK   \n",
       "1     OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...   \n",
       "2     OK OK OK OK BAD BAD OK OK OK OK OK OK OK OK BA...   \n",
       "3                    OK BAD BAD OK OK OK OK OK OK OK OK   \n",
       "4     OK OK OK OK BAD OK OK OK OK OK BAD OK BAD OK O...   \n",
       "...                                                 ...   \n",
       "6995  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...   \n",
       "6996  OK OK OK OK OK OK OK OK OK OK BAD BAD OK OK OK...   \n",
       "6997              OK OK OK OK BAD OK BAD OK OK OK OK OK   \n",
       "6998   OK OK OK BAD OK BAD OK OK BAD BAD OK OK OK OK OK   \n",
       "6999  OK OK OK OK OK OK OK OK OK OK OK OK BAD OK OK ...   \n",
       "\n",
       "                                             tar_tokens  \n",
       "0     OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "1     OK BAD OK BAD OK BAD OK OK OK OK OK OK OK OK O...  \n",
       "2     OK OK OK OK OK OK OK OK OK BAD OK BAD OK OK OK...  \n",
       "3     OK OK OK BAD OK OK OK OK BAD BAD OK OK OK OK O...  \n",
       "4     OK OK OK BAD OK OK OK BAD OK OK OK OK OK OK OK...  \n",
       "...                                                 ...  \n",
       "6995  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "6996  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "6997  OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...  \n",
       "6998  OK OK OK OK OK BAD OK OK OK OK OK BAD OK OK OK...  \n",
       "6999  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "\n",
       "[7000 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataObj = loadDatafromFile(config.filePath_src,config.filePath_tar, config.filePath_srcTags,config.filePath_tarTags)\n",
    "df= dataObj.createDf() \n",
    "\n",
    "df\n",
    "# list(df.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (3.6.7)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from nltk) (4.51.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.6/site-packages (from nltk) (2022.3.15)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import DataAugmentation\n",
    "DataAugmentation = reload(DataAugmentation)\n",
    "from DataAugmentation import DataAugmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>src_tokens</th>\n",
       "      <th>tar_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>José Ortega y Gasset visited Husserl at Freibu...</td>\n",
       "      <td>1934 besuchte José Ortega y Gasset Husserl in ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>However , a disappointing ninth in China meant...</td>\n",
       "      <td>Eine enttäuschende Neunte in China bedeutete j...</td>\n",
       "      <td>OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...</td>\n",
       "      <td>OK BAD OK BAD OK BAD OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In his diary , Chase wrote that the release of...</td>\n",
       "      <td>In seinem Tagebuch , Chase schrieb , dass die ...</td>\n",
       "      <td>OK OK OK OK BAD BAD OK OK OK OK OK OK OK OK BA...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK BAD OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heavy arquebuses mounted on wagons were called...</td>\n",
       "      <td>Schwere Arquebuses auf Waggons montiert wurden...</td>\n",
       "      <td>OK BAD BAD OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK BAD OK OK OK OK BAD BAD OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once North Pacific salmon die off after spawni...</td>\n",
       "      <td>Sobald der nordpazifische Lachs nach dem Laich...</td>\n",
       "      <td>OK OK OK OK BAD OK OK OK OK OK BAD OK BAD OK O...</td>\n",
       "      <td>OK OK OK BAD OK OK OK BAD OK OK OK OK OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13995</th>\n",
       "      <td>Some crataegus laevigata also discourage or di...</td>\n",
       "      <td>Einige können auch unhygienische Praktiken wie...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13996</th>\n",
       "      <td>In the late 1860s , the crinolines disappeared...</td>\n",
       "      <td>In den späten 1860er Jahren verschwanden die K...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK BAD BAD OK OK OK...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13997</th>\n",
       "      <td>Disco was criticized as fatuous , consumerist ...</td>\n",
       "      <td>Disco wurde als geistlos , konsumistisch , übe...</td>\n",
       "      <td>OK OK OK OK BAD OK BAD OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13998</th>\n",
       "      <td>Planters would then fill large hogshead with t...</td>\n",
       "      <td>Die Pflanzer würden dann große Heuschrecken mi...</td>\n",
       "      <td>OK OK OK BAD OK BAD OK OK BAD BAD OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK BAD OK OK OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13999</th>\n",
       "      <td>He slew krishna 's most dangerous enemy , Jara...</td>\n",
       "      <td>Er tötete Krishnas gefährlichsten Feind Jarasa...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK BAD OK OK ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  source  \\\n",
       "0      José Ortega y Gasset visited Husserl at Freibu...   \n",
       "1      However , a disappointing ninth in China meant...   \n",
       "2      In his diary , Chase wrote that the release of...   \n",
       "3      Heavy arquebuses mounted on wagons were called...   \n",
       "4      Once North Pacific salmon die off after spawni...   \n",
       "...                                                  ...   \n",
       "13995  Some crataegus laevigata also discourage or di...   \n",
       "13996  In the late 1860s , the crinolines disappeared...   \n",
       "13997  Disco was criticized as fatuous , consumerist ...   \n",
       "13998  Planters would then fill large hogshead with t...   \n",
       "13999  He slew krishna 's most dangerous enemy , Jara...   \n",
       "\n",
       "                                                  target  \\\n",
       "0      1934 besuchte José Ortega y Gasset Husserl in ...   \n",
       "1      Eine enttäuschende Neunte in China bedeutete j...   \n",
       "2      In seinem Tagebuch , Chase schrieb , dass die ...   \n",
       "3      Schwere Arquebuses auf Waggons montiert wurden...   \n",
       "4      Sobald der nordpazifische Lachs nach dem Laich...   \n",
       "...                                                  ...   \n",
       "13995  Einige können auch unhygienische Praktiken wie...   \n",
       "13996  In den späten 1860er Jahren verschwanden die K...   \n",
       "13997  Disco wurde als geistlos , konsumistisch , übe...   \n",
       "13998  Die Pflanzer würden dann große Heuschrecken mi...   \n",
       "13999  Er tötete Krishnas gefährlichsten Feind Jarasa...   \n",
       "\n",
       "                                              src_tokens  \\\n",
       "0                       OK OK OK OK OK OK OK OK OK OK OK   \n",
       "1      OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...   \n",
       "2      OK OK OK OK BAD BAD OK OK OK OK OK OK OK OK BA...   \n",
       "3                     OK BAD BAD OK OK OK OK OK OK OK OK   \n",
       "4      OK OK OK OK BAD OK OK OK OK OK BAD OK BAD OK O...   \n",
       "...                                                  ...   \n",
       "13995  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...   \n",
       "13996  OK OK OK OK OK OK OK OK OK OK BAD BAD OK OK OK...   \n",
       "13997              OK OK OK OK BAD OK BAD OK OK OK OK OK   \n",
       "13998   OK OK OK BAD OK BAD OK OK BAD BAD OK OK OK OK OK   \n",
       "13999  OK OK OK OK OK OK OK OK OK OK OK OK BAD OK OK ...   \n",
       "\n",
       "                                              tar_tokens  \n",
       "0      OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "1      OK BAD OK BAD OK BAD OK OK OK OK OK OK OK OK O...  \n",
       "2      OK OK OK OK OK OK OK OK OK BAD OK BAD OK OK OK...  \n",
       "3      OK OK OK BAD OK OK OK OK BAD BAD OK OK OK OK O...  \n",
       "4      OK OK OK BAD OK OK OK BAD OK OK OK OK OK OK OK...  \n",
       "...                                                  ...  \n",
       "13995  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "13996  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "13997  OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...  \n",
       "13998  OK OK OK OK OK BAD OK OK OK OK OK BAD OK OK OK...  \n",
       "13999  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "\n",
       "[14000 rows x 4 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataaug_obj = DataAugmentation(df,swap_words=1,syn_words=2,del_words_prob=0.2,num_sentences=1)  \n",
    "# swapDataset = dataaug_obj.random_swap()\n",
    "# del_augDataset = dataaug_obj.random_deletion()\n",
    "# del_augDataset = del_augDataset[7000:]\n",
    "syn_dataset = dataaug_obj.synonym_replacement()\n",
    "# syn_dataset = syn_dataset[7000:]\n",
    "# frames = [swapDataset , del_augDataset,syn_dataset]\n",
    "# aug_df  = pd.concat(frames)\n",
    "syn_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['José Ortega y Gasset visited Husserl at Freiburg in 1934 .'\n",
      "  '1934 besuchte José Ortega y Gasset Husserl in Freiburg .'\n",
      "  'OK OK OK OK OK OK OK OK OK OK OK'\n",
      "  'OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK']]\n",
      "[['José Ortega wye Gasset visited edmund husserl at Freiburg in 1934 .'\n",
      "  '1934 besuchte José Ortega y Gasset Husserl in Freiburg .'\n",
      "  'OK OK OK OK OK OK OK OK OK OK OK OK'\n",
      "  'OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK']]\n"
     ]
    }
   ],
   "source": [
    "print(syn_dataset[0:1].values)\n",
    "print(syn_dataset[7000:7001].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['José Ortega y Gasset visited Husserl at Freiburg in 1934 .'\n",
      "  '1934 besuchte José Ortega y Gasset Husserl in Freiburg .'\n",
      "  'OK OK OK OK OK OK OK OK OK OK OK'\n",
      "  'OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK']]\n",
      "[['José daniel ortega y Gasset confabulate Husserl at Freiburg in 1934 .'\n",
      "  '1934 besuchte José Ortega y Gasset Husserl in Freiburg .'\n",
      "  'OK OK OK OK OK OK OK OK OK OK OK OK'\n",
      "  'OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK']]\n"
     ]
    }
   ],
   "source": [
    "print(syn_dataset[0:1].values)\n",
    "print(syn_dataset[7000:7001].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>src_tokens</th>\n",
       "      <th>tar_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>José Ortega y Gasset visited Husserl at Freibu...</td>\n",
       "      <td>1934 besuchte José Ortega y Gasset Husserl in ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>However , a disappointing ninth in China meant...</td>\n",
       "      <td>Eine enttäuschende Neunte in China bedeutete j...</td>\n",
       "      <td>OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...</td>\n",
       "      <td>OK BAD OK BAD OK BAD OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In his diary , Chase wrote that the release of...</td>\n",
       "      <td>In seinem Tagebuch , Chase schrieb , dass die ...</td>\n",
       "      <td>OK OK OK OK BAD BAD OK OK OK OK OK OK OK OK BA...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK BAD OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heavy arquebuses mounted on wagons were called...</td>\n",
       "      <td>Schwere Arquebuses auf Waggons montiert wurden...</td>\n",
       "      <td>OK BAD BAD OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK BAD OK OK OK OK BAD BAD OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once North Pacific salmon die off after spawni...</td>\n",
       "      <td>Sobald der nordpazifische Lachs nach dem Laich...</td>\n",
       "      <td>OK OK OK OK BAD OK OK OK OK OK BAD OK BAD OK O...</td>\n",
       "      <td>OK OK OK BAD OK OK OK BAD OK OK OK OK OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20995</th>\n",
       "      <td>discotheque was criticize as forgetful , consu...</td>\n",
       "      <td>Disco wurde als geistlos , konsumistisch , übe...</td>\n",
       "      <td>OK OK OK OK BAD OK BAD OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20996</th>\n",
       "      <td>planter would then fill large hogshead with to...</td>\n",
       "      <td>Die Pflanzer würden dann große Heuschrecken mi...</td>\n",
       "      <td>OK OK OK BAD OK BAD OK OK BAD BAD OK OK OK OK ...</td>\n",
       "      <td>OK OK OK OK OK BAD OK OK OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20997</th>\n",
       "      <td>plantation owner would then fill boastfully ho...</td>\n",
       "      <td>Die Pflanzer würden dann große Heuschrecken mi...</td>\n",
       "      <td>OK OK OK OK BAD OK BAD OK OK BAD BAD OK OK OK ...</td>\n",
       "      <td>OK OK OK OK OK BAD OK OK OK OK OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20998</th>\n",
       "      <td>He slew krishna 's most dangerous foeman , Jar...</td>\n",
       "      <td>Er tötete Krishnas gefährlichsten Feind Jarasa...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK BAD OK OK ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20999</th>\n",
       "      <td>He slew krishna 's most dangerous enemy , Jara...</td>\n",
       "      <td>Er tötete Krishnas gefährlichsten Feind Jarasa...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK BAD OK OK ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  source  \\\n",
       "0      José Ortega y Gasset visited Husserl at Freibu...   \n",
       "1      However , a disappointing ninth in China meant...   \n",
       "2      In his diary , Chase wrote that the release of...   \n",
       "3      Heavy arquebuses mounted on wagons were called...   \n",
       "4      Once North Pacific salmon die off after spawni...   \n",
       "...                                                  ...   \n",
       "20995  discotheque was criticize as forgetful , consu...   \n",
       "20996  planter would then fill large hogshead with to...   \n",
       "20997  plantation owner would then fill boastfully ho...   \n",
       "20998  He slew krishna 's most dangerous foeman , Jar...   \n",
       "20999  He slew krishna 's most dangerous enemy , Jara...   \n",
       "\n",
       "                                                  target  \\\n",
       "0      1934 besuchte José Ortega y Gasset Husserl in ...   \n",
       "1      Eine enttäuschende Neunte in China bedeutete j...   \n",
       "2      In seinem Tagebuch , Chase schrieb , dass die ...   \n",
       "3      Schwere Arquebuses auf Waggons montiert wurden...   \n",
       "4      Sobald der nordpazifische Lachs nach dem Laich...   \n",
       "...                                                  ...   \n",
       "20995  Disco wurde als geistlos , konsumistisch , übe...   \n",
       "20996  Die Pflanzer würden dann große Heuschrecken mi...   \n",
       "20997  Die Pflanzer würden dann große Heuschrecken mi...   \n",
       "20998  Er tötete Krishnas gefährlichsten Feind Jarasa...   \n",
       "20999  Er tötete Krishnas gefährlichsten Feind Jarasa...   \n",
       "\n",
       "                                              src_tokens  \\\n",
       "0                       OK OK OK OK OK OK OK OK OK OK OK   \n",
       "1      OK OK OK BAD OK OK OK OK OK OK OK OK OK OK OK ...   \n",
       "2      OK OK OK OK BAD BAD OK OK OK OK OK OK OK OK BA...   \n",
       "3                     OK BAD BAD OK OK OK OK OK OK OK OK   \n",
       "4      OK OK OK OK BAD OK OK OK OK OK BAD OK BAD OK O...   \n",
       "...                                                  ...   \n",
       "20995              OK OK OK OK BAD OK BAD OK OK OK OK OK   \n",
       "20996  OK OK OK BAD OK BAD OK OK BAD BAD OK OK OK OK ...   \n",
       "20997  OK OK OK OK BAD OK BAD OK OK BAD BAD OK OK OK ...   \n",
       "20998  OK OK OK OK OK OK OK OK OK OK OK OK BAD OK OK ...   \n",
       "20999  OK OK OK OK OK OK OK OK OK OK OK OK BAD OK OK ...   \n",
       "\n",
       "                                              tar_tokens  \n",
       "0      OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "1      OK BAD OK BAD OK BAD OK OK OK OK OK OK OK OK O...  \n",
       "2      OK OK OK OK OK OK OK OK OK BAD OK BAD OK OK OK...  \n",
       "3      OK OK OK BAD OK OK OK OK BAD BAD OK OK OK OK O...  \n",
       "4      OK OK OK BAD OK OK OK BAD OK OK OK OK OK OK OK...  \n",
       "...                                                  ...  \n",
       "20995  OK OK OK OK OK OK OK BAD OK OK OK BAD OK OK OK...  \n",
       "20996  OK OK OK OK OK BAD OK OK OK OK OK BAD OK OK OK...  \n",
       "20997  OK OK OK OK OK BAD OK OK OK OK OK BAD OK OK OK...  \n",
       "20998  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "20999  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "\n",
       "[21000 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>src_tokens</th>\n",
       "      <th>tar_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rainfall in Puerto rico peak at 17.6 in in Río...</td>\n",
       "      <td>Die Niederschläge in Puerto Rico erreichten ih...</td>\n",
       "      <td>BAD OK OK OK BAD OK BAD OK OK OK OK OK</td>\n",
       "      <td>OK BAD OK BAD OK OK OK OK OK OK OK BAD OK BAD ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Airbags can deploy due to the vehicle 's under...</td>\n",
       "      <td>Airbags können einsetzen , weil das Fahrzeug U...</td>\n",
       "      <td>OK OK BAD OK OK OK BAD OK BAD OK OK OK OK BAD ...</td>\n",
       "      <td>OK OK OK OK BAD BAD OK OK OK BAD OK OK BAD BAD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Plenimaran necromancers sometimes appropriated...</td>\n",
       "      <td>Plenimaran Nekromanten manchmal angeeignet und...</td>\n",
       "      <td>BAD OK OK OK BAD OK OK OK OK BAD OK OK OK OK O...</td>\n",
       "      <td>OK BAD OK OK BAD OK OK BAD OK BAD OK BAD OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After trekking across the jungle Jack exposes ...</td>\n",
       "      <td>Nach einer Wanderung durch den Dschungel entla...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They spring the Mestizo and Mulatto population...</td>\n",
       "      <td>Sie bildeten die Mestizo- und Mulatto-Bevölker...</td>\n",
       "      <td>OK OK OK OK OK BAD BAD OK OK OK OK OK OK OK OK OK</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK BAD OK OK OK ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20995</th>\n",
       "      <td>With fresh troops the portuguese find the bulw...</td>\n",
       "      <td>Mit frischen Truppen eroberten die Portugiesen...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20996</th>\n",
       "      <td>Winged male pismire , telephone drones , emerg...</td>\n",
       "      <td>Geflügelte männliche Ameisen , genannt Drohnen...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20997</th>\n",
       "      <td>maize is the john roy major staple fiber , and...</td>\n",
       "      <td>Mais ist das wichtigste Grundnahrungsmittel , ...</td>\n",
       "      <td>OK OK OK BAD BAD BAD BAD BAD OK OK OK OK OK OK...</td>\n",
       "      <td>OK OK OK OK OK OK OK BAD OK BAD OK OK OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20998</th>\n",
       "      <td>In , Shapiro sign on vet assistance for the de...</td>\n",
       "      <td>In , unterzeichnete Shapiro Veteran Hilfe für ...</td>\n",
       "      <td>OK OK OK OK OK BAD BAD OK OK BAD BAD OK OK OK ...</td>\n",
       "      <td>OK OK OK OK OK OK OK OK OK BAD OK BAD OK OK OK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20999</th>\n",
       "      <td>The keel was a flattened plank about twice as ...</td>\n",
       "      <td>Der Kiel war eine abgeflachte Planke etwa dopp...</td>\n",
       "      <td>OK OK OK BAD BAD BAD OK OK OK OK OK BAD BAD OK...</td>\n",
       "      <td>OK OK OK OK OK OK BAD BAD OK BAD OK BAD OK OK ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  source  \\\n",
       "0      rainfall in Puerto rico peak at 17.6 in in Río...   \n",
       "1      Airbags can deploy due to the vehicle 's under...   \n",
       "2      Plenimaran necromancers sometimes appropriated...   \n",
       "3      After trekking across the jungle Jack exposes ...   \n",
       "4      They spring the Mestizo and Mulatto population...   \n",
       "...                                                  ...   \n",
       "20995  With fresh troops the portuguese find the bulw...   \n",
       "20996  Winged male pismire , telephone drones , emerg...   \n",
       "20997  maize is the john roy major staple fiber , and...   \n",
       "20998  In , Shapiro sign on vet assistance for the de...   \n",
       "20999  The keel was a flattened plank about twice as ...   \n",
       "\n",
       "                                                  target  \\\n",
       "0      Die Niederschläge in Puerto Rico erreichten ih...   \n",
       "1      Airbags können einsetzen , weil das Fahrzeug U...   \n",
       "2      Plenimaran Nekromanten manchmal angeeignet und...   \n",
       "3      Nach einer Wanderung durch den Dschungel entla...   \n",
       "4      Sie bildeten die Mestizo- und Mulatto-Bevölker...   \n",
       "...                                                  ...   \n",
       "20995  Mit frischen Truppen eroberten die Portugiesen...   \n",
       "20996  Geflügelte männliche Ameisen , genannt Drohnen...   \n",
       "20997  Mais ist das wichtigste Grundnahrungsmittel , ...   \n",
       "20998  In , unterzeichnete Shapiro Veteran Hilfe für ...   \n",
       "20999  Der Kiel war eine abgeflachte Planke etwa dopp...   \n",
       "\n",
       "                                              src_tokens  \\\n",
       "0                 BAD OK OK OK BAD OK BAD OK OK OK OK OK   \n",
       "1      OK OK BAD OK OK OK BAD OK BAD OK OK OK OK BAD ...   \n",
       "2      BAD OK OK OK BAD OK OK OK OK BAD OK OK OK OK O...   \n",
       "3                    OK OK OK OK OK OK OK OK OK OK OK OK   \n",
       "4      OK OK OK OK OK BAD BAD OK OK OK OK OK OK OK OK OK   \n",
       "...                                                  ...   \n",
       "20995  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...   \n",
       "20996  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...   \n",
       "20997  OK OK OK BAD BAD BAD BAD BAD OK OK OK OK OK OK...   \n",
       "20998  OK OK OK OK OK BAD BAD OK OK BAD BAD OK OK OK ...   \n",
       "20999  OK OK OK BAD BAD BAD OK OK OK OK OK BAD BAD OK...   \n",
       "\n",
       "                                              tar_tokens  \n",
       "0      OK BAD OK BAD OK OK OK OK OK OK OK BAD OK BAD ...  \n",
       "1      OK OK OK OK BAD BAD OK OK OK BAD OK OK BAD BAD...  \n",
       "2      OK BAD OK OK BAD OK OK BAD OK BAD OK BAD OK OK...  \n",
       "3      OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "4      OK OK OK OK OK OK OK OK OK OK OK BAD OK OK OK ...  \n",
       "...                                                  ...  \n",
       "20995  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "20996  OK OK OK OK OK OK OK OK OK OK OK OK OK OK OK O...  \n",
       "20997  OK OK OK OK OK OK OK BAD OK BAD OK OK OK OK OK...  \n",
       "20998  OK OK OK OK OK OK OK OK OK BAD OK BAD OK OK OK...  \n",
       "20999  OK OK OK OK OK OK BAD BAD OK BAD OK BAD OK OK ...  \n",
       "\n",
       "[21000 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_dataset =syn_dataset.sample(frac=1).reset_index(drop=True)\n",
    "syn_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DataAugmentation methods\n",
    "# def random_swap(dataframe,n): # n is number of times to swap randomly 2 words\n",
    "    \n",
    "#     source_sentences  = list(dataframe.source)\n",
    "#     target_sentences = list(dataframe.target)\n",
    "#     labels_src = list(dataframe.src_tokens)\n",
    "#     labels_tar = list(dataframe.tar_tokens)\n",
    "#     source_sentences_temp =[]\n",
    "#     labels_sec_temp=[]\n",
    "#     i=0\n",
    "    \n",
    "#     for sentences, labels in zip(source_sentences,labels_src):\n",
    "        \n",
    "#         sentences = sentences.split()\n",
    "#         labels = labels.split()\n",
    "        \n",
    "#         for _ in range(5):\n",
    "#             for _ in range(n):\n",
    "#                 sentences, labels = swap_word(sentences,labels)\n",
    "#             assert(len(sentences) == len(labels))\n",
    "#             sentences_str = ' '.join(sentences)\n",
    "#             labels_str = ' '.join(labels)\n",
    "#             target_sentences.append(target_sentences[i])\n",
    "#             labels_tar.append(labels_tar[i])\n",
    "#             source_sentences_temp.append(sentences_str)\n",
    "#             labels_sec_temp.append(labels_str)\n",
    "        \n",
    "# #         break\n",
    "#         i+=1\n",
    "    \n",
    "#     source_sentences.extend(source_sentences_temp)\n",
    "#     labels_src.extend(labels_sec_temp)\n",
    "# #     print(source_sentences)\n",
    "# #     print(labels_src)\n",
    "#     column_names = [\"source\",\"target\",\"src_tokens\",\"tar_tokens\"]\n",
    "#     df = pd.DataFrame(columns=column_names,dtype=object)\n",
    "#     df = df.assign(source=source_sentences)\n",
    "#     df = df.assign(target = target_sentences)\n",
    "#     df = df.assign(src_tokens = labels_src)\n",
    "#     df = df.assign(tar_tokens = labels_tar)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# def swap_word(new_words,labels_src):\n",
    "\n",
    "#     random_idx_1 = random.randint(0, len(new_words)-1)\n",
    "#     random_idx_2 = random_idx_1\n",
    "#     counter = 0\n",
    "#     while random_idx_2 == random_idx_1:\n",
    "#         random_idx_2 = random.randint(0, len(new_words)-1)\n",
    "#         counter += 1\n",
    "#         if counter > 3:\n",
    "#             return (new_words,labels_src)\n",
    "#     new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n",
    "#     labels_src[random_idx_1], labels_src[random_idx_2] = labels_src[random_idx_2], labels_src[random_idx_1]\n",
    "# #     print(labels_src)\n",
    "# #     new_str = ''.join(new_words)\n",
    "#     return (new_words, labels_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df = random_swap(df,2) # tune how much words you want to swap\n",
    "# # new_df = new_df.sample(frac=1).reset_index(drop=True) # shuffling the dafaframe with resetting the index\n",
    "# print(new_df.iloc[0].values)\n",
    "# print(new_df.iloc[7000].values)\n",
    "# print(new_df.iloc[7001].values)\n",
    "# print(new_df.iloc[7002].values)\n",
    "# print(new_df.iloc[7003].values)\n",
    "# print(new_df.iloc[7004].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def random_deletion(dataframe, p):\n",
    "    \n",
    "#     source_sentences  = list(dataframe.source)\n",
    "#     target_sentences = list(dataframe.target)\n",
    "#     labels_src = list(dataframe.src_tokens)\n",
    "#     labels_tar = list(dataframe.tar_tokens)\n",
    "#     senetences_temp=[]\n",
    "#     labels_temp= []\n",
    "#     #randomly delete words with probability p\n",
    "#     i=0\n",
    "#     for sentences, labels in zip(source_sentences,labels_src):\n",
    "            \n",
    "#         sentences = sentences.split()\n",
    "#         labels = labels.split() \n",
    "        \n",
    "        \n",
    "        \n",
    "#         if len(sentences) == 1:\n",
    "#             i+=1\n",
    "#             continue\n",
    "            \n",
    "        \n",
    "               \n",
    "#         for _ in range(5):\n",
    "            \n",
    "#             source_sentences_temp=[]\n",
    "#             labels_sec_temp=[]\n",
    "#             for word,label in zip(sentences,labels):\n",
    "#                 r = random.uniform(0, 1)\n",
    "#                 if r > p:\n",
    "#                     source_sentences_temp.append(word)\n",
    "#                     labels_sec_temp.append(label)\n",
    "#             if len(source_sentences_temp) == 0: #if you end up deleting all words, just return a random word\n",
    "#                 rand_int = random.randint(0, len(source_sentences_temp)-1)\n",
    "#                 source_sentences_temp.append(sentences[rand_int])\n",
    "#                 labels_sec_temp.append(labels[rand_int])\n",
    "#             assert(len(source_sentences_temp) == len(labels_sec_temp))\n",
    "#             sentences_str = ' '.join(source_sentences_temp)\n",
    "#             labels_str = ' '.join(labels_sec_temp)\n",
    "#             senetences_temp.append(sentences_str)\n",
    "#             labels_temp.append(labels_str)\n",
    "#             target_sentences.append(target_sentences[i])\n",
    "#             labels_tar.append(labels_tar[i])\n",
    "# #         break\n",
    "#         i+=1\n",
    "#     source_sentences.extend(senetences_temp)\n",
    "#     labels_src.extend(labels_temp)    \n",
    "#     column_names = [\"source\",\"target\",\"src_tokens\",\"tar_tokens\"]\n",
    "#     df = pd.DataFrame(columns=column_names,dtype=object)\n",
    "#     df = df.assign(source=source_sentences)\n",
    "#     df = df.assign(target = target_sentences)\n",
    "#     df = df.assign(src_tokens = labels_src)\n",
    "#     df = df.assign(tar_tokens = labels_tar)  \n",
    "\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in /opt/conda/lib/python3.6/site-packages (1.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.6/site-packages (from seqeval) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.6/site-packages (from seqeval) (1.19.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn>=0.21.3->seqeval) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.6/site-packages (from scikit-learn>=0.21.3->seqeval) (1.5.4)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.6/site-packages (4.15.0)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers) (4.4.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.51.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.19.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (0.1.96)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import data_utils\n",
    "data_utils=reload(data_utils)\n",
    "from data_utils import loadDatafromFile,createTokenizedDf,CompDataset,createkfoldData,createDataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_tokenized = createTokenizedDf(syn_dataset,model_type = 'xlm')\n",
    "df_new= obj_tokenized.convertDf()\n",
    "# enc_label = preprocessing.LabelEncoder()\n",
    "# df_new['labels']= enc_label.fit_transform(df_new['labels'])\n",
    "train_data = CompDataset(df_new, model_type = 'xlm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([     0, 102044,   9146,     23,  83775,  23306,    280,    344,     99,\n",
       "           7076,    910,     23,     23,  93460, 143650,      6,      5,      2,\n",
       "              2,    656,    622,    656,  82237, 167191,    656,     23,    656,\n",
       "          83775,    656,  70867,    656,  72558,     33,    656,  22667,    656,\n",
       "          46439,  15803,    656,   1079,    656,   7076,    910,    656,     23,\n",
       "            656,  93460,    656, 143650,    656,      6,      5,    656,      2,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([-100,    0,    0,    1,    1,    1,    0,    0,    1,    0,    0,    1,\n",
       "            1,    1,    1,    1,    1, -100, -100,    1,    0,    1,    0,    0,\n",
       "            1,    1,    1,    1,    1,    1,    1,    0,    0,    1,    0,    1,\n",
       "            0,    0,    1,    0,    1,    0,    0,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f1c19ebf6d8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataloaders\n",
    "loader_obj = createDataloaders(train_data,config.TRAIN_BATCH_SIZE)\n",
    "train_dataloader = loader_obj.createDataloaders()\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1313"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_obj = createDataloaders(dataset_eval,config.VALID_BATCH_SIZE)\n",
    "val_dataloader = loader_obj.createDataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-01-12 15:43:50.273 pytorch-1-6-gpu-py-ml-g4dn-2xlarge-775645915fb58e49f1a4db4ff39d:63 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2022-01-12 15:43:50.306 pytorch-1-6-gpu-py-ml-g4dn-2xlarge-775645915fb58e49f1a4db4ff39d:63 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([     0,    360,    267,      6,      4,     70,    391,  69466,  38857,\n",
       "           685,   6226,    111,     70, 186672,  13038,      6,      4,  12960,\n",
       "             6, 100184,    214,    391,  69466,     56,  13320,  25365,    169,\n",
       "           237,  62493,     42,      6,      5,      2,      2,    656,   3370,\n",
       "           656,   7418,    656,    267,    656,   1659,  10697,     39,    656,\n",
       "            68,    656,    391,  69466,    656,     68,    656, 112202,    656,\n",
       "          1659,    656,    381,    656, 186672,    656,  13038,    656,      6,\n",
       "             4,    656,  31005,    656,     68,    656,  34288,    656,    391,\n",
       "         69466,     56,    656,  13320,    656,  25365,    169,    656,    737,\n",
       "           656, 114824,  73396,    474,    656,      6,      5,    656,      2,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(val_dataloader))\n",
    "batch[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(output,target,mask):\n",
    "    lfn = nn.CrossEntropyLoss()\n",
    "    active_loss = mask.view(-1) == 1 #loss calculation for non padded tokens only (mask =1)\n",
    "    active_logits = output.view(-1,2)\n",
    "    active_labels = torch.where(\n",
    "        active_loss,\n",
    "        target.view(-1),\n",
    "        torch.tensor(lfn.ignore_index).type_as(target)    \n",
    "    )\n",
    "    loss = lfn(active_logits,active_labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(EntityModel, self).__init__()\n",
    "        self.bert = XLMRobertaForTokenClassification.from_pretrained(config.BASE_MODEL,output_attentions = False, output_hidden_states = False)\n",
    "#         self.bert_drop_1 = nn.Dropout(0.3)\n",
    "#         self.out_tag = nn.Linear(768, 2)\n",
    "    \n",
    "    def forward(self, ids, attention_mask, labels):\n",
    "        \n",
    "        outputs = self.bert(ids,\n",
    "                                attention_mask = attention_mask,\n",
    "                                labels = labels,return_dict=False)\n",
    "#         bo_tag = self.bert_drop_1(output_1)\n",
    "        \n",
    "#         tag = self.out_tag(bo_tag)  \n",
    "        \n",
    "#         loss_tag = loss_fn(outputs[1],labels,attention_mask)\n",
    "        \n",
    "#         return bo_tag,loss\n",
    "        return outputs[0], outputs[1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EntityModel(\n",
       "  (bert): XLMRobertaForTokenClassification(\n",
       "    (roberta): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EntityModel()\n",
    "model.cuda()\n",
    "# model = nn.DataParallel(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "# torch.cuda.empty_cache()\n",
    "# outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    def __init__(self, mode='max', min_delta=0, patience=10, percentage=False):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = 0\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "\n",
    "        if np.isnan(metrics):\n",
    "            return True\n",
    "\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            print('terminating because of early stopping!')\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best \n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best \n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (\n",
    "                            best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (\n",
    "                            best * min_delta / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.001,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "num_train_steps = int(len(syn_dataset) / config.TRAIN_BATCH_SIZE * config.EPOCHS) #10 is the batchsize\n",
    "optimizer = AdamW(optimizer_parameters, lr=2e-5) # used 3e-5\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",
    ")\n",
    "early_stopping = EarlyStopping(patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:13<00:00,  9.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.0\n",
      "Average F1 Validation score for class 1 : 0.9294044211680166\n",
      "Average Accuracy Validation score  : 0.8681190540521324\n",
      "Average mcc Validation score  : 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      9274\n",
      "           1       0.87      1.00      0.93     61047\n",
      "\n",
      "    accuracy                           0.87     70321\n",
      "   macro avg       0.43      0.50      0.46     70321\n",
      "weighted avg       0.75      0.87      0.81     70321\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "loss_test,f1_score_0_test, f1_score_1_test, accuracy_score_test, mcc_score_test,labels_test,preds_test = engine.eval_fn(val_dataloader, model)\n",
    "print(classification_report(labels_test,preds_test)) # validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1313/1313 [04:29<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.2446423072639241\n",
      "Average F1 Validation score for class 1 : 0.7406893927159744\n",
      "Average Accuracy Validation score  : 0.6139186332791281\n",
      "Average mcc Validation score  : 0.08753074919196924\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.50      0.24    187438\n",
      "           1       0.90      0.63      0.74   1305654\n",
      "\n",
      "    accuracy                           0.61   1493092\n",
      "   macro avg       0.53      0.56      0.49   1493092\n",
      "weighted avg       0.81      0.61      0.68   1493092\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_test,f1_score_0_test, f1_score_1_test, accuracy_score_test, mcc_score_test,labels_test,preds_test = engine.eval_fn(train_dataloader, model)\n",
    "print(classification_report(labels_test,preds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.6/site-packages (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.19.1)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.51.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers) (4.4.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (0.1.96)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "config = reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,param in model.named_parameters():\n",
    "    if 'classifier' not in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1313/1313 [16:04<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Training score for class 0 : 0.21621739719383187\n",
      "Average F1 Training score for class 1 : 0.9349475996897\n",
      "Average Accuracy Training score  : 0.879866076571303\n",
      "Average mcc Training score  : 0.2405450922127301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.13      0.22    187438\n",
      "           1       0.89      0.99      0.93   1305654\n",
      "\n",
      "    accuracy                           0.88   1493092\n",
      "   macro avg       0.74      0.56      0.58   1493092\n",
      "weighted avg       0.85      0.88      0.84   1493092\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:13<00:00,  9.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.2792963911140474\n",
      "Average F1 Validation score for class 1 : 0.9321030396863063\n",
      "Average Accuracy Validation score  : 0.8758976692595384\n",
      "Average mcc Validation score  : 0.2814235775595017\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.18      0.28      9274\n",
      "           1       0.89      0.98      0.93     61047\n",
      "\n",
      "    accuracy                           0.88     70321\n",
      "   macro avg       0.74      0.58      0.61     70321\n",
      "weighted avg       0.85      0.88      0.85     70321\n",
      "\n",
      "Train Loss = 0.30570135504484 Valid Loss = 0.3142368052005768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1313/1313 [16:26<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Training score for class 0 : 0.55876953125\n",
      "Average F1 Training score for class 1 : 0.9494039531404442\n",
      "Average Accuracy Training score  : 0.9092179182528605\n",
      "Average mcc Training score  : 0.5268460558793198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.46      0.56    187438\n",
      "           1       0.93      0.97      0.95   1305654\n",
      "\n",
      "    accuracy                           0.91   1493092\n",
      "   macro avg       0.82      0.72      0.75   1493092\n",
      "weighted avg       0.90      0.91      0.90   1493092\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:13<00:00,  9.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.3540788134360913\n",
      "Average F1 Validation score for class 1 : 0.9299854990227603\n",
      "Average Accuracy Validation score  : 0.8736650502694785\n",
      "Average mcc Validation score  : 0.31734660563196504\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.26      0.35      9274\n",
      "           1       0.90      0.97      0.93     61047\n",
      "\n",
      "    accuracy                           0.87     70321\n",
      "   macro avg       0.72      0.61      0.64     70321\n",
      "weighted avg       0.85      0.87      0.85     70321\n",
      "\n",
      "Train Loss = 0.22569522981709955 Valid Loss = 0.3852432572841644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1313/1313 [15:46<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Training score for class 0 : 0.71370142550569\n",
      "Average F1 Training score for class 1 : 0.9630234351160337\n",
      "Average Accuracy Training score  : 0.9345057102978249\n",
      "Average mcc Training score  : 0.6812551002576461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.65      0.71    187438\n",
      "           1       0.95      0.98      0.96   1305654\n",
      "\n",
      "    accuracy                           0.93   1493092\n",
      "   macro avg       0.87      0.81      0.84   1493092\n",
      "weighted avg       0.93      0.93      0.93   1493092\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:13<00:00,  8.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.3372190476190477\n",
      "Average F1 Validation score for class 1 : 0.9317816448003012\n",
      "Average Accuracy Validation score  : 0.8762958433469377\n",
      "Average mcc Validation score  : 0.3149751768340832\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.24      0.34      9274\n",
      "           1       0.89      0.97      0.93     61047\n",
      "\n",
      "    accuracy                           0.88     70321\n",
      "   macro avg       0.73      0.61      0.63     70321\n",
      "weighted avg       0.85      0.88      0.85     70321\n",
      "\n",
      "Train Loss = 0.1644940828735694 Valid Loss = 0.52990469789505\n",
      "Epoch 4 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1313/1313 [16:11<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Training score for class 0 : 0.7882572023222697\n",
      "Average F1 Training score for class 1 : 0.9713374088883802\n",
      "Average Accuracy Training score  : 0.9495094742989715\n",
      "Average mcc Training score  : 0.761060234270728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.75      0.79    187438\n",
      "           1       0.96      0.98      0.97   1305654\n",
      "\n",
      "    accuracy                           0.95   1493092\n",
      "   macro avg       0.90      0.86      0.88   1493092\n",
      "weighted avg       0.95      0.95      0.95   1493092\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:13<00:00,  9.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.36437709956400544\n",
      "Average F1 Validation score for class 1 : 0.9297834205809666\n",
      "Average Accuracy Validation score  : 0.8735370657413859\n",
      "Average mcc Validation score  : 0.32373106008712976\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.27      0.36      9274\n",
      "           1       0.90      0.96      0.93     61047\n",
      "\n",
      "    accuracy                           0.87     70321\n",
      "   macro avg       0.72      0.62      0.65     70321\n",
      "weighted avg       0.85      0.87      0.86     70321\n",
      "\n",
      "Train Loss = 0.12704592027048164 Valid Loss = 0.5866565897464752\n",
      "Epoch 5 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1313/1313 [16:08<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Training score for class 0 : 0.8339483191611226\n",
      "Average F1 Training score for class 1 : 0.977010574022734\n",
      "Average Accuracy Training score  : 0.9596126695474894\n",
      "Average mcc Training score  : 0.8115343168694776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.81      0.83    187438\n",
      "           1       0.97      0.98      0.98   1305654\n",
      "\n",
      "    accuracy                           0.96   1493092\n",
      "   macro avg       0.92      0.89      0.91   1493092\n",
      "weighted avg       0.96      0.96      0.96   1493092\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:14<00:00,  8.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.35769004124926335\n",
      "Average F1 Validation score for class 1 : 0.9313742464545984\n",
      "Average Accuracy Validation score  : 0.8759972127813882\n",
      "Average mcc Validation score  : 0.3262976269642181\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.26      0.36      9274\n",
      "           1       0.90      0.97      0.93     61047\n",
      "\n",
      "    accuracy                           0.88     70321\n",
      "   macro avg       0.73      0.62      0.64     70321\n",
      "weighted avg       0.85      0.88      0.86     70321\n",
      "\n",
      "Train Loss = 0.10130218130175339 Valid Loss = 0.6680939408540726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1313/1313 [16:27<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Training score for class 0 : 0.8679062607934165\n",
      "Average F1 Training score for class 1 : 0.9814507102012883\n",
      "Average Accuracy Training score  : 0.9674695196277255\n",
      "Average mcc Training score  : 0.8495753871299485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.85      0.87    187438\n",
      "           1       0.98      0.98      0.98   1305654\n",
      "\n",
      "    accuracy                           0.97   1493092\n",
      "   macro avg       0.93      0.92      0.92   1493092\n",
      "weighted avg       0.97      0.97      0.97   1493092\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:13<00:00,  8.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.3606846396826562\n",
      "Average F1 Validation score for class 1 : 0.9314880853978226\n",
      "Average Accuracy Validation score  : 0.876238961334452\n",
      "Average mcc Validation score  : 0.32885822415321614\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.26      0.36      9274\n",
      "           1       0.90      0.97      0.93     61047\n",
      "\n",
      "    accuracy                           0.88     70321\n",
      "   macro avg       0.73      0.62      0.65     70321\n",
      "weighted avg       0.85      0.88      0.86     70321\n",
      "\n",
      "Train Loss = 0.08233160487541423 Valid Loss = 0.7265387146472931\n",
      "Epoch 7 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1313/1313 [16:09<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Training score for class 0 : 0.8929220775717424\n",
      "Average F1 Training score for class 1 : 0.9848234474563389\n",
      "Average Accuracy Training score  : 0.9734149000865319\n",
      "Average mcc Training score  : 0.8778202593086828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89    187438\n",
      "           1       0.98      0.99      0.98   1305654\n",
      "\n",
      "    accuracy                           0.97   1493092\n",
      "   macro avg       0.94      0.93      0.94   1493092\n",
      "weighted avg       0.97      0.97      0.97   1493092\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:13<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.39665455542606426\n",
      "Average F1 Validation score for class 1 : 0.9301812700805203\n",
      "Average Accuracy Validation score  : 0.8748453520285547\n",
      "Average mcc Validation score  : 0.3486336299571777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.31      0.40      9274\n",
      "           1       0.90      0.96      0.93     61047\n",
      "\n",
      "    accuracy                           0.87     70321\n",
      "   macro avg       0.72      0.64      0.66     70321\n",
      "weighted avg       0.85      0.87      0.86     70321\n",
      "\n",
      "Train Loss = 0.06728881029362499 Valid Loss = 0.7158814071416855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1313/1313 [16:10<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Training score for class 0 : 0.9129244205576083\n",
      "Average F1 Training score for class 1 : 0.9876043348677288\n",
      "Average Accuracy Training score  : 0.9782980553107243\n",
      "Average mcc Training score  : 0.9005616429958894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91    187438\n",
      "           1       0.99      0.99      0.99   1305654\n",
      "\n",
      "    accuracy                           0.98   1493092\n",
      "   macro avg       0.95      0.95      0.95   1493092\n",
      "weighted avg       0.98      0.98      0.98   1493092\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:13<00:00,  8.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.3562105880627658\n",
      "Average F1 Validation score for class 1 : 0.9308683191080596\n",
      "Average Accuracy Validation score  : 0.8751439825941042\n",
      "Average mcc Validation score  : 0.32287352537569997\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.26      0.36      9274\n",
      "           1       0.90      0.97      0.93     61047\n",
      "\n",
      "    accuracy                           0.88     70321\n",
      "   macro avg       0.73      0.62      0.64     70321\n",
      "weighted avg       0.85      0.88      0.86     70321\n",
      "\n",
      "Train Loss = 0.05496863259097614 Valid Loss = 0.8353734058141709\n",
      "Epoch 9 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1313/1313 [15:58<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Training score for class 0 : 0.9296016069635087\n",
      "Average F1 Training score for class 1 : 0.9899399458590352\n",
      "Average Accuracy Training score  : 0.9823955925020026\n",
      "Average mcc Training score  : 0.9195514445194947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93    187438\n",
      "           1       0.99      0.99      0.99   1305654\n",
      "\n",
      "    accuracy                           0.98   1493092\n",
      "   macro avg       0.96      0.96      0.96   1493092\n",
      "weighted avg       0.98      0.98      0.98   1493092\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:13<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.35691458454386993\n",
      "Average F1 Validation score for class 1 : 0.930214228289484\n",
      "Average Accuracy Validation score  : 0.8740916653631206\n",
      "Average mcc Validation score  : 0.32034862143257303\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.26      0.36      9274\n",
      "           1       0.90      0.97      0.93     61047\n",
      "\n",
      "    accuracy                           0.87     70321\n",
      "   macro avg       0.72      0.62      0.64     70321\n",
      "weighted avg       0.85      0.87      0.85     70321\n",
      "\n",
      "Train Loss = 0.04497170452665472 Valid Loss = 0.8764177551269531\n",
      "Epoch 10 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1313/1313 [16:06<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Training score for class 0 : 0.9425625053514856\n",
      "Average F1 Training score for class 1 : 0.9917832108942696\n",
      "Average Accuracy Training score  : 0.9856231230225599\n",
      "Average mcc Training score  : 0.9343515644124208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94    187438\n",
      "           1       0.99      0.99      0.99   1305654\n",
      "\n",
      "    accuracy                           0.99   1493092\n",
      "   macro avg       0.97      0.97      0.97   1493092\n",
      "weighted avg       0.99      0.99      0.99   1493092\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:13<00:00,  9.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.3451890677648821\n",
      "Average F1 Validation score for class 1 : 0.9312969902660916\n",
      "Average Accuracy Validation score  : 0.8756417002033532\n",
      "Average mcc Validation score  : 0.31758629363133917\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.25      0.35      9274\n",
      "           1       0.89      0.97      0.93     61047\n",
      "\n",
      "    accuracy                           0.88     70321\n",
      "   macro avg       0.73      0.61      0.64     70321\n",
      "weighted avg       0.85      0.88      0.85     70321\n",
      "\n",
      "Train Loss = 0.03721813064406187 Valid Loss = 0.948239264011383\n",
      "Epoch 11 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1313/1313 [16:09<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Training score for class 0 : 0.9523977527249012\n",
      "Average F1 Training score for class 1 : 0.9931815797755167\n",
      "Average Accuracy Training score  : 0.98807173302114\n",
      "Average mcc Training score  : 0.9455817407716577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95    187438\n",
      "           1       0.99      0.99      0.99   1305654\n",
      "\n",
      "    accuracy                           0.99   1493092\n",
      "   macro avg       0.97      0.97      0.97   1493092\n",
      "weighted avg       0.99      0.99      0.99   1493092\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:14<00:00,  8.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Validation score for class 0 : 0.3358685731670216\n",
      "Average F1 Validation score for class 1 : 0.9315105024550174\n",
      "Average Accuracy Validation score  : 0.8758265667439314\n",
      "Average mcc Validation score  : 0.31261335624760744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.24      0.34      9274\n",
      "           1       0.89      0.97      0.93     61047\n",
      "\n",
      "    accuracy                           0.88     70321\n",
      "   macro avg       0.73      0.61      0.63     70321\n",
      "weighted avg       0.85      0.88      0.85     70321\n",
      "\n",
      "Train Loss = 0.031005724326488245 Valid Loss = 1.0030939769744873\n",
      "terminating because of early stopping!\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = -1\n",
    "train_loss_lst = []\n",
    "val_loss_lst = []\n",
    "with open('metrics_xlmroberta__DataAugmentation_synonyms.txt', 'w') as f:\n",
    "    f.write('With 2 synonyms and 2 sentences new created for each'+'\\n')\n",
    "    for epoch in range(config.EPOCHS):\n",
    "            print(f'Epoch {epoch+1} of {config.EPOCHS}')\n",
    "            train_metrics = engine.train_fn(train_dataloader, model, optimizer, scheduler)\n",
    "            print(classification_report(train_metrics[5],train_metrics[6]))\n",
    "            test_metrics = engine.eval_fn(val_dataloader, model)\n",
    "            print(classification_report(test_metrics[5],test_metrics[6]))\n",
    "            print(f\"Train Loss = {train_metrics[0]} Valid Loss = {test_metrics[0]}\")\n",
    "            train_loss_lst.append(train_metrics[:-2])\n",
    "            val_loss_lst.append(test_metrics[:-2])\n",
    "            f.write(f\"Train_loss {epoch+1} : {str(train_loss_lst)}\" + '\\n')\n",
    "            f.write(f\"val_loss {epoch+1} : {str(val_loss_lst)}\" + '\\n')\n",
    "            if early_stopping.step(float(\"{:.2f}\".format(test_metrics[4]))): #mcc score for early stopping \"{:.2f}\".format(a_float)\n",
    "                  break  # early stop criterion is met, we can stop now\n",
    "            if float(\"{:.2f}\".format(test_metrics[4])) >  best_accuracy:\n",
    "                torch.save(model.state_dict(), './models/training_data/model_xlmrobertatokenclassificationmodel_DataAugsyn_earlystopping.bin')\n",
    "                best_accuracy = float(\"{:.2f}\".format(test_metrics[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_train : [0.2596988670428594, 0.14535545800413405, 0.09486724980956032, 0.06549827310052656, 0.045746724264252754, 0.03231346381482269, 0.023793228765018284, 0.01822031439125671, 0.014465644095281494, 0.011736312873212487, 0.00975374302065133] \n",
      " loss_test :  [0.41108598244190214, 0.5780734926462173, 0.7182654665112496, 0.7967876689434051, 0.8810246188640595, 0.8989573067426682, 0.9440780160427094, 1.0269084532260895, 1.0335683481693267, 1.089434811115265, 1.136594078540802] \n",
      " f1_class0_train : [0.4423549401437414, 0.7568812560106071, 0.8482965696001926, 0.8972757686363754, 0.9300456093481112, 0.9521440480160053, 0.9656451898604365, 0.974330639470636, 0.9802559775114599, 0.9839995953981867, 0.9866133988445922] \n",
      " f1_class0_test : [0.3796382355043969, 0.3774881855935845, 0.3644411656215419, 0.36616252985020625, 0.36134024381341706, 0.39102829537612144, 0.3907983393452665, 0.37212285327442457, 0.38136373291793524, 0.39230089097463106, 0.37711056163899676] \n",
      " f1_class1_train : [0.9430568975210891, 0.967716097217064, 0.978948651802155, 0.9854895324832081, 0.9900320566254329, 0.9931613479692177, 0.9950825050609498, 0.9963243340524522, 0.9971718268656239, 0.9977079306514609, 0.9980821643573136] \n",
      " f1_class1_test : [0.9314910583869568, 0.9313682149736334, 0.9322117919220692, 0.9309352404532301, 0.9310793033093593, 0.9300526349166086, 0.9289315516597987, 0.9304077909153378, 0.9309431784949296, 0.9290529542079896, 0.9302932317155121] \n",
      " accuracy_score_train : [0.8966656316008377, 0.9430011150027067, 0.96302780435855, 0.974571067497521, 0.9825505244982071, 0.9880328157003588, 0.9913965016998538, 0.9935694728735485, 0.9950523631019822, 0.9959902594857779, 0.9966449858731858] \n",
      " accuracy_score_test : [0.8766086944156084, 0.8763669458625446, 0.8774903656091353, 0.8754426131596535, 0.8755848181908676, 0.8745182804567626, 0.8727122765603447, 0.8747031469973408, 0.8757554642283244, 0.8729398046102871, 0.8746178239786123] \n",
      " mcc_score_train : [0.42575340281886703, 0.7270081931893919, 0.8277030327095686, 0.8828600023288193, 0.9200927155940536, 0.9453091288516656, 0.9607283117728831, 0.9706552474835313, 0.9774278848485424, 0.9817075843311471, 0.9846955921132662] \n",
      " mcc_score_test : [0.34176269965220984, 0.3397383216114773, 0.3349140532080163, 0.3299979218395618, 0.32735780993961483, 0.34402715830199043, 0.33981878229986434, 0.331849370462251, 0.34061369504195677, 0.3413711231293385, 0.33491168926083303]\n"
     ]
    }
   ],
   "source": [
    "train_loss_lst = [(0.2596988670428594, 0.4423549401437414, 0.9430568975210891, 0.8966656316008377, 0.42575340281886703), (0.14535545800413405, 0.7568812560106071, 0.967716097217064, 0.9430011150027067, 0.7270081931893919), (0.09486724980956032, 0.8482965696001926, 0.978948651802155, 0.96302780435855, 0.8277030327095686), (0.06549827310052656, 0.8972757686363754, 0.9854895324832081, 0.974571067497521, 0.8828600023288193), (0.045746724264252754, 0.9300456093481112, 0.9900320566254329, 0.9825505244982071, 0.9200927155940536), (0.03231346381482269, 0.9521440480160053, 0.9931613479692177, 0.9880328157003588, 0.9453091288516656), (0.023793228765018284, 0.9656451898604365, 0.9950825050609498, 0.9913965016998538, 0.9607283117728831), (0.01822031439125671, 0.974330639470636, 0.9963243340524522, 0.9935694728735485, 0.9706552474835313), (0.014465644095281494, 0.9802559775114599, 0.9971718268656239, 0.9950523631019822, 0.9774278848485424), (0.011736312873212487, 0.9839995953981867, 0.9977079306514609, 0.9959902594857779, 0.9817075843311471), (0.00975374302065133, 0.9866133988445922, 0.9980821643573136, 0.9966449858731858, 0.9846955921132662)]\n",
    "val_loss_lst = [(0.41108598244190214, 0.3796382355043969, 0.9314910583869568, 0.8766086944156084, 0.34176269965220984), (0.5780734926462173, 0.3774881855935845, 0.9313682149736334, 0.8763669458625446, 0.3397383216114773), (0.7182654665112496, 0.3644411656215419, 0.9322117919220692, 0.8774903656091353, 0.3349140532080163), (0.7967876689434051, 0.36616252985020625, 0.9309352404532301, 0.8754426131596535, 0.3299979218395618), (0.8810246188640595, 0.36134024381341706, 0.9310793033093593, 0.8755848181908676, 0.32735780993961483), (0.8989573067426682, 0.39102829537612144, 0.9300526349166086, 0.8745182804567626, 0.34402715830199043), (0.9440780160427094, 0.3907983393452665, 0.9289315516597987, 0.8727122765603447, 0.33981878229986434), (1.0269084532260895, 0.37212285327442457, 0.9304077909153378, 0.8747031469973408, 0.331849370462251), (1.0335683481693267, 0.38136373291793524, 0.9309431784949296, 0.8757554642283244, 0.34061369504195677), (1.089434811115265, 0.39230089097463106, 0.9290529542079896, 0.8729398046102871, 0.3413711231293385), (1.136594078540802, 0.37711056163899676, 0.9302932317155121, 0.8746178239786123, 0.33491168926083303)]\n",
    "train_metrices = np.array(train_loss_lst)\n",
    "test_metrices = np.array(val_loss_lst)\n",
    "loss_train = train_metrices[:,0]\n",
    "loss_test = test_metrices[:,0]\n",
    "f1_class0_train = train_metrices[:,1]\n",
    "f1_class0_test= test_metrices[:,1]\n",
    "f1_class1_train = train_metrices[:,2]\n",
    "f1_class1_test = test_metrices[:,2]\n",
    "accuracy_score_train = train_metrices[:,3]\n",
    "accuracy_score_test = test_metrices[:,3]\n",
    "mcc_score_train = train_metrices[:,4]\n",
    "mcc_score_test = test_metrices[:,4]\n",
    "print('loss_train :',list(loss_train),'\\n','loss_test : ',list(loss_test),'\\n','f1_class0_train :',list(f1_class0_train),'\\n','f1_class0_test :',list(f1_class0_test),'\\n','f1_class1_train :',list(f1_class1_train),'\\n','f1_class1_test :',list(f1_class1_test),'\\n','accuracy_score_train :',list(accuracy_score_train),'\\n','accuracy_score_test :',list(accuracy_score_test),'\\n','mcc_score_train :',list(mcc_score_train),'\\n',\n",
    "    'mcc_score_test :', list(mcc_score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ff4676d27c1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loss_lst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cc5f9b23adcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loss_lst' is not defined"
     ]
    }
   ],
   "source": [
    "print(train_loss_lst)\n",
    "print(val_loss_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b3e8efdeef61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(epochs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "epochs = np.arange(1,epoch+1)\n",
    "# print(epochs)\n",
    "plt.plot(epochs,loss_train[:-1],label='train_loss')\n",
    "plt.plot(epochs,loss_test[:-1],label='validation_loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.title('XMLRobertaForTokenClassification_earlyStopping_DataAugmentation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = np.arange(1,epoch+1)\n",
    "plt.plot(epochs,mcc_score_train[:-1],label='mcc_train')\n",
    "plt.plot(epochs,mcc_score_test[:-1],label='mcc_test')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('mcc_score')\n",
    "plt.title('XMLRobertaForTokenClassification_earlyStopping')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained('xlm-roberta-base')\n",
    "config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): EntityModel(\n",
      "    (bert): XLMRobertaForTokenClassification(\n",
      "      (roberta): RobertaModel(\n",
      "        (embeddings): RobertaEmbeddings(\n",
      "          (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
      "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "          (token_type_embeddings): Embedding(1, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): RobertaEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConsistencyCost(s_logits, t_logits,mask):\n",
    "    \n",
    "#     assert len(student_output) == len(teacher_output)\n",
    "    \n",
    "    loss = nn.MSELoss()\n",
    "    s_logits = s_logits.view(-1, s_logits.size(-1)).float()\n",
    "    t_logits = t_logits.view(-1, t_logits.size(-1)).float()\n",
    "\n",
    "#     active_outputs_teacher=flatenned_outputs_teacher[torch.where(active_outputs == 1)]\n",
    "#     active_outputs_student=flattened_outputs_student[torch.where(active_outputs == 1)]\n",
    "    print(s_logits.view(-1))\n",
    "    return loss(s_logits.view(-1),t_logits.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2000, 0.5000, 0.9000, 0.4000, 0.9000, 0.2000, 0.2000, 0.9000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1587499976158142"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[0.2,0.5],[0.9,0.4],[0.9,0.2],[0.2,0.9]])\n",
    "b = torch.tensor([[0.5,0.3],[0.7,0.2],[0.5,0.6],[0.7,0.2]])\n",
    "c = torch.tensor([1,1,1,1])\n",
    "ConsistencyCost(a,b,c).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/pytorch-1.6-gpu-py36-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
